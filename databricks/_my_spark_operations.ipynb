{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bc85f78-5ddd-4eb9-a5d9-566ffca98acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/29 11:35:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=Spark Example App #1>\n",
      "Spark App Name : Spark Example App #1\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession v1\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "            #.master(\"local[1]\")  # local- no parallelizm at all, local[2] - 2 cores, local[*] - as many cores as local logical cores\n",
    "            .appName('Spark Example App #1')\n",
    "            .enableHiveSupport()  # if not enabled, tables are not persistent throughout the sessions...!!\n",
    "            .getOrCreate())\n",
    "\n",
    "print(spark.sparkContext)\n",
    "print(\"Spark App Name : \"+ spark.sparkContext.appName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e17ef2b-cd0e-4f92-b60f-9884c512a83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 4\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# Create RDD (Resilient Distributed Dataset)\n",
    "rdd = spark.sparkContext.range(0, 10)\n",
    "print(f\"Number of partitions: {rdd.getNumPartitions()}\")\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7740eade-bcaa-4897-84df-b715e5fb2a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop SparkContext (there can be only one context, otherwise get error: \n",
    "# ValueError: Cannot run multiple SparkContexts at once)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "387ca331-e4ac-49be-b1bb-800a13bd62e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/14 10:42:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/14 10:42:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Example App #2\n"
     ]
    }
   ],
   "source": [
    "# Create or get Spark Context v2. if previous not stopped, it will return that. \n",
    "# if stopped, will create a new context\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local\").setAppName(\"Spark Example App #2\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "print(sc.appName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4081be6-f8b5-47e5-ac50-dd6145d68c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the context\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c5bb99d-1361-4c4d-a1b0-9e7c0395e69f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Spark Example App #2, master=local) created by getOrCreate at /tmp/ipykernel_6213/3106436231.py:6 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create SparkContext v3\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[0;32m----> 3\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSpark Example App #3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(sc\u001b[38;5;241m.\u001b[39mappName)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/context.py:449\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    446\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    454\u001b[0m             currentAppName,\n\u001b[1;32m    455\u001b[0m             currentMaster,\n\u001b[1;32m    456\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    457\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[1;32m    458\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[1;32m    459\u001b[0m         )\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Spark Example App #2, master=local) created by getOrCreate at /tmp/ipykernel_6213/3106436231.py:6 "
     ]
    }
   ],
   "source": [
    "# Create SparkContext v3\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"Spark Example App #3\")\n",
    "print(sc.appName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a3e87ea-18ea-4694-9fd5-dfac7205f6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Create RDD (Resilient Distributed Dataset)\n",
    "#rdd = spark.sparkContext.range(1, 5)\n",
    "rdd = sc.range(1, 5)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b9adb50-f2ee-459f-ae9d-c4356589d604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Application name:  Spark Example App #2\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Norbert| 15|\n",
      "|   John| 30|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load some data\n",
    "sc = spark.sparkContext\n",
    "print('Application name: ', sc.appName)\n",
    "lines = sc.textFile('testfile.txt')\n",
    "parts = lines.map(lambda l: l.split(','))\n",
    "people = parts.map(lambda p: Row(name=p[0],age=int(p[1].strip())))\n",
    "people_df = spark.createDataFrame(people)\n",
    "people_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8462ead0-c73d-4f32-bb5a-18a2a026d079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+--------------------+\n",
      "| id|  city|      date|            datetime|\n",
      "+---+------+----------+--------------------+\n",
      "|  1|city-1|2025-01-01| 2026-01-01 17:01:59|\n",
      "|  2|city-2|2025-01-02| 2026-01-01 17:02:59|\n",
      "|  3|city-3|2025-01-03| 2026-01-01 17:03:59|\n",
      "+---+------+----------+--------------------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- datetime: string (nullable = true)\n",
      "\n",
      "+---+------+----------+-------------------+\n",
      "| id|  city|      date|           datetime|\n",
      "+---+------+----------+-------------------+\n",
      "|  1|city-1|2025-01-01|2026-01-01 17:01:59|\n",
      "|  2|city-2|2025-01-02|2026-01-01 17:02:59|\n",
      "|  3|city-3|2025-01-03|2026-01-01 17:03:59|\n",
      "+---+------+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- datetime: timestamp (nullable = true)\n",
      "\n",
      "+---+-----+\n",
      "| id|id_10|\n",
      "+---+-----+\n",
      "|  1|   11|\n",
      "|  2|   12|\n",
      "|  3|   13|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://spark.apache.org/docs/latest/sql-data-sources-csv.html\n",
    "# load some data without schema\n",
    "df = (spark.read\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"delimiter\",\",\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "    .option(\"timestampFormat\",\"yyyy-MM-dd HH:mm:ss\")\n",
    "    .csv('csv_operations_source.csv')   \n",
    ")\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "# load some data with schema\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, TimestampType, BooleanType\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"datetime\", TimestampType(), True),\n",
    "])\n",
    "\n",
    "df = (spark.read\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"delimiter\",\",\")\n",
    "    .option(\"ignoreLeadingWhiteSpace\", \"true\") \\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "    .option(\"timestampFormat\",\"yyyy-MM-dd HH:mm:ss\")\n",
    "    .schema(schema)\n",
    "    .csv('csv_operations_source.csv')   \n",
    ")\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "# query the data\n",
    "#df.select('id','city').show()\n",
    "df.select(df.id, (df.id + 10).alias('id_10')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1b9cb650-9655-44a6-b97f-5d22915b7ba3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- knownLanguages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+----------+-------------------+--------------------+\n",
      "|      name|     knownLanguages|          properties|\n",
      "+----------+-------------------+--------------------+\n",
      "|     James|      [Java, Scala]|{eye -> brown, ha...|\n",
      "|   Michael|[Spark, Java, NULL]|{eye -> NULL, hai...|\n",
      "|    Robert|         [CSharp, ]|{eye -> , hair ->...|\n",
      "|Washington|               NULL|                NULL|\n",
      "| Jefferson|             [1, 2]|                  {}|\n",
      "+----------+-------------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- col: string (nullable = true)\n",
      " |-- key: string (nullable = false)\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+-------+------+----+-----+\n",
      "|   name|   col| key|value|\n",
      "+-------+------+----+-----+\n",
      "|  James|  Java| eye|brown|\n",
      "|  James|  Java|hair|black|\n",
      "|  James| Scala| eye|brown|\n",
      "|  James| Scala|hair|black|\n",
      "|Michael| Spark| eye| NULL|\n",
      "|Michael| Spark|hair|brown|\n",
      "|Michael|  Java| eye| NULL|\n",
      "|Michael|  Java|hair|brown|\n",
      "|Michael|  NULL| eye| NULL|\n",
      "|Michael|  NULL|hair|brown|\n",
      "| Robert|CSharp| eye|     |\n",
      "| Robert|CSharp|hair|  red|\n",
      "| Robert|      | eye|     |\n",
      "| Robert|      |hair|  red|\n",
      "+-------+------+----+-----+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- col: string (nullable = true)\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+----------+------+----+-----+\n",
      "|      name|   col| key|value|\n",
      "+----------+------+----+-----+\n",
      "|     James|  Java| eye|brown|\n",
      "|     James|  Java|hair|black|\n",
      "|     James| Scala| eye|brown|\n",
      "|     James| Scala|hair|black|\n",
      "|   Michael| Spark| eye| NULL|\n",
      "|   Michael| Spark|hair|brown|\n",
      "|   Michael|  Java| eye| NULL|\n",
      "|   Michael|  Java|hair|brown|\n",
      "|   Michael|  NULL| eye| NULL|\n",
      "|   Michael|  NULL|hair|brown|\n",
      "|    Robert|CSharp| eye|     |\n",
      "|    Robert|CSharp|hair|  red|\n",
      "|    Robert|      | eye|     |\n",
      "|    Robert|      |hair|  red|\n",
      "|Washington|  NULL|NULL| NULL|\n",
      "| Jefferson|     1|NULL| NULL|\n",
      "| Jefferson|     2|NULL| NULL|\n",
      "+----------+------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})]\n",
    "    \n",
    "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "# explode the knownLanguages (array) and properties (map) columns\n",
    "# will only create a record if all columns exist, hence \"inner join\"\n",
    "from pyspark.sql.functions import explode\n",
    "df2 = df.select(df.name, explode(df.knownLanguages), explode(df.properties))\n",
    "df2.printSchema()\n",
    "df2.show()\n",
    "\n",
    "# explode outer will create an \"outer join\", so a record for any data available\n",
    "from pyspark.sql.functions import explode_outer\n",
    "df3 = df.select(df.name, explode_outer(df.knownLanguages), explode_outer(df.properties))\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "000eb59b-4abe-4961-b8f4-4c7794dde714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name| name2|\n",
      "+---+------+------+\n",
      "|  1|Aname1|Bname1|\n",
      "|  2|Aname2|Bname2|\n",
      "|  3|Aname3|  NULL|\n",
      "+---+------+------+\n",
      "\n",
      "+---+----+----+------+------+------+\n",
      "|id1| id2| id3| name1| name2| name3|\n",
      "+---+----+----+------+------+------+\n",
      "|  1|   1|NULL|Aname1|Bname1|  NULL|\n",
      "|  2|   2|   2|Aname2|Bname2|Cname2|\n",
      "|  3|NULL|   3|Aname3|  NULL|Cname3|\n",
      "+---+----+----+------+------+------+\n",
      "\n",
      "+---+----+----+------+------+------+\n",
      "|id1| id2| id3| name1| name2| name3|\n",
      "+---+----+----+------+------+------+\n",
      "|  1|   1|NULL|Aname1|Bname1|  NULL|\n",
      "|  2|   2|   2|Aname2|Bname2|Cname2|\n",
      "|  3|NULL|NULL|Aname3|  NULL|  NULL|\n",
      "+---+----+----+------+------+------+\n",
      "\n",
      "+---+------+------+\n",
      "| id|  name|  name|\n",
      "+---+------+------+\n",
      "|  1|Aname1|Bname1|\n",
      "|  2|Aname2|Bname2|\n",
      "+---+------+------+\n",
      "\n",
      "+---+------+------+\n",
      "| id|  name| name2|\n",
      "+---+------+------+\n",
      "|  1|Aname1|Bname1|\n",
      "|  2|Aname2|Bname2|\n",
      "+---+------+------+\n",
      "\n",
      "+---+------+------+\n",
      "| id| name1| name2|\n",
      "+---+------+------+\n",
      "|  1|Aname1|Bname1|\n",
      "|  2|Aname2|Bname2|\n",
      "|  3|Aname3|  NULL|\n",
      "+---+------+------+\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# outer join 2 tables, resolving ambiguous colmuns\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, TimestampType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "])\n",
    "\n",
    "lst1 = [\n",
    "        (1, 'Aname1'),\n",
    "        (2, 'Aname2'),\n",
    "        (3, 'Aname3'),\n",
    "        ]\n",
    "df1 = spark.createDataFrame(data=lst1, schema = df_schema)\n",
    "\n",
    "lst2 = [\n",
    "        (1, 'Bname1'),\n",
    "        (2, 'Bname2'),\n",
    "        ]\n",
    "df2 = spark.createDataFrame(data=lst2, schema = df_schema)\n",
    "\n",
    "lst3 = [\n",
    "        (2, 'Cname2'),\n",
    "        (3, 'Cname3'),\n",
    "        ]\n",
    "df3 = spark.createDataFrame(data=lst3, schema = df_schema)\n",
    "\n",
    "\n",
    "# join key is id, only id will remain in the result DF. The rest of the columns need to be unique otherwise DF will be ambiguous\n",
    "df_r1 = df1.alias(\"df1\").join(df2.alias(\"df2\"), F.col(\"df1.id\") == F.col('df2.id'), \"left\").select(df1.id, df1.name, df2.name.alias('name2'))\n",
    "df_r1.show()\n",
    "\n",
    "# 3 tables (df1-+df2, df1-+df3), as the df1.id is projected after the first join and not both df1.id, df2.id(!), the 3rd table is joined to df1!\n",
    "df_r1 = (\n",
    "     df1.alias(\"df1\")\n",
    "    .join(df2.alias(\"df2\"), \"id\", \"left\") # df1.id(!) by default!, df1.name, df2.name\n",
    "    .join(df3.alias(\"df3\"), \"id\", \"left\") # df1(!) join to df3\n",
    ").select(df1.id.alias('id1'), df2.id.alias('id2'), df3.id.alias('id3'), df1.name.alias('name1'), df2.name.alias('name2'), df3.name.alias('name3'))\n",
    "df_r1.show()\n",
    "\n",
    "# if we want to join df1-+df2-+df3\n",
    "df_r1 = (\n",
    "     df1.alias(\"df1\")\n",
    "    .join(\n",
    "        df2.alias(\"df2\").join(df3.alias(\"df3\"), \"id\", \"left\"), \n",
    "        \"id\", \"left\") # \n",
    ").select(df1.id.alias('id1'), df2.id.alias('id2'), df3.id.alias('id3'), df1.name.alias('name1'), df2.name.alias('name2'), df3.name.alias('name3'))\n",
    "df_r1.show()\n",
    "\n",
    "# otherwise there is a problem:\n",
    "df_r1 = df1.join(df2, \"id\", \"inner\")\n",
    "df_r1.show() # OK\n",
    "#df_r1.describe() # FAIL!\n",
    "\n",
    "# to resolve even after the join, result schema metadata of the join sources are kept under the hood for data lineage (can be inspected from df.explain())\n",
    "# if the join sources are not yet deleted(!)\n",
    "df_r2=df1.join(df2,\"id\", \"inner\")\n",
    "df_r2 = df_r2.select(df1.id, df1.name, df2.name.alias('name2'))\n",
    "df_r2.show()\n",
    "\n",
    "# if deleted, but we have alias\n",
    "df_r3 = df1.alias(\"df1\").join(df2.alias(\"df2\"), \"id\", \"left\")\n",
    "del df1, df2\n",
    "df_r3 = df_r3.select(F.col(\"df1.id\"), F.col(\"df1.name\").alias(\"name1\"), F.col(\"df2.name\").alias(\"name2\"))\n",
    "df_r3.show()\n",
    "\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
