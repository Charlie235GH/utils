{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "508e31e8-aeab-4241-927f-d43190956d39",
   "metadata": {},
   "source": [
    "## https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "049c4c21-3e6c-4bd3-b56c-97d821073a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active spark session not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/25 12:37:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/25 12:37:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/04/25 12:37:53 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark version: 3.5.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/25 12:37:55 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/04/25 12:37:55 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/04/25 12:37:57 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "25/04/25 12:37:57 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore jovyan@172.17.0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "| mytestdb|\n",
      "|   testdb|\n",
      "+---------+\n",
      "\n",
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|           default|\n",
      "+------------------+\n",
      "\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n",
      "+---------+--------+-----------+\n",
      "|namespace|viewName|isTemporary|\n",
      "+---------+--------+-----------+\n",
      "+---------+--------+-----------+\n",
      "\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/25 12:37:59 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    }
   ],
   "source": [
    "# setup spark on linux\n",
    "from pyspark.sql import *\n",
    "\n",
    "if SparkSession.getActiveSession() == None:\n",
    "    print(\"Active spark session not found\")\n",
    "else:\n",
    "    print(\"Active spark session found\")\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         #.master(\"local\") # local- no parallelizm at all, local[2] - 2 cores, local[*] - as many cores as local logical cores\n",
    "         .appName(\"SparkSession#1\")\n",
    "         .enableHiveSupport() # enableHiveSupport() needed to make data persistent... \n",
    "         #.config(\"spark.driver.allowMultipleContexts\", \"true\")\n",
    "         .config(\"spark.sql.cbo.enabled\", \"true\")\n",
    "         .config(\"spark.sql.cbo.optimizer\", \"true\")\n",
    "         .getOrCreate())\n",
    "\n",
    "print('spark version:', spark.version)\n",
    "\n",
    "spark.sql(\"Show databases\").show()\n",
    "spark.sql(\"select current_database()\").show()\n",
    "#spark.sql(\"use mytestdb\")\n",
    "spark.sql(\"Show tables\").show()\n",
    "spark.sql(\"Show views\").show()\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed16ce79-897f-429d-ad8b-afc74c36dfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# stop the session if needed (for another notebook)\n",
    "spark.stop()\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cec1456c-d49a-499b-b9be-71b5871e9205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/03 14:13:05 WARN ObjectStore: Failed to get database mytestdb, returning NoSuchObjectException\n",
      "25/04/03 14:13:05 WARN ObjectStore: Failed to get database mytestdb, returning NoSuchObjectException\n",
      "25/04/03 14:13:05 WARN ObjectStore: Failed to get database mytestdb, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "| mytestdb|\n",
      "+---------+\n",
      "\n",
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|          mytestdb|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create database\n",
    "spark.sql(\"create database if not exists mytestdb\")\n",
    "spark.sql(\"Show databases\").show()\n",
    "spark.sql(\"use mytestdb\")\n",
    "spark.sql(\"select current_database()\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3996388f-0db7-4d35-9b7e-1b0ed9fd68d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "|  1|name_1| 21|\n",
      "|  2|name_2| 22|\n",
      "|  3|name_3| 23|\n",
      "+---+------+---+\n",
      "\n",
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "|  1|name_1| 21|\n",
      "|  2|name_2| 22|\n",
      "|  3|name_3| 23|\n",
      "+---+------+---+\n",
      "\n",
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1| name_1| 21|\n",
      "| 12|name_12| 32|\n",
      "| 13|name_13| 33|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dataframes\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "# from list, just column names\n",
    "l = [\n",
    "    (1, 'name_1', 21),\n",
    "    (2, 'name_2', 22),\n",
    "    (3, 'name_3', 23),\n",
    "     ]\n",
    "\n",
    "df = spark.createDataFrame(l, schema= [\"id\", \"name\", \"age\"])\n",
    "df.show()\n",
    "\n",
    "# from list, separate schema\n",
    "l = [\n",
    "    (1, 'name_1', 21),\n",
    "    (2, 'name_2', 22),\n",
    "    (3, 'name_3', 23),\n",
    "     ]\n",
    "df_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "df = spark.createDataFrame(l, df_schema)\n",
    "df.show()\n",
    "\n",
    "# from rows implicite schema\n",
    "df = spark.createDataFrame([\n",
    "    Row(id = 1, name = \"name_1\", age = 21),\n",
    "    Row(id = 12, name = \"name_12\", age = 32),\n",
    "    Row(id = 13, name = \"name_13\", age = 33),\n",
    "])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4b5d1c27-1d31-4db4-9225-b7fff53b5a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_df:\n",
      "+---+-------+---+--------------------+\n",
      "| id|   name|age|                  ts|\n",
      "+---+-------+---+--------------------+\n",
      "| 12|name_12| 32|2025-03-19 19:19:...|\n",
      "| 13|name_13| 33|2025-03-19 19:19:...|\n",
      "+---+-------+---+--------------------+\n",
      "\n",
      "df_rslt:\n",
      "+---+-------+---+--------------------+\n",
      "| id|   name|age|                  ts|\n",
      "+---+-------+---+--------------------+\n",
      "|  1| name_1| 21|2025-03-19 19:19:...|\n",
      "| 12|name_12| 32|2025-03-19 19:19:...|\n",
      "| 13|name_13| 33|2025-03-19 19:19:...|\n",
      "+---+-------+---+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########\n",
    "# overview\n",
    "###########\n",
    "# df --> temp view\n",
    "df = spark.createDataFrame([\n",
    "    Row(id = 1, name = \"name_1\", age = 21, ts = datetime.now()),\n",
    "    Row(id = 12, name = \"name_12\", age = 32, ts = datetime.now()),\n",
    "    Row(id = 13, name = \"name_13\", age = 33, ts = datetime.now()),\n",
    "])\n",
    "df.createOrReplaceTempView('v_df')\n",
    "df_rslt = spark.sql('select * from v_df where id > 2')\n",
    "print('v_df:')\n",
    "df_rslt.show()\n",
    "spark.catalog.dropTempView('v_df')\n",
    "\n",
    "# df --> table\n",
    "df = spark.createDataFrame([\n",
    "    Row(id = 1, name = \"name_1\", age = 21, ts = datetime.now()),\n",
    "    Row(id = 12, name = \"name_12\", age = 32, ts = datetime.now()),\n",
    "    Row(id = 13, name = \"name_13\", age = 33, ts = datetime.now()),\n",
    "])\n",
    "\n",
    "# save as managed table (as no path given)\n",
    "(\n",
    "    df.write\n",
    "    .mode('overwrite')\n",
    "    .format('parquet')\n",
    "    .saveAsTable('df_table')\n",
    ")\n",
    "\n",
    "# table --> df\n",
    "df_rslt = spark.sql('select * from df_table')\n",
    "print('df_rslt:')\n",
    "df_rslt.show()\n",
    "spark.sql('drop table if exists df_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3342b6f6-380b-4f88-a2ab-af3cc28acc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+\n",
      "| id|   name| age|\n",
      "+---+-------+----+\n",
      "|  1| name_1|  21|\n",
      "| 12|name_12|NULL|\n",
      "| 13|   NULL|  23|\n",
      "+---+-------+----+\n",
      "\n",
      "+---+-------+----+\n",
      "| id|   name| age|\n",
      "+---+-------+----+\n",
      "|  1| name_1|  99|\n",
      "| 12|name_12|NULL|\n",
      "| 13|   NULL|  23|\n",
      "+---+-------+----+\n",
      "\n",
      "+---+-------+----+\n",
      "| id|   name| age|\n",
      "+---+-------+----+\n",
      "|  1| name_1|  99|\n",
      "| 12|name_12|NULL|\n",
      "| 13|   NULL|  24|\n",
      "+---+-------+----+\n",
      "\n",
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "|  1|name_1| 21|\n",
      "+---+------+---+\n",
      "\n",
      "+---+-------+----+\n",
      "| id|   name| age|\n",
      "+---+-------+----+\n",
      "|  1| name_1|  21|\n",
      "| 12|name_12|NULL|\n",
      "| 13|      ?|  23|\n",
      "+---+-------+----+\n",
      "\n",
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1| name_1| 21|\n",
      "| 12|name_12|  0|\n",
      "| 13|   NULL| 23|\n",
      "+---+-------+---+\n",
      "\n",
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1| name_1| 21|\n",
      "| 12|name_12|  0|\n",
      "| 13|unknown| 23|\n",
      "+---+-------+---+\n",
      "\n",
      "+---+-------+---+--------------------+\n",
      "| id|   name|age| ingestion_timestamp|\n",
      "+---+-------+---+--------------------+\n",
      "|  1| name_1| 21|2025-03-27 12:42:...|\n",
      "| 12|name_12|  0|2025-03-27 12:42:...|\n",
      "| 13|unknown| 23|2025-03-27 12:42:...|\n",
      "+---+-------+---+--------------------+\n",
      "\n",
      "+---+-------+---+--------------------+\n",
      "| id|   name|age|        ingestion_ts|\n",
      "+---+-------+---+--------------------+\n",
      "|  1| name_1| 21|2025-03-27 12:42:...|\n",
      "| 12|name_12|  0|2025-03-27 12:42:...|\n",
      "| 13|unknown| 23|2025-03-27 12:42:...|\n",
      "+---+-------+---+--------------------+\n",
      "\n",
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1| name_1| 21|\n",
      "| 12|name_12|  0|\n",
      "| 13|unknown| 23|\n",
      "+---+-------+---+\n",
      "\n",
      "+---+------+-------+-------+\n",
      "|age|name_1|name_12|unknown|\n",
      "+---+------+-------+-------+\n",
      "|  0|  NULL|     12|   NULL|\n",
      "| 21|     1|   NULL|   NULL|\n",
      "| 23|  NULL|   NULL|     13|\n",
      "+---+------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataframe column manipulations\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "df = spark.createDataFrame([\n",
    "    Row(id = 1, name = \"name_1\", age = 21),\n",
    "    Row(id = 12, name = \"name_12\", age = None),\n",
    "    Row(id = 13, name = None, age = 23),\n",
    "])\n",
    "df.show()\n",
    "\n",
    "# handling empty values (fillna() is alias for .na.fill())\n",
    "df.na.replace(21, 99).show()  # simply replace all values with new one\n",
    "df.na.replace([21,23], [99, 24]).show()  # replace 21 to 99 and 23 to 24!\n",
    "df.na.drop().show() # drop all rows having null values in any column\n",
    "df.na.fill(\"?\").show()  # fill all null values with \"?\"\n",
    "df.na.fill(value=0, subset = ['age']).show() # fill null values of age with 0\n",
    "df=df.na.fill({'name': 'unknown', 'age': 0}) # fill null name with unknown and null age with 0\n",
    "df.show()\n",
    "\n",
    "# add column\n",
    "df_new1 = df.withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "df_new1.show()\n",
    "\n",
    "# rename column\n",
    "df_new2 = df_new1.withColumnRenamed(\"ingestion_timestamp\", \"ingestion_ts\")\n",
    "df_new2.show()\n",
    "\n",
    "# drop column\n",
    "df_new3 = df_new1.drop(\"ingestion_timestamp\")\n",
    "df_new3.show()\n",
    "\n",
    "# pivot\n",
    "df_new4 = df_new3.groupby(\"age\").pivot(\"name\").min(\"id\")\n",
    "df_new4.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3bf1ddd2-5ea6-4735-9501-cb920929c955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/19 18:32:49 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "25/03/19 18:32:49 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "25/03/19 18:32:49 WARN HiveMetaStore: Location: file:/home/jovyan/work/various_tests/spark/spark-warehouse/mytestdb.db/large_table specified for non-external table:large_table\n",
      "25/03/19 18:32:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/19 18:32:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/19 18:32:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/19 18:32:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/19 18:32:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/19 18:32:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/19 18:32:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/19 18:32:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/19 18:32:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|    cnt|\n",
      "+-------+\n",
      "|1000000|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create small_table and large_table\n",
    "\n",
    "spark.sql(\"USE mytestdb\")\n",
    "spark.sql(\"\"\"\n",
    "create table if not exists small_table as\n",
    "select 0 id, 'name00' name union all\n",
    "select 1 id, 'name01' name union all\n",
    "select 2 id, 'name02' name union all\n",
    "select 3 id, 'name03' name union all\n",
    "select 4 id, 'name04' name union all\n",
    "select 5 id, 'name05' name union all\n",
    "select 6 id, 'name06' name union all\n",
    "select 7 id, 'name07' name union all\n",
    "select 8 id, 'name08' name union all\n",
    "select 9 id, 'name09' name\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"drop table if exists large_table\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "create table if not exists large_table as\n",
    "select row_number() over (order by 1) id,\n",
    "        'name' || lpad(row_number() over (order by 1), 8, '0') name\n",
    "from small_table t1\n",
    "    inner join\n",
    "    small_table t2\n",
    "    inner join\n",
    "    small_table t3\n",
    "    inner join\n",
    "    small_table t4\n",
    "    inner join\n",
    "    small_table t5\n",
    "    inner join\n",
    "    small_table t6\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"select count(1) as cnt from large_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "cc5f8561-50c7-4b6c-9a0f-3485d2832cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-19 20:42:51.383042 2025-03-29 20:45:35.335687\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "d_now = datetime.datetime.now() \n",
    "d_10 = d_now + datetime.timedelta(days=10)\n",
    "print(d, d_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "eb71ec94-5160-468f-bd4f-6f52d120253a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joined as df op:\n",
      "+-------+\n",
      "|    cnt|\n",
      "+-------+\n",
      "|1089991|\n",
      "+-------+\n",
      "\n",
      "done in 0:00:01.070806.\n",
      "joined as sql op:\n",
      "+-------+\n",
      "|    cnt|\n",
      "+-------+\n",
      "|1089991|\n",
      "+-------+\n",
      "\n",
      "done in 0:00:00.628944.\n"
     ]
    }
   ],
   "source": [
    "# perf test\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "ct = datetime.now()\n",
    "print('joined as df op:')\n",
    "\n",
    "# 100000x1000000\n",
    "df_t1 = spark.sql('select t1.* from small_table t1 cross join small_table t2 cross join small_table t3 cross join small_table t4 cross join small_table t5')\n",
    "df_t2 = spark.sql('select * from large_table')\n",
    "\n",
    "df_rst1 = (\n",
    "    df_t1.alias('t1')\n",
    "     .join(df_t2.alias('t2'), \n",
    "          (col(\"t1.id\") == col(\"t2.id\")) & (col(\"t1.name\") > col(\"t2.name\")), \"full\")\n",
    "    .filter(~df_t2.name.like('%e1%')) #  not like \n",
    "    .groupBy()\n",
    "    .count().withColumnRenamed('count', 'cnt') # the only way instead of alias?\n",
    ").show()\n",
    "\n",
    "print(f\"done in {(datetime.now() - ct)}.\")\n",
    "\n",
    "ct = datetime.now()\n",
    "print('joined as sql op:')\n",
    "# join as sql using local temp view\n",
    "df_t1.createOrReplaceTempView('t1')  # df --> temp table\n",
    "df_t2.createOrReplaceTempView('t2')  # df --> table\n",
    "\n",
    "# table --> df\n",
    "df_rst1 = spark.sql('''\n",
    "select count(1) as cnt\n",
    "from t1 full outer join t2 \n",
    "on (t1.id = t2.id)\n",
    "where t2.name not like '%e1%'\n",
    "''')\n",
    "df_rst1.show()\n",
    "spark.catalog.dropTempView('t1')\n",
    "spark.catalog.dropTempView('t2')\n",
    "print(f\"done in {(datetime.now() - ct)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5247c6c8-b004-4bb4-866d-c540dd7a09af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+--------------------+\n",
      "| id|  name|age|                  ts|\n",
      "+---+------+---+--------------------+\n",
      "|  1|name_1| 21|2025-03-24 15:35:...|\n",
      "|  2|name_2| 22|2025-03-24 15:35:...|\n",
      "|  3|name_3| 23|2025-03-24 15:35:...|\n",
      "+---+------+---+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/24 15:35:23 WARN SimpleFunctionRegistry: The function f1 replaced a previously registered function.\n",
      "25/03/24 15:35:23 WARN SimpleFunctionRegistry: The function f2 replaced a previously registered function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------------------+-------------------+\n",
      "| id|f1_name|age|                fs_5|           fs_5_fmt|\n",
      "+---+-------+---+--------------------+-------------------+\n",
      "|  1| nAme_1| 21|2025-03-29 15:35:...|2025-03-29 15:35:23|\n",
      "|  2| nAme_2| 22|2025-03-29 15:35:...|2025-03-29 15:35:23|\n",
      "|  3| nAme_3| 23|2025-03-29 15:35:...|2025-03-29 15:35:23|\n",
      "+---+-------+---+--------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "# sql UDF functions\n",
    "def f1(s: str) -> str:\n",
    "    if len(s) < 2:\n",
    "        return s\n",
    "    elif len(s) == 2:\n",
    "        return s[0] + s[1].upper()\n",
    "    else:\n",
    "        return s[0] + s[1].upper() + s[2:]\n",
    "\n",
    "def f2(dt: datetime, n: int) -> TimestampType:\n",
    "    return dt + datetime.timedelta(days = n)\n",
    "\n",
    "l = [\n",
    "    (1, 'name_1', 21, datetime.datetime.now()),\n",
    "    (2, 'name_2', 22, datetime.datetime.now()),\n",
    "    (3, 'name_3', 23, datetime.datetime.now()),\n",
    "     ]\n",
    "df_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"ts\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(l, df_schema)\n",
    "df.show()\n",
    "\n",
    "spark.udf.register(\"f1\", f1)\n",
    "spark.udf.register(\"f2\", f2, TimestampType())\n",
    "df.createOrReplaceTempView(\"udf_test\")\n",
    "spark.sql('''\n",
    "select id, f1(name) as f1_name, \n",
    "    age, \n",
    "    f2(ts, 5) as fs_5,\n",
    "    date_format(f2(ts, 5),\"yyyy-MM-dd HH:mm:ss\") as fs_5_fmt\n",
    "from udf_test\n",
    "''').show()\n",
    "spark.catalog.dropTempView(\"udf_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e92e38f1-6dae-4544-b36f-041c1263b7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+\n",
      "|namespace|   tableName|isTemporary|\n",
      "+---------+------------+-----------+\n",
      "| mytestdb| large_table|      false|\n",
      "| mytestdb| small_table|      false|\n",
      "|         |age_district|       true|\n",
      "+---------+------------+-----------+\n",
      "\n",
      "+---------+------------+-----------+\n",
      "|namespace|    viewName|isTemporary|\n",
      "+---------+------------+-----------+\n",
      "|         |age_district|       true|\n",
      "+---------+------------+-----------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 1001000|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      10|\n",
      "+--------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- ts: date (nullable = true)\n",
      "\n",
      "+---+------+---+----------+\n",
      "| id|  name|age|        ts|\n",
      "+---+------+---+----------+\n",
      "|  1|name_1| 21|2025-03-30|\n",
      "|  2|name_2| 22|2025-03-30|\n",
      "|  3|name_3| 23|2025-03-30|\n",
      "+---+------+---+----------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n",
      "+---+-------+---+--------------------+\n",
      "| id|   name|age|                  ts|\n",
      "+---+-------+---+--------------------+\n",
      "|  1| name_1| 21|2025-03-30 20:56:...|\n",
      "| 12|name_12| 32|2025-03-30 20:56:...|\n",
      "| 13|name_13| 33|2025-03-30 20:56:...|\n",
      "+---+-------+---+--------------------+\n",
      "\n",
      "joined as df op:\n",
      "+----+---+\n",
      "|  id| id|\n",
      "+----+---+\n",
      "|NULL|  1|\n",
      "|NULL| 12|\n",
      "|NULL| 13|\n",
      "+----+---+\n",
      "\n",
      "joined as sql op:\n",
      "+---+------+---+----------+---+------+---+--------------------+\n",
      "| id|  name|age|        ts| id|  name|age|                  ts|\n",
      "+---+------+---+----------+---+------+---+--------------------+\n",
      "|  1|name_1| 21|2025-03-30|  1|name_1| 21|2025-03-30 20:56:...|\n",
      "+---+------+---+----------+---+------+---+--------------------+\n",
      "\n",
      "+---+------+---+----------+---+------+---+--------------------+\n",
      "| id|  name|age|        ts| id|  name|age|                  ts|\n",
      "+---+------+---+----------+---+------+---+--------------------+\n",
      "|  1|name_1| 21|2025-03-30|  1|name_1| 21|2025-03-30 20:56:...|\n",
      "+---+------+---+----------+---+------+---+--------------------+\n",
      "\n",
      "+---+------+---+----------+----+------+----+--------------------+\n",
      "| id|  name|age|        ts|  id|  name| age|                  ts|\n",
      "+---+------+---+----------+----+------+----+--------------------+\n",
      "|  1|name_1| 21|2025-03-30|   1|name_1|  21|2025-03-30 20:56:...|\n",
      "|  3|name_3| 23|2025-03-30|NULL|  NULL|NULL|                NULL|\n",
      "|  2|name_2| 22|2025-03-30|NULL|  NULL|NULL|                NULL|\n",
      "+---+------+---+----------+----+------+----+--------------------+\n",
      "\n",
      "+----+------+----+----------+---+-------+---+--------------------+\n",
      "|  id|  name| age|        ts| id|   name|age|                  ts|\n",
      "+----+------+----+----------+---+-------+---+--------------------+\n",
      "|   1|name_1|  21|2025-03-30|  1| name_1| 21|2025-03-30 20:56:...|\n",
      "|NULL|  NULL|NULL|      NULL| 12|name_12| 32|2025-03-30 20:56:...|\n",
      "|NULL|  NULL|NULL|      NULL| 13|name_13| 33|2025-03-30 20:56:...|\n",
      "+----+------+----+----------+---+-------+---+--------------------+\n",
      "\n",
      "+----+------+----+----------+----+-------+----+--------------------+\n",
      "|  id|  name| age|        ts|  id|   name| age|                  ts|\n",
      "+----+------+----+----------+----+-------+----+--------------------+\n",
      "|   1|name_1|  21|2025-03-30|   1| name_1|  21|2025-03-30 20:56:...|\n",
      "|   2|name_2|  22|2025-03-30|NULL|   NULL|NULL|                NULL|\n",
      "|   3|name_3|  23|2025-03-30|NULL|   NULL|NULL|                NULL|\n",
      "|NULL|  NULL|NULL|      NULL|  12|name_12|  32|2025-03-30 20:56:...|\n",
      "|NULL|  NULL|NULL|      NULL|  13|name_13|  33|2025-03-30 20:56:...|\n",
      "+----+------+----+----------+----+-------+----+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joins, filter\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "\n",
    "spark.sql(\"use mytestdb\")\n",
    "spark.sql(\"Show tables\").show()\n",
    "spark.sql(\"Show views\").show()\n",
    "spark.sql(\"SELECT count(1) FROM large_table\").show()\n",
    "spark.sql(\"SELECT count(1) FROM small_table\").show()\n",
    "\n",
    "t1 = [\n",
    "    (1, 'name_1', 21, datetime.now()),\n",
    "    (2, 'name_2', 22, datetime.now()),\n",
    "    (3, 'name_3', 23, datetime.now()),\n",
    "     ]\n",
    "t1_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"ts\", DateType(), True)\n",
    "])\n",
    "\n",
    "# from list\n",
    "df_t1 = spark.createDataFrame(t1, schema = t1_schema)\n",
    "df_t1.printSchema()\n",
    "df_t1.show()\n",
    "\n",
    "# from rows of data\n",
    "df_t2 = spark.createDataFrame([\n",
    "    Row(id = 1, name = \"name_1\", age = 21, ts = datetime.now()),\n",
    "    Row(id = 12, name = \"name_12\", age = 32, ts = datetime.now()),\n",
    "    Row(id = 13, name = \"name_13\", age = 33, ts = datetime.now()),\n",
    "])\n",
    "\n",
    "df_t2.printSchema()\n",
    "df_t2.show()\n",
    "\n",
    "\n",
    "print('joined as df op:')\n",
    "# join as df\n",
    "from pyspark.sql.functions import col\n",
    "df_rst1 = (\n",
    "    df_t1.alias('t1')\n",
    "    .filter(df_t1.age  > 5)  # <, ==, >, !=, isin(), &,|, \n",
    "    .filter(df_t1.age.isin([32, 33]))  # <, ==, >, !=, isin(), &,|, startswith(), endswith(), contains(), like('%x%')\n",
    "    .filter(df_t1.name.like('%am%'))\n",
    "    .filter(~df_t1.name.like('%xx%')) #  not like\n",
    "    .filter(\"t1.age > 5 and t1.age in (32,33) and t1.name like '%am%'\") # simple spark sql where condition copy\n",
    "    .join(df_t2.alias('t2'), \n",
    "          (col(\"t1.id\") == col(\"t2.id\")) & (col(\"t1.name\") > col(\"t2.name\")), \"full\")\n",
    "     .select('t1.id', 't2.id')\n",
    ").show()\n",
    "\n",
    "print('joined as sql op:')\n",
    "# join as sql using local temp view\n",
    "df_t1.createTempView('t1')\n",
    "df_t2.createTempView('t2')\n",
    "df_rst1 = spark.sql('select * from t1 inner join t2 on (t1.id = t2.id)')\n",
    "df_rst1.show()\n",
    "spark.catalog.dropTempView('t1')\n",
    "spark.catalog.dropTempView('t2')\n",
    "\n",
    "# join as sql using global temp view\n",
    "spark.sql('drop view if exists global_temp.t1')\n",
    "spark.sql('drop view if exists global_temp.t2')\n",
    "df_t1.createOrReplaceGlobalTempView('t1')\n",
    "df_t2.createOrReplaceGlobalTempView('t2')\n",
    "df_rst1 = spark.sql('select * from global_temp.t1 inner join global_temp.t2 on (t1.id = t2.id)')\n",
    "df_rst1.show()\n",
    "df_rst2 = spark.sql('select * from global_temp.t1 left outer join global_temp.t2 on (t1.id = t2.id)')\n",
    "df_rst2.show()\n",
    "df_rst2 = spark.sql('select * from global_temp.t1 right outer join global_temp.t2 on (t1.id = t2.id)')\n",
    "df_rst2.show()\n",
    "df_rst2 = spark.sql('select * from global_temp.t1 full outer join global_temp.t2 on (t1.id = t2.id)')\n",
    "df_rst2.show()\n",
    "\n",
    "# join hints (BROADCAST, MERGE, SHUFFLE_HASH and SHUFFLE_REPLICATE_NL)\n",
    "df_rst1 = spark.sql('select /*+ BROADCAST(global_temp.t1)*/ * from global_temp.t1 inner join global_temp.t2 on (t1.id = t2.id)')\n",
    "\n",
    "# drop temp views\n",
    "spark.catalog.dropGlobalTempView('t1')\n",
    "spark.catalog.dropGlobalTempView('t2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "50ac4365-5940-47cf-ae1c-2c03d54dd25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "|  1|name_1| 21|\n",
      "|  2|name_2| 22|\n",
      "|  3|name_3| 23|\n",
      "+---+------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/19 20:36:32 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "# cache the DF to memory\n",
    "df.cache()\n",
    "\n",
    "\n",
    "# persist / cache (persist to StorageLevel.MEMORY_AND_DISK) data for optimization\n",
    "# MEMORY_ONLY,MEMORY_AND_DISK, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER, DISK_ONLY, MEMORY_ONLY_2,MEMORY_AND_DISK_2\n",
    "df.persist(pyspark.StorageLevel.DISK_ONLY)  \n",
    "df.show()\n",
    "\n",
    "# checkpoint: write DF only to disk. by default, checkpoint happens after compute, as a separate job\n",
    "spark.sparkContext.setCheckpointDir(\"./checkpoint\")\n",
    "spark.conf.set(\"spark.cleaner.referenceTracking.cleanCheckpoints\", True)  # cleanup checkpoint dir if DF is out of scope\n",
    "df_r1.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "331146c7-0cc9-411d-a24f-b657bd9edbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------------------+\n",
      "| id|   name|age|                  ts|\n",
      "+---+-------+---+--------------------+\n",
      "|  1| name_1| 21|2025-03-30 20:52:...|\n",
      "| 12|name_12| 32|2025-03-30 20:52:...|\n",
      "| 13|name_13| 33|2025-03-30 20:52:...|\n",
      "+---+-------+---+--------------------+\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# save / remove dataframe as parquet\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# df --> table\n",
    "df = spark.createDataFrame([\n",
    "    Row(id = 1, name = \"name_1\", age = 21, ts = datetime.now()),\n",
    "    Row(id = 12, name = \"name_12\", age = 32, ts = datetime.now()),\n",
    "    Row(id = 13, name = \"name_13\", age = 33, ts = datetime.now()),\n",
    "])\n",
    "df.show()\n",
    "\n",
    "# save as parquet file\n",
    "if not os.path.exists('df_test1'):\n",
    "    print('saving df as df_test1 parquet')\n",
    "    df.write.format('parquet').save('df_test1')\n",
    "\n",
    "# remove parquet file\n",
    "del df\n",
    "shutil.rmtree('df_test1', shutil.rmtree)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "320d95ab-7412-4d0c-886d-63f9a791044d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating large list: \n",
      "Done in 0:00:00.000399\n",
      "creating large DF from list: \n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- district: integer (nullable = true)\n",
      "\n",
      "+---+-------+---+--------+\n",
      "| id|   name|age|district|\n",
      "+---+-------+---+--------+\n",
      "|  0| name-0| 56|    NULL|\n",
      "|  1| name-1| 24|    1000|\n",
      "|  2| name-2| 99|    1003|\n",
      "|  3| name-3| 40|    1000|\n",
      "|  4| name-4| 49|    1000|\n",
      "|  5| name-5| 33|    1000|\n",
      "|  6| name-6| 40|    1000|\n",
      "|  7| name-7| 51|    1000|\n",
      "|  8| name-8| 93|    1001|\n",
      "|  9| name-9| 92|    1003|\n",
      "| 10|name-10| 96|    1001|\n",
      "| 11|name-11| 12|    NULL|\n",
      "| 12|name-12| 33|    1000|\n",
      "| 13|name-13| 54|    1003|\n",
      "| 14|name-14| 18|    1001|\n",
      "| 15|name-15| 47|    1002|\n",
      "| 16|name-16| 68|    1003|\n",
      "| 17|name-17| 71|    1000|\n",
      "| 18|name-18| 73|    1003|\n",
      "| 19|name-19| 63|    1004|\n",
      "+---+-------+---+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Done in 0:00:00.152793\n",
      "repartitioning DF: \n",
      "Done (1) in 0:00:00.107518\n",
      "saving DF to disk: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 0:00:00.797781\n",
      "clearing cache: \n",
      "Done in 0:00:00.000300\n",
      "reading DF: root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- count(age): long (nullable = false)\n",
      "\n",
      "+----+----------+\n",
      "| age|count(age)|\n",
      "+----+----------+\n",
      "|NULL|         0|\n",
      "|   1|         2|\n",
      "|   3|         2|\n",
      "|   4|         1|\n",
      "|   6|         2|\n",
      "|   8|         1|\n",
      "|  11|         1|\n",
      "|  12|         1|\n",
      "|  13|         1|\n",
      "|  14|         1|\n",
      "|  16|         1|\n",
      "|  17|         1|\n",
      "|  18|         2|\n",
      "|  20|         2|\n",
      "|  21|         1|\n",
      "|  22|         1|\n",
      "|  23|         2|\n",
      "|  24|         1|\n",
      "|  25|         1|\n",
      "|  28|         1|\n",
      "+----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Done in 0:00:01.289620\n",
      "clearing cache: \n",
      "Done in 0:00:00.000285\n",
      "reading DF from file for SQL: \n",
      "Done in 0:00:00.881780\n",
      "reading specific age and district dataset as SQL... \n",
      "+---+--------+---+\n",
      "|age|district|cnt|\n",
      "+---+--------+---+\n",
      "+---+--------+---+\n",
      "\n",
      "Done in 0:00:00.055287\n",
      "removing DF: \n",
      "Done in 0:00:00.000071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 20:31:21 WARN ObjectStore: Failed to get database parquet, returning NoSuchObjectException\n",
      "25/03/30 20:31:22 ERROR Executor: Exception in task 0.0 in stage 30.0 (TID 682)\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/home/jovyan/work/various_tests/spark/my_partitioned_df/age=1/district=1000/part-00000-72bb792d-3e25-46fe-a863-d6cc1a238a2d.c000.csv. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1057)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n",
      "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
      "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
      "Caused by: java.lang.RuntimeException: file:/home/jovyan/work/various_tests/spark/my_partitioned_df/age=1/district=1000/part-00000-72bb792d-3e25-46fe-a863-d6cc1a238a2d.c000.csv is not a Parquet file. Expected magic number at tail, but found [45, 53, 52, 10]\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n",
      "\t... 14 more\n",
      "25/03/30 20:31:22 WARN TaskSetManager: Lost task 0.0 in stage 30.0 (TID 682) (efd92000ab18 executor driver): org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/home/jovyan/work/various_tests/spark/my_partitioned_df/age=1/district=1000/part-00000-72bb792d-3e25-46fe-a863-d6cc1a238a2d.c000.csv. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1057)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n",
      "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
      "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
      "Caused by: java.lang.RuntimeException: file:/home/jovyan/work/various_tests/spark/my_partitioned_df/age=1/district=1000/part-00000-72bb792d-3e25-46fe-a863-d6cc1a238a2d.c000.csv is not a Parquet file. Expected magic number at tail, but found [45, 53, 52, 10]\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n",
      "\t... 14 more\n",
      "\n",
      "25/03/30 20:31:22 ERROR TaskSetManager: Task 0 in stage 30.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 1 pos 14",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 115\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mct\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# querying a parquet file DIRECTLY\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM parquet.`my_partitioned_df`\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: parquet; line 1 pos 14"
     ]
    }
   ],
   "source": [
    "# read / select / group by performance tests\n",
    "# partitioning (hash, range,round-robin)\n",
    "# use repartition() and coalesce() for dynamic partitioning\n",
    "# use  partitionBy() partition by multiple columns\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-partitionby-example/\n",
    "\n",
    "from datetime import datetime\n",
    "import random\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "\n",
    "# creating large DF:\n",
    "print(\"creating large list: \")\n",
    "ct = datetime.now()\n",
    "l = []\n",
    "rows = 100\n",
    "for c in range(rows):\n",
    "    l.append([c, ('name-' + str(c)),  None if random.random() < 0.1 else random.randint(1, 100),  None if random.random() < 0.1 else random.randint(1000, 1004)])\n",
    "\n",
    "df_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"district\", IntegerType(), True),\n",
    "])\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "print(\"creating large DF from list: \")\n",
    "ct = datetime.now()\n",
    "df = spark.createDataFrame(l, df_schema)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "\n",
    "# repartitioning DF in memory (weather partitionBy is used or not)\n",
    "print(\"repartitioning DF: \")\n",
    "ct = datetime.now()\n",
    "#df4 = df.repartition(4, \"age\")\n",
    "df4 = df.repartition(1)\n",
    "print(f\"Done ({df4.rdd.getNumPartitions()}) in {datetime.now() - ct}\")\n",
    "\n",
    "# data skew (repartition by maxRecordsPerFile in case partition size varies much)\n",
    "\n",
    "# us partitionBy (physically partition, not logically as group by) to split data into partitions and write, creating subfolters, eg. age=77/district=1004.\n",
    "# data file does not include the partitioned columns (they woud be redundant)\n",
    "print(\"saving DF to disk: \")\n",
    "ct = datetime.now()\n",
    "(\n",
    "    df4.write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", True)\n",
    "    #.option(\"maxRecordsPerFile\", 1000)\n",
    "    .partitionBy(\"age\", \"district\")  # test: partition the data (into folders & subfolders)\n",
    "    .csv(\"my_partitioned_df\")\n",
    ")\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "# clear cache for perf testing\n",
    "print(\"clearing cache: \")\n",
    "ct = datetime.now()\n",
    "spark.catalog.clearCache()\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "# reading DF\n",
    "print(\"reading DF: \", end=\"\")\n",
    "ct = datetime.now()\n",
    "read_df = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    #.csv(\"my_partitioned_df/age=28/district=1003\")  # either read single partition (partition columns will not exist)\n",
    "    .csv(\"my_partitioned_df\")  # or read full DF, including partitioned columns\n",
    ")\n",
    "# cannot filter partition columns IF single partition is read. partitions do not exist (not written into datafile)!!!\n",
    "# if full DF is read and filtered, performance is not bad\n",
    "#read_df = read_df.filter((read_df.age == 28) & (read_df.district == 1003)).groupby(\"age\").count()\n",
    "read_df = read_df.groupBy(\"age\").agg(sf.count(read_df.age)).sort(read_df.age.asc())\n",
    "\n",
    "read_df.printSchema()\n",
    "read_df.show()\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "\n",
    "# clear cache for perf testing\n",
    "print(\"clearing cache: \")\n",
    "ct = datetime.now()\n",
    "spark.catalog.clearCache()\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "# reading DF as SQL\n",
    "print(\"reading DF from file for SQL: \")\n",
    "read_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", True)\n",
    "    .csv(\"my_partitioned_df\")\n",
    ")\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "print(\"reading specific age and district dataset as SQL... \")\n",
    "ct = datetime.now()\n",
    "read_df.createOrReplaceTempView(\"AGE_DISTRICT\")\n",
    "spark.sql(\"select age, district, count(1) cnt from AGE_DISTRICT  where age=28 and district = 1003 group by age, district order by age, district\").show()\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "# removing DF\n",
    "print(\"removing DF: \")\n",
    "ct = datetime.now()\n",
    "del df\n",
    "del df4\n",
    "del read_df\n",
    "print(f\"Done in {datetime.now() - ct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea850fd0-f3b4-466f-b86f-e6941a27a665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating list: \n",
      "Done in 0:00:00.000388\n",
      "creating DF from list: \n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- district: integer (nullable = true)\n",
      "\n",
      "+---+------+----+--------+\n",
      "| id|  name| age|district|\n",
      "+---+------+----+--------+\n",
      "|  0|name-0|  79|    1000|\n",
      "|  1|name-1|  20|    1004|\n",
      "|  2|name-2|  37|    1003|\n",
      "|  3|name-3|  30|    1003|\n",
      "|  4|name-4|NULL|    1000|\n",
      "|  5|name-5|NULL|    1002|\n",
      "|  6|name-6|  56|    1004|\n",
      "|  7|name-7|  60|    NULL|\n",
      "|  8|name-8|NULL|    1002|\n",
      "|  9|name-9|  34|    1001|\n",
      "+---+------+----+--------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- district: integer (nullable = true)\n",
      "\n",
      "+---+------+----+--------+\n",
      "| id|  name| age|district|\n",
      "+---+------+----+--------+\n",
      "|  0|name-0|  98|    1003|\n",
      "|  1|name-1|  64|    1003|\n",
      "|  2|name-2|NULL|    1001|\n",
      "|  3|name-3|  64|    1000|\n",
      "|  4|name-4|  11|    1000|\n",
      "|  5|name-5|NULL|    1002|\n",
      "|  6|name-6|  78|    1004|\n",
      "|  7|name-7|  59|    1000|\n",
      "|  8|name-8|NULL|    1004|\n",
      "|  9|name-9|  38|    1003|\n",
      "+---+------+----+--------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- district: integer (nullable = true)\n",
      "\n",
      "+---+------+----+--------+\n",
      "| id|  name| age|district|\n",
      "+---+------+----+--------+\n",
      "|  0|name-0|  19|    1003|\n",
      "|  1|name-1|NULL|    1003|\n",
      "|  2|name-2|  36|    1003|\n",
      "|  3|name-3|  41|    1000|\n",
      "|  4|name-4|  31|    1003|\n",
      "|  5|name-5|  58|    1003|\n",
      "|  6|name-6|  78|    1004|\n",
      "|  7|name-7|  98|    1004|\n",
      "|  8|name-8|   4|    1004|\n",
      "|  9|name-9|  19|    1003|\n",
      "+---+------+----+--------+\n",
      "\n",
      "Done in 0:00:00.344287\n",
      "DF join:\n",
      "+---+------+----+--------+------------+------------+\n",
      "| id|  name| age|district|district_df2|district_df3|\n",
      "+---+------+----+--------+------------+------------+\n",
      "|  7|name-7|  60|    NULL|        1000|        1004|\n",
      "|  5|name-5|NULL|    1002|        1002|        1003|\n",
      "|  3|name-3|  30|    1003|        1000|        1000|\n",
      "|  4|name-4|NULL|    1000|        1000|        1003|\n",
      "|  0|name-0|  79|    1000|        1003|        1003|\n",
      "|  8|name-8|NULL|    1002|        1004|        1004|\n",
      "|  9|name-9|  34|    1001|        1003|        1003|\n",
      "|  2|name-2|  37|    1003|        1001|        1003|\n",
      "|  1|name-1|  20|    1004|        1003|        1003|\n",
      "|  6|name-6|  56|    1004|        1004|        1004|\n",
      "+---+------+----+--------+------------+------------+\n",
      "\n",
      "+---+------+----+--------+------------+------------+\n",
      "| id|  name| age|district|district_df2|district_df3|\n",
      "+---+------+----+--------+------------+------------+\n",
      "|  7|name-7|  60|    NULL|        1000|        1004|\n",
      "|  5|name-5|NULL|    1002|        1002|        1003|\n",
      "|  3|name-3|  30|    1003|        1000|        1000|\n",
      "|  4|name-4|NULL|    1000|        1000|        1003|\n",
      "|  0|name-0|  79|    1000|        1003|        1003|\n",
      "|  8|name-8|NULL|    1002|        1004|        1004|\n",
      "|  9|name-9|  34|    1001|        1003|        1003|\n",
      "|  2|name-2|  37|    1003|        1001|        1003|\n",
      "|  1|name-1|  20|    1004|        1003|        1003|\n",
      "|  6|name-6|  56|    1004|        1004|        1004|\n",
      "+---+------+----+--------+------------+------------+\n",
      "\n",
      "removing DF: \n",
      "Done in 0:00:00.000097\n"
     ]
    }
   ],
   "source": [
    "# join performance tests DF vs SQL\n",
    "\n",
    "from datetime import datetime\n",
    "import random\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "\n",
    "# creating large DF:\n",
    "print(\"creating list: \")\n",
    "ct = datetime.now()\n",
    "l1 = []\n",
    "rows = 10\n",
    "for c in range(rows):\n",
    "    l1.append([c, ('name-' + str(c)), None if random.random() < 0.1 else random.randint(1, 100), None if random.random() < 0.1 else random.randint(1000, 1004)])\n",
    "\n",
    "l2 = []\n",
    "rows = 10\n",
    "for c in range(rows):\n",
    "    l2.append([c, ('name-' + str(c)), None if random.random() < 0.1 else random.randint(1, 100), None if random.random() < 0.1 else random.randint(1000, 1004)])\n",
    "\n",
    "l3 = []\n",
    "rows = 10\n",
    "for c in range(rows):\n",
    "    l3.append([c, ('name-' + str(c)), None if random.random() < 0.1 else random.randint(1, 100), None if random.random() < 0.1 else random.randint(1000, 1004)])\n",
    "\n",
    "df_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"district\", IntegerType(), True),\n",
    "])\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "print(\"creating DF from list: \")\n",
    "ct = datetime.now()\n",
    "df1 = spark.createDataFrame(l1, df_schema)\n",
    "df1.printSchema()\n",
    "df1.show()\n",
    "df2 = spark.createDataFrame(l2, df_schema)\n",
    "df2.printSchema()\n",
    "df2.show()\n",
    "df3 = spark.createDataFrame(l3, df_schema)\n",
    "df3.printSchema()\n",
    "df3.show()\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "del l1\n",
    "del l2\n",
    "del l3\n",
    "\n",
    "print(\"DF join:\")\n",
    "df_query = df1.join(other=df2, on=[\"id\", \"name\"], how='inner') # equi join on id=id, name=name, all columns retrieved, duplicates cause issues later\n",
    "df_query = df1.join(other=df2, on=[df1.id == df2.id, df1.name == df2.name], how='inner') # normal join, all columns retrieved, duplicates cause issues later\n",
    "df_query = (\n",
    "                df1\n",
    "                .join(other=df2, on=[df1.id == df2.id, df1.name == df2.name], how='inner')\n",
    "                .select(df1.id, df1.name, df1.district, df2.id.alias(\"id_df2\")) # normal join, all columns retrieved\n",
    "            )\n",
    "\n",
    "df_query = (\n",
    "                df1\n",
    "                .join(other=df2, on=[df1.id == df2.id, df1.name == df2.name], how='inner')\n",
    "                .join(other=df3, on=[df2.id == df3.id, df2.name == df3.name], how='inner')\n",
    "                .select(df1.id, df1.name, df1.age, df1.district, df2.district.alias(\"district_df2\"), df3.district.alias(\"district_df3\")) # normal join, all columns retrieved\n",
    "            )\n",
    "df_query.name = \"5\"\n",
    "#df_query.describe().show()\n",
    "df_query.show()\n",
    "\n",
    "\n",
    "df1.createOrReplaceTempView('t1')\n",
    "df2.createOrReplaceTempView('t2')\n",
    "df3.createOrReplaceTempView('t3')\n",
    "df_rst1 = spark.sql('''\n",
    "select t1.*, t2.district district_df2, t3.district district_df3\n",
    "from t1 inner join t2 on (t1.id = t2.id and t1.name = t2.name)\n",
    "    inner join t3 on (t2.id = t3.id and t2.name = t3.name)\n",
    "''')\n",
    "df_rst1.show()\n",
    "\n",
    "# removing DF\n",
    "print(\"removing DF: \")\n",
    "ct = datetime.now()\n",
    "del df1\n",
    "del df2\n",
    "del df_query\n",
    "del df_rst1\n",
    "\n",
    "print(f\"Done in {datetime.now() - ct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "787baa3c-ae2f-44ce-a9ba-defcc2e66055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-----+---------+\n",
      "| age| age| age|  age|(age + 1)|\n",
      "+----+----+----+-----+---------+\n",
      "|NULL|NULL|NULL| NULL|     NULL|\n",
      "|NULL|NULL|NULL| NULL|     NULL|\n",
      "|  38|  39|  38|false|       39|\n",
      "|  10|  11|  10|false|       11|\n",
      "|  12|  13|  12|false|       13|\n",
      "|  69|  70|  69|false|       70|\n",
      "|  75|  76|  75|false|       76|\n",
      "|  70|  71|  70|false|       71|\n",
      "|  98|  99|  98|false|       99|\n",
      "|  33|  34|  33|false|       34|\n",
      "|  91|  92|  91|false|       92|\n",
      "|  54|  55|  54|false|       55|\n",
      "|   8|   9|   8|false|        9|\n",
      "|  50|  51|  50|false|       51|\n",
      "|   3|   4|   3|false|        4|\n",
      "|  29|  30|  29|false|       30|\n",
      "|  82|  83|  82|false|       83|\n",
      "|  98|  99|  98|false|       99|\n",
      "|  94|  95|  94|false|       95|\n",
      "|  44|  45|  44|false|       45|\n",
      "+----+----+----+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+---------------+\n",
      "|count(age)|count(exr('1'))|\n",
      "+----------+---------------+\n",
      "|        84|            100|\n",
      "+----------+---------------+\n",
      "\n",
      "+---+-------+---+--------+\n",
      "| id|   name|age|district|\n",
      "+---+-------+---+--------+\n",
      "| 87|name-87| 56|    1004|\n",
      "+---+-------+---+--------+\n",
      "\n",
      "+---+-------+---+--------+\n",
      "| id|   name|age|district|\n",
      "+---+-------+---+--------+\n",
      "|  9| name-9| 33|    1004|\n",
      "| 55|name-55| 32|    1002|\n",
      "+---+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select, filter\n",
    "from datetime import datetime\n",
    "import random\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "l = []\n",
    "rows = 100\n",
    "for c in range(rows):\n",
    "    l.append([c, ('name-' + str(c)), None if random.random() < 0.1 else random.randint(1, 100), random.randint(1000, 1004)])\n",
    "\n",
    "df_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"district\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(l, df_schema)\n",
    "\n",
    "# 1. select logic result (true/false)\n",
    "df.select(df.age, (df['age'] + 1).alias('age'), 'age', (df['age'] == 56).alias('age'), df.age + 1).show()\n",
    "\n",
    "# select count\n",
    "df.select(count(df.age).alias(\"count(age)\"), count(expr(\"1\")).alias(\"count(exr('1'))\")).show() # not null columns\n",
    "\n",
    "# 2. real filter\n",
    "df.filter(df.age == 56).show()\n",
    "\n",
    "(\n",
    "df.filter(df.age  > 5)  # <, ==, >, !=, isin(), &,|, \n",
    "     .filter(df.age.isin([32, 33]))  # <, ==, >, !=, isin(), &,|, startswith(), endswith(), contains(), like('%x%')\n",
    "     .filter(df.name.like('%am%'))\n",
    "     .filter(~df.name.like('%xx%')) #  not like\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e0f65a2-58f8-46c8-b051-5ec4cb9cbfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- district: integer (nullable = true)\n",
      "\n",
      "describe:\n",
      "+-------+------------------+------+------------------+------------------+\n",
      "|summary|                id|  name|               age|          district|\n",
      "+-------+------------------+------+------------------+------------------+\n",
      "|  count|               100|   100|                93|                90|\n",
      "|   mean|              49.5|  NULL| 46.69892473118279|1001.8333333333334|\n",
      "| stddev|29.011491975882016|  NULL|27.957181913961932| 1.326056924227661|\n",
      "|    min|                 0|name-0|                 1|              1000|\n",
      "|    max|                99|name-9|                98|              1004|\n",
      "+-------+------------------+------+------------------+------------------+\n",
      "\n",
      "explain:\n",
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[id#5172,name#5173,age#5174,district#5175]\n",
      "\n",
      "\n",
      "group:\n",
      "+------+---+-----+\n",
      "|  name|age|count|\n",
      "+------+---+-----+\n",
      "|name-8| 71|    1|\n",
      "|name-8| 15|    1|\n",
      "|name-0| 46|    1|\n",
      "|name-5| 91|    1|\n",
      "|name-4| 42|    1|\n",
      "|name-0| 88|    1|\n",
      "|name-8| 81|    1|\n",
      "|name-1| 42|    1|\n",
      "|name-6| 59|    1|\n",
      "|name-7|  8|    1|\n",
      "|name-3| 61|    1|\n",
      "|name-4| 14|    1|\n",
      "|name-5| 80|    1|\n",
      "|name-0| 16|    1|\n",
      "|name-6| 10|    1|\n",
      "|name-3| 17|    1|\n",
      "|name-7|  3|    1|\n",
      "|name-5| 88|    1|\n",
      "|name-0| 59|    1|\n",
      "|name-4| 85|    1|\n",
      "+------+---+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+---+-----+\n",
      "|  name|age|count|\n",
      "+------+---+-----+\n",
      "|name-8| 71|    1|\n",
      "|name-8| 15|    1|\n",
      "|name-0| 46|    1|\n",
      "|name-5| 91|    1|\n",
      "|name-4| 42|    1|\n",
      "|name-0| 88|    1|\n",
      "|name-8| 81|    1|\n",
      "|name-1| 42|    1|\n",
      "|name-6| 59|    1|\n",
      "|name-7|  8|    1|\n",
      "|name-3| 61|    1|\n",
      "|name-4| 14|    1|\n",
      "|name-5| 80|    1|\n",
      "|name-0| 16|    1|\n",
      "|name-6| 10|    1|\n",
      "|name-3| 17|    1|\n",
      "|name-7|  3|    1|\n",
      "|name-5| 88|    1|\n",
      "|name-0| 59|    1|\n",
      "|name-4| 85|    1|\n",
      "+------+---+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+---+------+--------+-------------+\n",
      "|  name|age|min_id|min(age)|min(district)|\n",
      "+------+---+------+--------+-------------+\n",
      "|name-8| 71|    28|      71|         1000|\n",
      "|name-8| 15|    18|      15|         1002|\n",
      "|name-0| 46|    40|      46|         1001|\n",
      "|name-5| 91|    95|      91|         1002|\n",
      "|name-4| 42|    64|      42|         1004|\n",
      "|name-0| 88|     0|      88|         1001|\n",
      "|name-8| 81|     8|      81|         1004|\n",
      "|name-1| 42|    41|      42|         1002|\n",
      "|name-6| 59|    16|      59|         1004|\n",
      "|name-7|  8|    27|       8|         1002|\n",
      "|name-3| 61|     3|      61|         NULL|\n",
      "|name-4| 14|    24|      14|         1002|\n",
      "|name-5| 80|    75|      80|         1004|\n",
      "|name-0| 16|    60|      16|         1000|\n",
      "|name-6| 10|    66|      10|         1003|\n",
      "|name-3| 17|    13|      17|         1004|\n",
      "|name-7|  3|    47|       3|         1000|\n",
      "|name-5| 88|    55|      88|         1001|\n",
      "|name-0| 59|    10|      59|         1002|\n",
      "|name-4| 85|    14|      85|         1000|\n",
      "+------+---+------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+---+-------+\n",
      "|  name|age|min_age|\n",
      "+------+---+-------+\n",
      "|name-8| 71|     71|\n",
      "|name-8| 15|     15|\n",
      "|name-0| 46|     46|\n",
      "|name-5| 91|     91|\n",
      "|name-4| 42|     42|\n",
      "|name-0| 88|     88|\n",
      "|name-8| 81|     81|\n",
      "|name-1| 42|     42|\n",
      "|name-6| 59|     59|\n",
      "|name-7|  8|      8|\n",
      "|name-3| 61|     61|\n",
      "|name-4| 14|     14|\n",
      "|name-5| 80|     80|\n",
      "|name-0| 16|     16|\n",
      "|name-6| 10|     10|\n",
      "|name-3| 17|     17|\n",
      "|name-7|  3|      3|\n",
      "|name-5| 88|     88|\n",
      "|name-0| 59|     59|\n",
      "|name-4| 85|     85|\n",
      "+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+---+-------------+\n",
      "|  name|age|max(district)|\n",
      "+------+---+-------------+\n",
      "|name-8| 71|         1000|\n",
      "|name-8| 15|         1002|\n",
      "|name-0| 46|         1001|\n",
      "|name-5| 91|         1002|\n",
      "|name-4| 42|         1004|\n",
      "|name-0| 88|         1001|\n",
      "|name-8| 81|         1004|\n",
      "|name-1| 42|         1002|\n",
      "|name-6| 59|         1004|\n",
      "|name-7|  8|         1002|\n",
      "|name-3| 61|         NULL|\n",
      "|name-4| 14|         1002|\n",
      "|name-5| 80|         1004|\n",
      "|name-0| 16|         1000|\n",
      "|name-6| 10|         1003|\n",
      "|name-3| 17|         1004|\n",
      "|name-7|  3|         1000|\n",
      "|name-5| 88|         1001|\n",
      "|name-0| 59|         1002|\n",
      "|name-4| 85|         1000|\n",
      "+------+---+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "sort or orderby:\n",
      "+---+------+----+--------+\n",
      "| id|  name| age|district|\n",
      "+---+------+----+--------+\n",
      "|  0|name-0|  88|    1001|\n",
      "| 10|name-0|  59|    1002|\n",
      "| 70|name-0|  56|    1001|\n",
      "| 40|name-0|  46|    1001|\n",
      "| 50|name-0|  44|    1002|\n",
      "| 80|name-0|  41|    1004|\n",
      "| 60|name-0|  16|    1000|\n",
      "| 90|name-0|   4|    1004|\n",
      "| 20|name-0|NULL|    1000|\n",
      "| 30|name-0|NULL|    1000|\n",
      "| 61|name-1|  84|    1000|\n",
      "| 71|name-1|  55|    1002|\n",
      "| 31|name-1|  51|    1000|\n",
      "| 41|name-1|  42|    1002|\n",
      "| 21|name-1|  40|    1002|\n",
      "| 51|name-1|  36|    1003|\n",
      "| 91|name-1|  35|    1003|\n",
      "|  1|name-1|  25|    1003|\n",
      "| 81|name-1|  19|    1002|\n",
      "| 11|name-1|   8|    NULL|\n",
      "+---+------+----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+------+----+--------+\n",
      "| id|  name| age|district|\n",
      "+---+------+----+--------+\n",
      "|  0|name-0|  88|    1001|\n",
      "| 10|name-0|  59|    1002|\n",
      "| 70|name-0|  56|    1001|\n",
      "| 40|name-0|  46|    1001|\n",
      "| 50|name-0|  44|    1002|\n",
      "| 80|name-0|  41|    1004|\n",
      "| 60|name-0|  16|    1000|\n",
      "| 90|name-0|   4|    1004|\n",
      "| 20|name-0|NULL|    1000|\n",
      "| 30|name-0|NULL|    1000|\n",
      "| 61|name-1|  84|    1000|\n",
      "| 71|name-1|  55|    1002|\n",
      "| 31|name-1|  51|    1000|\n",
      "| 41|name-1|  42|    1002|\n",
      "| 21|name-1|  40|    1002|\n",
      "| 51|name-1|  36|    1003|\n",
      "| 91|name-1|  35|    1003|\n",
      "|  1|name-1|  25|    1003|\n",
      "| 81|name-1|  19|    1002|\n",
      "| 11|name-1|   8|    NULL|\n",
      "+---+------+----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+------+----+--------+\n",
      "| id|  name| age|district|\n",
      "+---+------+----+--------+\n",
      "|  0|name-0|  88|    1001|\n",
      "| 10|name-0|  59|    1002|\n",
      "| 70|name-0|  56|    1001|\n",
      "| 40|name-0|  46|    1001|\n",
      "| 50|name-0|  44|    1002|\n",
      "| 80|name-0|  41|    1004|\n",
      "| 60|name-0|  16|    1000|\n",
      "| 90|name-0|   4|    1004|\n",
      "| 20|name-0|NULL|    1000|\n",
      "| 30|name-0|NULL|    1000|\n",
      "| 61|name-1|  84|    1000|\n",
      "| 71|name-1|  55|    1002|\n",
      "| 31|name-1|  51|    1000|\n",
      "| 41|name-1|  42|    1002|\n",
      "| 21|name-1|  40|    1002|\n",
      "| 51|name-1|  36|    1003|\n",
      "| 91|name-1|  35|    1003|\n",
      "|  1|name-1|  25|    1003|\n",
      "| 81|name-1|  19|    1002|\n",
      "| 11|name-1|   8|    NULL|\n",
      "+---+------+----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+------+----+--------+\n",
      "| id|  name| age|district|\n",
      "+---+------+----+--------+\n",
      "|  0|name-0|  88|    1001|\n",
      "| 10|name-0|  59|    1002|\n",
      "| 70|name-0|  56|    1001|\n",
      "| 40|name-0|  46|    1001|\n",
      "| 50|name-0|  44|    1002|\n",
      "| 80|name-0|  41|    1004|\n",
      "| 60|name-0|  16|    1000|\n",
      "| 90|name-0|   4|    1004|\n",
      "| 20|name-0|NULL|    1000|\n",
      "| 30|name-0|NULL|    1000|\n",
      "| 61|name-1|  84|    1000|\n",
      "| 71|name-1|  55|    1002|\n",
      "| 31|name-1|  51|    1000|\n",
      "| 41|name-1|  42|    1002|\n",
      "| 21|name-1|  40|    1002|\n",
      "| 51|name-1|  36|    1003|\n",
      "| 91|name-1|  35|    1003|\n",
      "|  1|name-1|  25|    1003|\n",
      "| 81|name-1|  19|    1002|\n",
      "| 11|name-1|   8|    NULL|\n",
      "+---+------+----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+------+----+--------+\n",
      "| id|  name| age|district|\n",
      "+---+------+----+--------+\n",
      "|  0|name-0|  88|    1001|\n",
      "| 10|name-0|  59|    1002|\n",
      "| 70|name-0|  56|    1001|\n",
      "| 40|name-0|  46|    1001|\n",
      "| 50|name-0|  44|    1002|\n",
      "| 80|name-0|  41|    1004|\n",
      "| 60|name-0|  16|    1000|\n",
      "| 90|name-0|   4|    1004|\n",
      "| 20|name-0|NULL|    1000|\n",
      "| 30|name-0|NULL|    1000|\n",
      "| 61|name-1|  84|    1000|\n",
      "| 71|name-1|  55|    1002|\n",
      "| 31|name-1|  51|    1000|\n",
      "| 41|name-1|  42|    1002|\n",
      "| 21|name-1|  40|    1002|\n",
      "| 51|name-1|  36|    1003|\n",
      "| 91|name-1|  35|    1003|\n",
      "|  1|name-1|  25|    1003|\n",
      "| 81|name-1|  19|    1002|\n",
      "| 11|name-1|   8|    NULL|\n",
      "+---+------+----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "group and sort together:\n",
      "+----+----------+\n",
      "| age|count(age)|\n",
      "+----+----------+\n",
      "|NULL|         0|\n",
      "|   1|         1|\n",
      "|   3|         1|\n",
      "|   4|         1|\n",
      "|   8|         2|\n",
      "|   9|         1|\n",
      "|  10|         3|\n",
      "|  12|         1|\n",
      "|  13|         1|\n",
      "|  14|         1|\n",
      "|  15|         2|\n",
      "|  16|         3|\n",
      "|  17|         4|\n",
      "|  18|         1|\n",
      "|  19|         3|\n",
      "|  21|         1|\n",
      "|  24|         2|\n",
      "|  25|         3|\n",
      "|  28|         1|\n",
      "|  29|         1|\n",
      "+----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "limit:\n",
      "+---+------+---+--------+\n",
      "| id|  name|age|district|\n",
      "+---+------+---+--------+\n",
      "|  3|name-3| 61|    NULL|\n",
      "|  4|name-4| 58|    1002|\n",
      "+---+------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort/orderby, limit, offset, group by, withColumnRenamed, alias\n",
    "from datetime import datetime\n",
    "import random\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "l = []\n",
    "rows = 100\n",
    "for c in range(rows):\n",
    "    l.append([c, ('name-' + str(c % (rows//10))), None if random.random() < 0.1 else random.randint(1, 100), None if random.random() < 0.1 else random.randint(1000, 1004)])\n",
    "\n",
    "df_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"district\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(l, df_schema)\n",
    "\n",
    "print(\"schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"describe:\")\n",
    "df.describe().show()\n",
    "\n",
    "print(\"explain:\")\n",
    "df.explain()\n",
    "\n",
    "print(\"group:\") # groupBy returns GroupedData\n",
    "df.groupBy(\"name\", \"age\").count().show() # GroupedData.count\n",
    "df.groupBy(df.name, df.age).count().show() # GroupedData.count\n",
    "df.groupBy(df.name, df.age).min().withColumnRenamed(\"min(id)\",\"min_id\").show() # GroupedData.min of all int columns!\n",
    "df.groupBy(df.name, df.age).agg(min(\"age\").alias(\"min_age\")).show() # GroupedData.agg(#pyspark.sql.functions aggregate functions), similar to spark sql group by / aggregate selects\n",
    "df.groupBy(df.name, df.age).agg({\"district\": \"max\"}).show() # GroupedData.agg(#dict{col, funct} of pyspark.sql.functions aggregate functions). similar to spark sql group by / aggregate selects\n",
    "\n",
    "print(\"sort or orderby:\")\n",
    "df.orderBy([\"name\", \"age\"], ascending=[1, 0]).show()\n",
    "df.orderBy([col(\"name\"), col(\"age\")], ascending=[1, 0]).show()\n",
    "df.orderBy([df.name.asc(), df.age.desc()]).show()\n",
    "\n",
    "df.sort([\"name\", \"age\"], ascending=[1,0]).show()\n",
    "df.sort([df.name.asc(), df.age.desc()]).show()\n",
    "\n",
    "print(\"group and sort together:\")\n",
    "df.groupBy(\"age\").agg(count(df.age)).sort(df.age.asc()).show() # using count function\n",
    "\n",
    "print(\"limit:\")\n",
    "df.limit(5).offset(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7bd1714-ddec-4708-aca4-c5ba216a5eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202504\n",
      "+----------+\n",
      "| from_date|\n",
      "+----------+\n",
      "|2025-01-01|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "dt = datetime.now()\n",
    "print(dt.strftime(\"%Y%m\"))\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select to_date('20250101','yyyyMMdd') from_date\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e648851a-0527-4be6-a961-ef5e14128442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan with partition filter:\n",
      "plan: == Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet spark_catalog.mytestdb.t_my_date_partitioned[id#212,name#213,age#214,district#215,from_date#216,to_date#217,to_date_year_month#218] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(5 paths)[file:/home/jovyan/work/various_tests/spark/spark-warehouse/mytestdb.db..., PartitionFilters: [isnotnull(to_date_year_month#218), (to_date_year_month#218 >= 202710)], PushedFilters: [], ReadSchema: struct<id:int,name:string,age:int,district:int,from_date:date,to_date:date>\n",
      "\n",
      "\n",
      "\n",
      "plan with partition filter and extra filter DOES NOT WORK!!!:\n",
      "plan: == Physical Plan ==\n",
      "LocalTableScan <empty>, [id#212, age#214]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show explain plan\n",
    "def print_dict(d: dict, level: int = 0) -> None:\n",
    "\n",
    "    sp = \"\".ljust(level * 3)\n",
    "    for k in d.keys():\n",
    "        print(f\"{sp}{k}: {d[k]}\")\n",
    "        \n",
    "        if type(d[k]) is dict:\n",
    "            print_dict(d[k], level + 1)\n",
    "            \n",
    "    if level == 0:\n",
    "        print()\n",
    "\n",
    "print(\"plan with partition filter:\")\n",
    "plan = spark.sql(\"\"\"\n",
    "explain\n",
    "select *\n",
    "FROM t_my_date_partitioned t\n",
    "WHERE 1 = 1\n",
    "AND t.to_date_year_month >= to_number('202710','000000')\n",
    "\"\"\")\n",
    "\n",
    "for row in plan.collect():\n",
    "    None\n",
    "    print_dict(row.asDict())\n",
    "\n",
    "#####################################################################\n",
    "print(\"plan with partition filter and extra filter DOES NOT WORK!!!:\")\n",
    "plan = spark.sql(\"\"\"\n",
    "explain\n",
    "select t.id, t.age\n",
    "FROM t_my_date_partitioned t\n",
    "WHERE 1 = 1\n",
    "AND to_date_year_month >= 202710\n",
    "AND t.from_date < to_date('202710','yyyyMMdd')\n",
    "AND t.to_date > to_date('202710','yyyyMMdd')\n",
    "\"\"\")\n",
    "\n",
    "for row in plan.collect():\n",
    "    print_dict(row.asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "483359bf-76e3-4fcf-8860-463770e64ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating list: \n",
      "Done in 0:00:00.000978\n",
      "creating DF from list: \n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- district: integer (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      " |-- to_date_year_month: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+--------+----------+----------+------------------+\n",
      "| id|  name| age|district| from_date|   to_date|to_date_year_month|\n",
      "+---+------+----+--------+----------+----------+------------------+\n",
      "|  0|name-0|  72|    1001|2026-06-29|2026-08-15|            202608|\n",
      "|  1|name-1|  77|    1002|2025-06-24|2025-09-15|            202509|\n",
      "|  2|name-2|  90|    1003|2027-05-18|2027-06-27|            202706|\n",
      "|  3|name-3|NULL|    1004|2027-02-17|2027-05-01|            202705|\n",
      "|  4|name-4|  66|    1000|2025-12-03|2025-12-21|            202512|\n",
      "+---+------+----+--------+----------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Done in 0:00:01.187049\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "| mytestdb|\n",
      "+---------+\n",
      "\n",
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|          mytestdb|\n",
      "+------------------+\n",
      "\n",
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "| mytestdb|t_my_date_partiti...|      false|\n",
      "| mytestdb|v_my_date_partiti...|      false|\n",
      "| mytestdb|v_my_date_partiti...|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n",
      "+---------+--------------------+-----------+\n",
      "|namespace|            viewName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "| mytestdb|v_my_date_partiti...|      false|\n",
      "| mytestdb|v_my_date_partiti...|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n",
      "Dropping table/view...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/05 19:11:14 WARN DropTableCommand: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `t_my_date_partitioned` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
      "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
      "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 4 pos 5\n",
      "org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `t_my_date_partitioned` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
      "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
      "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 4 pos 5\n",
      "\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveViews(Analyzer.scala:1102)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveViews(Analyzer.scala:1105)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.$anonfun$applyOrElse$76(Analyzer.scala:1153)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1153)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\n",
      "\tat org.apache.spark.sql.SparkSession.table(SparkSession.scala:606)\n",
      "\tat org.apache.spark.sql.execution.command.DropTableCommand.run(ddl.scala:251)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/05 19:11:14 WARN DropTableCommand: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `t_my_date_partitioned` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
      "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
      "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 4 pos 5\n",
      "org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `t_my_date_partitioned` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
      "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
      "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 4 pos 5\n",
      "\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveViews(Analyzer.scala:1102)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveViews(Analyzer.scala:1105)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.$anonfun$applyOrElse$76(Analyzer.scala:1153)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1153)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\n",
      "\tat org.apache.spark.sql.SparkSession.table(SparkSession.scala:606)\n",
      "\tat org.apache.spark.sql.execution.command.DropTableCommand.run(ddl.scala:251)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/05 19:11:16 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/04/05 19:11:17 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "25/04/05 19:11:17 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/04/05 19:11:17 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying the table...\n",
      "+----------+----------+---+\n",
      "| from_date|   to_date|cnt|\n",
      "+----------+----------+---+\n",
      "|2025-04-14|2025-05-21|  1|\n",
      "|2025-04-30|2025-07-23|  1|\n",
      "|2025-05-13|2025-08-12|  1|\n",
      "|2025-05-16|2025-06-07|  1|\n",
      "|2025-05-17|2025-07-07|  1|\n",
      "+----------+----------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "Creating simple view...\n",
      "Creating simple view...\n",
      "Querying the view...\n",
      "Date to query: 20250601\n",
      "+---+-------+---+--------+----------+----------+------------------+\n",
      "|id |name   |age|district|from_date |to_date   |to_date_year_month|\n",
      "+---+-------+---+--------+----------+----------+------------------+\n",
      "|72 |name-72|79 |1004    |2025-04-30|2025-07-23|202507            |\n",
      "|59 |name-59|34 |1002    |2025-05-13|2025-08-12|202508            |\n",
      "|7  |name-7 |26 |1003    |2025-05-16|2025-06-07|202506            |\n",
      "|69 |name-69|65 |1001    |2025-05-17|2025-07-07|202507            |\n",
      "+---+-------+---+--------+----------+----------+------------------+\n",
      "\n",
      "Dropping table/view...\n",
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "| mytestdb|t_my_date_partiti...|      false|\n",
      "| mytestdb|v_my_date_partiti...|      false|\n",
      "| mytestdb|v_my_date_partiti...|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n",
      "+---------+--------------------+-----------+\n",
      "|namespace|            viewName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "| mytestdb|v_my_date_partiti...|      false|\n",
      "| mytestdb|v_my_date_partiti...|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# partitioning (hash, range, round-robin) test\n",
    "# CANNOT FIND A WAY to PARTITION a date (to_date) into date ranges!!!\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, TimestampType\n",
    "import shutil\n",
    "\n",
    "# creating large DF:\n",
    "print(\"Creating list: \")\n",
    "ct = datetime.now()\n",
    "l = []\n",
    "rows = 100\n",
    "for c in range(rows):\n",
    "    from_date = datetime.now() + timedelta(days = random.randint(0, 1000))\n",
    "    to_date = from_date + timedelta(days = random.randint(1, 100))\n",
    "    l.append([c, \n",
    "              ('name-' + str(c)),  \n",
    "              None if random.random() < 0.1 else random.randint(1, 100),  \n",
    "              None if random.random() < 0.1 else random.randint(1000, 1004),\n",
    "              from_date,\n",
    "              to_date,\n",
    "              int(to_date.strftime(\"%Y%m\")),\n",
    "             ])\n",
    "\n",
    "df_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"district\", IntegerType(), True),\n",
    "    StructField(\"from_date\", DateType(), True),\n",
    "    StructField(\"to_date\", DateType(), True),\n",
    "    # tech columns\n",
    "    StructField(\"to_date_year_month\", IntegerType(), True),\n",
    "])\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "print(\"creating DF from list: \")\n",
    "ct = datetime.now()\n",
    "df = spark.createDataFrame(l, df_schema)\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "spark.sql(\"create database if not exists mytestdb\")\n",
    "spark.sql(\"Show databases\").show()\n",
    "spark.sql(\"use mytestdb\")\n",
    "spark.sql(\"select current_database()\").show()\n",
    "spark.sql(\"Show tables\").show()\n",
    "spark.sql(\"Show views\").show()\n",
    "\n",
    "# Drop table\n",
    "print(\"Dropping table/view...\")\n",
    "spark.sql('drop table if exists t_my_date_partitioned')\n",
    "spark.sql('drop view if exists v_my_date_partitioned_simple')\n",
    "spark.sql('drop view if exists v_my_date_partitioned')\n",
    "\n",
    "# save as managed \n",
    "print(\"Saving table...\")\n",
    "(\n",
    "    df.write\n",
    "    .option(\"header\", True)\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"to_date_year_month\") \n",
    "    .format(\"parquet\") # parquet, csv for testing / readability\n",
    "    .saveAsTable(\"t_my_date_partitioned\")\n",
    ")\n",
    "\n",
    "# Querying table\n",
    "print(\"Querying the table...\")\n",
    "spark.sql(\"\"\"\n",
    "select from_date, to_date, count(1) cnt\n",
    "from t_my_date_partitioned\n",
    "group by from_date, to_date\n",
    "order by from_date, to_date nulls first\n",
    "\"\"\").show(5)\n",
    "\n",
    "# create simple view WITHOUT constraint on partition column\n",
    "print(\"Creating simple view...\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW v_my_date_partitioned_simple AS\n",
    "SELECT id, name, age, district, \n",
    "        from_date, to_date,\n",
    "        to_date_year_month\n",
    "FROM t_my_date_partitioned t\n",
    "\"\"\")\n",
    "\n",
    "# create view WITH constraint on partition column\n",
    "print(\"Creating simple view...\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW v_my_date_partitioned AS\n",
    "SELECT id, name, age, district, \n",
    "        from_date, to_date,\n",
    "        to_date_year_month\n",
    "FROM t_my_date_partitioned t\n",
    "WHERE 1 = 1\n",
    "AND t.to_date_year_month = to_number(date_format(t.to_date, 'yyyyMM'), '000000')\n",
    "\"\"\")\n",
    "\n",
    "# inserting into view IS NOT SUPPORTED!!!...\n",
    "\n",
    "\n",
    "# querying the view without specifying the technical column. partition key is NOT really getting used based on the explain plan above\n",
    "print(\"Querying the view...\")\n",
    "dt_str = '20250601'\n",
    "print(f\"Date to query: {dt_str}\") # create string from date\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "select *\n",
    "FROM v_my_date_partitioned v\n",
    "WHERE 1 = 1\n",
    "AND v.from_date <= to_date({dt_str},'yyyyMMdd')\n",
    "AND v.to_date > to_date({dt_str},'yyyyMMdd')\n",
    "ORDER BY from_date, to_date DESC NULLS FIRST\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "\n",
    "# Drop table\n",
    "print(\"Dropping table/view...\")\n",
    "#spark.sql('drop table if exists t_my_date_partitioned')\n",
    "#spark.sql('drop view if exists v_my_date_partitioned_simple')\n",
    "#spark.sql('drop view if exists v_my_date_partitioned')\n",
    "\n",
    "spark.sql(\"Show tables\").show()\n",
    "spark.sql(\"Show views\").show()\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
