{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3d78f0d-3078-4435-8f56-ecc263213f5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active spark session not found\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/12/10 11:20:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark version: 3.5.5\nDone.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\nException occurred during processing of request from ('127.0.0.1', 45884)\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n    self.process_request(request, client_address)\n  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n    self.finish_request(request, client_address)\n  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n    self.RequestHandlerClass(request, client_address, self)\n  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n    self.handle()\n  File \"/opt/conda/lib/python3.11/site-packages/pyspark/accumulators.py\", line 295, in handle\n    poll(accum_updates)\n  File \"/opt/conda/lib/python3.11/site-packages/pyspark/accumulators.py\", line 267, in poll\n    if self.rfile in r and func():\n                           ^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n    num_updates = read_int(self.rfile)\n                  ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n    raise EOFError\nEOFError\n----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# setup spark on linux\n",
    "from pyspark.sql import *\n",
    "\n",
    "if SparkSession.getActiveSession() == None:\n",
    "    print(\"Active spark session not found\")\n",
    "else:\n",
    "    print(\"Active spark session found\")\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         #.master(\"local\") # local- no parallelizm at all, local[2] - 2 cores, local[*] - as many cores as local logical cores\n",
    "         .appName(\"SparkSession#1\")\n",
    "         .enableHiveSupport() # enableHiveSupport() needed to make data persistent... \n",
    "         #.config(\"spark.driver.allowMultipleContexts\", \"true\")\n",
    "         .config(\"spark.sql.cbo.enabled\", \"true\")\n",
    "         .config(\"spark.sql.cbo.optimizer\", \"true\")\n",
    "         .getOrCreate())\n",
    "\n",
    "print('spark version:', spark.version)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cda75d28-8ce3-4d56-b711-ddebaba71428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#print(list(locals().keys()))\n",
    "#del cust\n",
    "print(random.randint(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc6e5230-22c5-4d81-bd62-e4ac35eb9753",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+\n| id|      name|nationality|\n+---+----------+-----------+\n|  0|cust_00000|         CH|\n|  1|cust_00001|         CH|\n|  2|cust_00002|         AT|\n|  3|cust_00003|         DE|\n|  4|cust_00004|         CH|\n|  5|cust_00005|         HU|\n|  6|cust_00006|         DE|\n|  7|cust_00007|         CH|\n|  8|cust_00008|         DE|\n|  9|cust_00009|         HU|\n+---+----------+-----------+\n\n+---+------------+------------+------+--------+-------------------+\n| id|orig_cust_id|dest_cust_id|amount|currency|  payment_timestamp|\n+---+------------+------------+------+--------+-------------------+\n|  0|           5|           3|   718|     CHF|2022-06-19 00:00:00|\n|  1|           5|           3|   889|     CHF|2020-04-28 00:00:00|\n|  2|           8|           4|   916|     CHF|               NULL|\n|  3|           6|           6|   103|     HUF|               NULL|\n|  4|           0|           9|   996|     CHF|               NULL|\n|  5|           3|           7|   144|     HUF|               NULL|\n|  6|           5|           2|   118|     CHF|               NULL|\n|  7|           3|           4|   719|     CHF|               NULL|\n|  8|           8|           2|   439|     EUR|2021-12-27 00:00:00|\n|  9|           3|           7|   904|     CHF|               NULL|\n| 10|           4|           1|   784|     HUF|               NULL|\n| 11|           5|           8|   605|     CHF|2022-09-08 00:00:00|\n| 12|           8|           2|   312|     EUR|               NULL|\n| 13|           0|           4|   691|     CHF|2020-05-06 00:00:00|\n| 14|           0|           7|   972|     CHF|               NULL|\n| 15|           0|           9|   796|     CHF|               NULL|\n| 16|           3|           9|   184|     EUR|2022-01-10 00:00:00|\n| 17|           8|           9|   391|     CHF|               NULL|\n| 18|           2|           9|   654|     CHF|               NULL|\n| 19|           4|           2|   257|     HUF|2022-04-11 00:00:00|\n+---+------------+------------+------+--------+-------------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "max_cust = 10\n",
    "max_days=1000\n",
    "\n",
    "# df_customer(id, name, nationality)\n",
    "l_data = []\n",
    "for c in range(max_cust):\n",
    "    l_data.append([c, 'cust_' + f'{c}'.rjust(5,'0'), random.choice(['AT', 'CH', 'DE', 'HU'])])\n",
    "        \n",
    "df_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"nationality\", StringType(), True),\n",
    "])\n",
    "\n",
    "df_customer = spark.createDataFrame(l_data, df_schema)\n",
    "df_customer.show()\n",
    "\n",
    "# df_payment(id, orig_cust_id, dest_cust_id, amount, currency, payment_timestamp)\n",
    "l_data = []\n",
    "for c in range(100):\n",
    "    l_data.append([\n",
    "        c, # id\n",
    "        random.randint(0, max_cust-1),  # orig_cust_id\n",
    "        random.randint(0, max_cust-1),  # dest_cust_id\n",
    "        random.randint(1, 1000),  # amount\n",
    "        random.choice(['EUR', 'CHF', 'HUF']), # currency\n",
    "        datetime.strptime('20200101','%Y%m%d') + timedelta(days=random.randint(0, max_days)) if random.randint(0,1) == 1 else None, # payment_timestamp\n",
    "    ])\n",
    "\n",
    "df_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"orig_cust_id\", IntegerType(), False),\n",
    "    StructField(\"dest_cust_id\", IntegerType(), False),\n",
    "    StructField(\"amount\", IntegerType(), False),\n",
    "    StructField(\"currency\", StringType(), False),\n",
    "    StructField(\"payment_timestamp\", TimestampType(), True),\n",
    "])\n",
    "\n",
    "df_payment = spark.createDataFrame(l_data, df_schema)\n",
    "df_payment.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7749e88-eb37-497f-8dc8-e978fc74d977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+------+--------+-------------------+------------------------+\n| id|orig_cust_id|dest_cust_id|amount|currency|  payment_timestamp|payment_timestamp_filled|\n+---+------------+------------+------+--------+-------------------+------------------------+\n|  0|           5|           3|   718|     CHF|2022-06-19 00:00:00|     2022-06-19 00:00:00|\n|  1|           5|           3|   889|     CHF|2020-04-28 00:00:00|     2020-04-28 00:00:00|\n|  2|           8|           4|   916|     CHF|               NULL|     2019-01-01 00:00:00|\n|  3|           6|           6|   103|     HUF|               NULL|     2019-01-01 00:00:00|\n|  4|           0|           9|   996|     CHF|               NULL|     2019-01-01 00:00:00|\n|  5|           3|           7|   144|     HUF|               NULL|     2019-01-01 00:00:00|\n|  6|           5|           2|   118|     CHF|               NULL|     2019-01-01 00:00:00|\n|  7|           3|           4|   719|     CHF|               NULL|     2019-01-01 00:00:00|\n|  8|           8|           2|   439|     EUR|2021-12-27 00:00:00|     2021-12-27 00:00:00|\n|  9|           3|           7|   904|     CHF|               NULL|     2019-01-01 00:00:00|\n+---+------------+------------+------+--------+-------------------+------------------------+\nonly showing top 10 rows\n+---+------------+------------+------+--------+-------------------+------------------------+-------------+\n| id|orig_cust_id|dest_cust_id|amount|currency|  payment_timestamp|payment_timestamp_filled|payment_month|\n+---+------------+------------+------+--------+-------------------+------------------------+-------------+\n|  0|           5|           3|   718|     CHF|2022-06-19 00:00:00|     2022-06-19 00:00:00|       202206|\n|  1|           5|           3|   889|     CHF|2020-04-28 00:00:00|     2020-04-28 00:00:00|       202004|\n|  2|           8|           4|   916|     CHF|               NULL|     2019-01-01 00:00:00|       201901|\n|  3|           6|           6|   103|     HUF|               NULL|     2019-01-01 00:00:00|       201901|\n|  4|           0|           9|   996|     CHF|               NULL|     2019-01-01 00:00:00|       201901|\n|  5|           3|           7|   144|     HUF|               NULL|     2019-01-01 00:00:00|       201901|\n|  6|           5|           2|   118|     CHF|               NULL|     2019-01-01 00:00:00|       201901|\n|  7|           3|           4|   719|     CHF|               NULL|     2019-01-01 00:00:00|       201901|\n|  8|           8|           2|   439|     EUR|2021-12-27 00:00:00|     2021-12-27 00:00:00|       202112|\n|  9|           3|           7|   904|     CHF|               NULL|     2019-01-01 00:00:00|       201901|\n+---+------------+------------+------+--------+-------------------+------------------------+-------------+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# some transformation, normalization, standardization, DQ improvements\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# add missing values\n",
    "fixed_timestamp_str = \"2019-01-01 00:00:00\"\n",
    "df_payment_filled = df_payment.withColumn('payment_timestamp_filled', \n",
    "                                          F\n",
    "                                          .when(F.col('payment_timestamp').isNull(), F.lit(fixed_timestamp_str).cast(\"timestamp\"))\n",
    "                                          .otherwise(F.col('payment_timestamp'))\n",
    "                                         )\n",
    "df_payment_filled.show(10)\n",
    "    \n",
    "# add payment_month\n",
    "df_payment_filled_ym=df_payment_filled.withColumn('payment_month', F.date_format(F.col('payment_timestamp_filled'), 'yyyyMM'))\n",
    "df_payment_filled_ym.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2846a36a-bed6-4d6c-bd26-04b2e2ae1e4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+---+------------+------------+------+--------+-------------------+------------------------+-------------+\n|id |name      |nationality|id |orig_cust_id|dest_cust_id|amount|currency|payment_timestamp  |payment_timestamp_filled|payment_month|\n+---+----------+-----------+---+------------+------------+------+--------+-------------------+------------------------+-------------+\n|0  |cust_00000|CH         |94 |0           |5           |419   |CHF     |2021-10-21 00:00:00|2021-10-21 00:00:00     |202110       |\n|0  |cust_00000|CH         |84 |0           |1           |842   |CHF     |2020-01-19 00:00:00|2020-01-19 00:00:00     |202001       |\n|0  |cust_00000|CH         |77 |0           |4           |762   |EUR     |2020-12-21 00:00:00|2020-12-21 00:00:00     |202012       |\n|0  |cust_00000|CH         |72 |0           |2           |357   |CHF     |NULL               |2019-01-01 00:00:00     |201901       |\n|0  |cust_00000|CH         |57 |0           |7           |421   |CHF     |2020-08-07 00:00:00|2020-08-07 00:00:00     |202008       |\n|0  |cust_00000|CH         |55 |0           |8           |845   |EUR     |NULL               |2019-01-01 00:00:00     |201901       |\n|0  |cust_00000|CH         |53 |0           |1           |182   |CHF     |2021-06-24 00:00:00|2021-06-24 00:00:00     |202106       |\n|0  |cust_00000|CH         |51 |0           |7           |66    |CHF     |NULL               |2019-01-01 00:00:00     |201901       |\n|0  |cust_00000|CH         |15 |0           |9           |796   |CHF     |NULL               |2019-01-01 00:00:00     |201901       |\n|0  |cust_00000|CH         |14 |0           |7           |972   |CHF     |NULL               |2019-01-01 00:00:00     |201901       |\n+---+----------+-----------+---+------------+------------+------+--------+-------------------+------------------------+-------------+\nonly showing top 10 rows\n+---+-------------+----------+\n|id |payment_month|sum_amount|\n+---+-------------+----------+\n|0  |201901       |4032      |\n|0  |202001       |842       |\n|0  |202005       |691       |\n|0  |202008       |421       |\n|0  |202012       |762       |\n|0  |202106       |182       |\n|0  |202110       |419       |\n|1  |201901       |2304      |\n|1  |202103       |346       |\n|4  |201901       |4338      |\n+---+-------------+----------+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# q1: total monthly payment per customer (for each) which are CH nationals\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# apply filter\n",
    "df_customer_ch = df_customer.filter(F.col('nationality') == 'CH')\n",
    "\n",
    "# join \n",
    "df_joined = df_customer_ch.alias('c').join(df_payment_filled_ym.alias('p'), on=F.col('c.id') == F.col('p.orig_cust_id'), how='left')\n",
    "df_joined.show(10, truncate=False)\n",
    "\n",
    "# aggregate, sum, order, round\n",
    "df_result = (\n",
    "        df_joined\n",
    "        .groupBy(F.col('c.id'), F.col('p.payment_month'))\n",
    "        .agg(F.round(F.sum(F.col('p.amount')),1).alias('sum_amount'))\n",
    "        .orderBy(F.col('c.id'), F.col('p.payment_month'))\n",
    ")\n",
    "df_result.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77cbdd90-5fed-4844-9836-ca8ec30f9e2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+------+--------+-------------------+------------------------+-------------+---+----------+-----------+---+----------+-----------+\n|id |orig_cust_id|dest_cust_id|amount|currency|payment_timestamp  |payment_timestamp_filled|payment_month|id |name      |nationality|id |name      |nationality|\n+---+------------+------------+------+--------+-------------------+------------------------+-------------+---+----------+-----------+---+----------+-----------+\n|10 |4           |1           |784   |HUF     |NULL               |2019-01-01 00:00:00     |201901       |4  |cust_00004|CH         |1  |cust_00001|CH         |\n|13 |0           |4           |691   |CHF     |2020-05-06 00:00:00|2020-05-06 00:00:00     |202005       |0  |cust_00000|CH         |4  |cust_00004|CH         |\n|14 |0           |7           |972   |CHF     |NULL               |2019-01-01 00:00:00     |201901       |0  |cust_00000|CH         |7  |cust_00007|CH         |\n|33 |1           |7           |346   |EUR     |2021-03-20 00:00:00|2021-03-20 00:00:00     |202103       |1  |cust_00001|CH         |7  |cust_00007|CH         |\n|35 |4           |7           |807   |EUR     |2022-05-16 00:00:00|2022-05-16 00:00:00     |202205       |4  |cust_00004|CH         |7  |cust_00007|CH         |\n|39 |1           |4           |614   |CHF     |NULL               |2019-01-01 00:00:00     |201901       |1  |cust_00001|CH         |4  |cust_00004|CH         |\n|47 |4           |1           |828   |EUR     |NULL               |2019-01-01 00:00:00     |201901       |4  |cust_00004|CH         |1  |cust_00001|CH         |\n|51 |0           |7           |66    |CHF     |NULL               |2019-01-01 00:00:00     |201901       |0  |cust_00000|CH         |7  |cust_00007|CH         |\n|53 |0           |1           |182   |CHF     |2021-06-24 00:00:00|2021-06-24 00:00:00     |202106       |0  |cust_00000|CH         |1  |cust_00001|CH         |\n|57 |0           |7           |421   |CHF     |2020-08-07 00:00:00|2020-08-07 00:00:00     |202008       |0  |cust_00000|CH         |7  |cust_00007|CH         |\n+---+------------+------------+------+--------+-------------------+------------------------+-------------+---+----------+-----------+---+----------+-----------+\nonly showing top 10 rows\n+------------+------------+----------+\n|orig_cust_id|dest_cust_id|avg_amount|\n+------------+------------+----------+\n|0           |1           |512.0     |\n|0           |4           |726.5     |\n|0           |7           |486.3     |\n|1           |1           |372.0     |\n|1           |4           |614.0     |\n|1           |7           |346.0     |\n|4           |1           |806.0     |\n|4           |7           |807.0     |\n|7           |1           |192.5     |\n|7           |4           |398.7     |\n+------------+------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# q2: average payment per orig/dest for customers which are CH nationals\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_customer_ch = df_customer.filter(F.col('nationality') == 'CH')\n",
    "\n",
    "df_joined = (\n",
    "        df_payment_filled_ym.alias('p')\n",
    "        .join(df_customer_ch.alias('co'), on=F.col('co.id') == F.col('p.orig_cust_id'), how='inner')\n",
    "        .join(df_customer_ch.alias('cd'), on=F.col('cd.id') == F.col('p.dest_cust_id'), how='inner')\n",
    ")\n",
    "df_joined.show(10, truncate=False)\n",
    "\n",
    "df_result = (\n",
    "        df_joined\n",
    "        .groupBy(F.col('p.orig_cust_id'), F.col('p.dest_cust_id'))\n",
    "        .agg(F.round(F.avg(F.col('p.amount')),1).alias('avg_amount'))\n",
    "        .orderBy(F.col('p.orig_cust_id'), F.col('p.dest_cust_id'))\n",
    ")\n",
    "df_result.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06427d10-bbaf-400a-9c10-150303fab934",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+----------+\n|orig_cust_id|dest_cust_id|avg_amount|\n+------------+------------+----------+\n|0           |1           |512.0     |\n|0           |4           |726.5     |\n|0           |7           |486.3     |\n|1           |1           |372.0     |\n|1           |4           |614.0     |\n|1           |7           |346.0     |\n|4           |1           |806.0     |\n|4           |7           |807.0     |\n|7           |1           |192.5     |\n|7           |4           |398.7     |\n+------------+------------+----------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>orig_cust_id</th><th>dest_cust_id</th><th>avg_amount</th></tr></thead><tbody><tr><td>0</td><td>1</td><td>512.0</td></tr><tr><td>0</td><td>4</td><td>726.5</td></tr><tr><td>0</td><td>7</td><td>486.3</td></tr><tr><td>1</td><td>1</td><td>372.0</td></tr><tr><td>1</td><td>4</td><td>614.0</td></tr><tr><td>1</td><td>7</td><td>346.0</td></tr><tr><td>4</td><td>1</td><td>806.0</td></tr><tr><td>4</td><td>7</td><td>807.0</td></tr><tr><td>7</td><td>1</td><td>192.5</td></tr><tr><td>7</td><td>4</td><td>398.7</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         1,
         512.0
        ],
        [
         0,
         4,
         726.5
        ],
        [
         0,
         7,
         486.3
        ],
        [
         1,
         1,
         372.0
        ],
        [
         1,
         4,
         614.0
        ],
        [
         1,
         7,
         346.0
        ],
        [
         4,
         1,
         806.0
        ],
        [
         4,
         7,
         807.0
        ],
        [
         7,
         1,
         192.5
        ],
        [
         7,
         4,
         398.7
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "orig_cust_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "dest_cust_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "avg_amount",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Q3: achieve Q2 with Spark SQL not PySpark\n",
    "\n",
    "# set up the temp views for the SQL\n",
    "df_customer.createOrReplaceTempView('customer')\n",
    "df_payment.createOrReplaceTempView('payment')\n",
    "\n",
    "df_result_sql=spark.sql('''\n",
    "WITH customer_ch as\n",
    "(\n",
    "  SELECT *\n",
    "  FROM customer c\n",
    "  WHERE c.nationality = 'CH'\n",
    ")\n",
    "SELECT orig_cust_id, dest_cust_id,\n",
    "    round(avg(p.amount),1) as avg_amount\n",
    "FROM payment p\n",
    "    inner join\n",
    "    customer_ch co\n",
    "    on (p.orig_cust_id = co.id)\n",
    "    inner join\n",
    "    customer_ch cd\n",
    "    on (p.dest_cust_id = cd.id)\n",
    "GROUP BY orig_cust_id, dest_cust_id\n",
    "ORDER BY orig_cust_id, dest_cust_id\n",
    "''')\n",
    "\n",
    "df_result_sql.show(10, truncate=False)\n",
    "display(df_result_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e110d821-3c5c-4a06-a21b-c450f1110ed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write out\n",
    "df_result.write.format('parquet').mode('overwrite').save('my_data')\n",
    "df_result.coalesce(1).write.format('csv').option('header', 'true').mode('overwrite').save('my_data1')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "UBS tech_interview",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}