{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb282954-29f5-4b42-a5b6-ad07f63b4d44",
   "metadata": {},
   "source": [
    "## https://medium.com/@ansabiqbal/delta-lake-introduction-with-examples-using-pyspark-cb2a0d7a549d\n",
    "\n",
    "### https://docs.delta.io/latest/delta-intro.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d972dfe3-fd97-43e7-868b-baa4dd6ffde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install delta-spark\n",
    "#!pip show delta-spark\n",
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59942f89-f8cd-4fad-98c4-0c382698399c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active spark session not found\n",
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a6fa9ffd-3a73-4c32-86b3-06523758251b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.3.0 in central\n",
      "\tfound io.delta#delta-storage;3.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 194ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a6fa9ffd-3a73-4c32-86b3-06523758251b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/6ms)\n",
      "25/10/02 14:37:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from SQL:\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n",
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|           default|\n",
      "+------------------+\n",
      "\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n",
      "+---------+--------+-----------+\n",
      "|namespace|viewName|isTemporary|\n",
      "+---------+--------+-----------+\n",
      "+---------+--------+-----------+\n",
      "\n",
      "from python:\n",
      "[Database(name='default', catalog='spark_catalog', description='default database', locationUri='file:/home/jovyan/work/various_tests/spark/delta_tests/spark-warehouse')]\n",
      "default\n",
      "[]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#  Create a spark session with Delta\n",
    "\n",
    "import pyspark\n",
    "from delta import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#help(delta)\n",
    "\n",
    "if SparkSession.getActiveSession() == None:\n",
    "    print(\"Active spark session not found\")\n",
    "else:\n",
    "    print(\"Active spark session found\")\n",
    "\n",
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder\n",
    "    #.master('local') # local- no parallelizm at all, local[2] - 2 cores, local[*] - as many cores as local logical cores\n",
    "    .appName(\"DeltaTutorial\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")  # to allow deltatable.vacuum(0)\n",
    ")\n",
    "\n",
    "# Create spark context\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate() # function of the delta module\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# check spark-warehouse databases & tables\n",
    "print(\"from SQL:\")\n",
    "spark.sql(\"Show databases\").show()\n",
    "spark.sql(\"select current_database()\").show()\n",
    "#spark.sql(\"use mytestdb\")\n",
    "spark.sql(\"Show tables\").show()\n",
    "spark.sql(\"Show views\").show()\n",
    "\n",
    "# same as\n",
    "print(\"from python:\")\n",
    "print(spark.catalog.listDatabases())\n",
    "print(spark.catalog.currentDatabase())\n",
    "#spark.catalog.setCurrentDatabase('mytestdb')\n",
    "print(spark.catalog.listTables())\n",
    "\n",
    "#spark.catalog.listColumns('t1')\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd1e864e-0c2f-4fbb-ab09-80da5b3ecd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size(data/delta-table): 41060\n"
     ]
    }
   ],
   "source": [
    "# get the size (in bytes) of a directory\n",
    "\n",
    "import os\n",
    "\n",
    "def get_size(path: str) -> int:    \n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            if not os.path.islink(fp):\n",
    "                total_size += os.path.getsize(fp)\n",
    "                \n",
    "    print(f\"Size({path}): {total_size}\")\n",
    "\n",
    "get_size(\"data/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb46fc8-cca8-4a0c-bfcd-5b63430bb5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data (100 rows)...\n",
      "dropping old table files if exists...\n",
      "creating delta table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:============================================>            (39 + 4) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a spark dataframe and write as a delta table to an UNMANAGED!! database (file system folder)\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "df_data = []\n",
    "rows = 100\n",
    "print(f\"Generating data ({rows} rows)...\")\n",
    "for c in range(rows):\n",
    "    df_data.append([\n",
    "        c,\n",
    "        None if random.random() < 0.1 else \"firstname-\" + str(random.randint(1, 20)).zfill(3), \n",
    "        None if random.random() < 0.1 else \"lastname-\" + str(random.randint(1, 20)).zfill(3), \n",
    "        None if random.random() < 0.1 else \"house-\" + str(random.randint(1, 20)).zfill(3), \n",
    "        None if random.random() < 0.1 else \"location-\" + str(random.randint(1, 20)).zfill(3), \n",
    "        None if random.random() < 0.1 else random.randint(1, 99)\n",
    "        ])\n",
    "\n",
    "df_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"firstname\", StringType(), True),\n",
    "    StructField(\"lastname\", StringType(), True),\n",
    "    StructField(\"house\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# drop any delta table files...\n",
    "print(\"dropping old table files if exists...\")\n",
    "shutil.rmtree(\"data/delta-table\", ignore_errors=True)\n",
    "\n",
    "print(\"creating delta table...\")\n",
    "df = spark.createDataFrame(data=df_data, schema=df_schema)\n",
    "df.write.mode(saveMode=\"overwrite\").format(\"delta\").save(\"data/delta-table\")  # would go to managed DB if saveAsTable(...) is used!\n",
    "   \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4d239131-18c4-410c-9c13-19f08bedf548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading delta file as df...\n",
      "+---+-------------+------------+---------+------------+---+\n",
      "| id|    firstname|    lastname|    house|    location|age|\n",
      "+---+-------------+------------+---------+------------+---+\n",
      "|  0|firstname-015|lastname-018|house-017|location-002| 22|\n",
      "|  1|firstname-016|lastname-006|house-011|location-001|  2|\n",
      "|  2|firstname-015|lastname-013|house-010|        NULL| 77|\n",
      "|  3|firstname-001|lastname-003|house-011|location-015| 55|\n",
      "|  4|         NULL|lastname-002|house-002|location-008| 59|\n",
      "+---+-------------+------------+---------+------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data as df (not as DeltaTable) from UNMANAGED!! database (file system folder)\n",
    "print(\"Reading delta file as df...\")\n",
    "df = spark.read.format(\"delta\").load(\"data/delta-table\")\n",
    "df.orderBy(\"id\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55c3a18e-1d83-4579-85b3-8535fca1d004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading delta file as delta object...\n",
      "Converting delta object to df...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+---+\n",
      "| id|    firstname|    lastname|        house|    location|age|\n",
      "+---+-------------+------------+-------------+------------+---+\n",
      "|  0|firstname-015|lastname-017|new_house-002|location-012| 84|\n",
      "|  1|firstname-017|        NULL|    house-017|location-017| 80|\n",
      "|  2|firstname-009|lastname-004|    house-009|location-003| 81|\n",
      "|  3|firstname-002|lastname-015|    house-020|        NULL|  8|\n",
      "|  4|firstname-011|lastname-001|    house-019|location-002| 20|\n",
      "+---+-------------+------------+-------------+------------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "Size(data/delta-table): 23904\n",
      "Updating delta table (object)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size(data/delta-table): 41060\n",
      "+---+-------------+------------+-----------------+------------+---+\n",
      "| id|    firstname|    lastname|            house|    location|age|\n",
      "+---+-------------+------------+-----------------+------------+---+\n",
      "|  0|firstname-015|lastname-017|new_new_house-002|location-012| 84|\n",
      "| 33|         NULL|lastname-003|    new_house-015|location-002| 12|\n",
      "| 44|         NULL|lastname-002|    new_house-014|location-008| 63|\n",
      "| 53|firstname-018|lastname-016|    new_house-006|location-016| 34|\n",
      "| 55|         NULL|lastname-012|    new_house-010|location-020| 51|\n",
      "+---+-------------+------------+-----------------+------------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Update data in Delta table, UNMANAGED!! database (file system folder)\n",
    "\n",
    "# read delta table to deltaTable object directly, then converting to df\n",
    "print(\"Reading delta file as delta object...\")\n",
    "deltaTable = DeltaTable.forPath(spark, \"data/delta-table\")\n",
    "print(\"Converting delta object to df...\")\n",
    "deltaTable.toDF().show(5)\n",
    "\n",
    "get_size(\"data/delta-table\")\n",
    "\n",
    "print(\"Updating delta table (object)...\")\n",
    "deltaTable.update(\n",
    "    condition = expr(\"id = 0\"),  # sql style\n",
    "    #condition = expr(\"firstname = 'firstname-018'\"),  # sql style\n",
    "    #condition = expr(\"firstname is null\"), # sql style\n",
    "    #condition = col(\"firstname\").isNull(), # spark style\n",
    "    #set = {\"lastname\": lit(\"Hello7\"), \"house\": lit(\"hello8\")}\n",
    "    #set = {\"age\": expr(\"age + 1000\")} # lit('literal\"), col('colname\"), expr(col(colname)+'hello\")\n",
    "    set = {\"house\": expr(\"concat('new_', house)\")}  # must use concat instead of \"+\"!!\n",
    "        )\n",
    "\n",
    "get_size(\"data/delta-table\")\n",
    "\n",
    "deltaTable.toDF().filter(\"id = 0 or firstname == 'firstname-018' or firstname is null\").orderBy(\"id\").show(5)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d2614d53-3b14-4109-ae88-726abb64e503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+---+\n",
      "| id|    firstname|    lastname|        house|    location|age|\n",
      "+---+-------------+------------+-------------+------------+---+\n",
      "|  0|firstname-015|lastname-018|new_house-017|location-002| 22|\n",
      "|  1|firstname-016|lastname-006| newhouse-011|location-001|  2|\n",
      "|  2|firstname-015|lastname-013|    house-010|        NULL| 77|\n",
      "|  3|firstname-001|lastname-003| newhouse-011|location-015| 55|\n",
      "|  4|         NULL|lastname-002|    house-002|location-008| 59|\n",
      "+---+-------------+------------+-------------+------------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# merge with SQL syntax using UNMANAGED!! database (file system folder)\n",
    "spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW newData AS SELECT col1 AS id FROM VALUES 1,3,5,7,9,11,13,15,17,19\")\n",
    "\n",
    "# get the delta table to update\n",
    "deltaTable = DeltaTable.forPath(spark, \"data/delta-table\")\n",
    "deltaTable.toDF().createOrReplaceTempView(\"t1\")  # df --> temp table\n",
    "\n",
    "# do the merge\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO t1 AS oldData\n",
    "USING newData\n",
    "ON oldData.id = newData.id\n",
    "WHEN MATCHED\n",
    "  THEN UPDATE SET house = concat('new', house)\n",
    "WHEN NOT MATCHED\n",
    "  THEN INSERT (id) VALUES (newData.id)\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "deltaTable.toDF().orderBy(\"id\").show(5)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "76723195-3885-473b-89f4-aaf505f439e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserting delta...\n",
      "+---+-------------+------------+-------------+------------+---+\n",
      "| id|    firstname|    lastname|        house|    location|age|\n",
      "+---+-------------+------------+-------------+------------+---+\n",
      "|  0|firstname-015|lastname-018|new_house-017|location-002| 22|\n",
      "|  1|firstname-016|lastname-006| newhouse-011|location-001|  2|\n",
      "|  2|firstname-015|lastname-013|    house-010|        NULL| 77|\n",
      "|  3|firstname-001|lastname-003| newhouse-011|location-015| 55|\n",
      "|  4|         NULL|lastname-002|    house-002|location-008| 59|\n",
      "+---+-------------+------------+-------------+------------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+-------------+------------+---------+-------------+----+\n",
      "| id|    firstname|    lastname|    house|     location| age|\n",
      "+---+-------------+------------+---------+-------------+----+\n",
      "|127|        Jamie|   Lannister|Lannister|Casterly Rock|  36|\n",
      "|126|          Jon|        Snow|    Stark|   Winterfell|  21|\n",
      "| 99|       Gendry|   Baratheon|Baratheon|Kings Landing|  19|\n",
      "| 98|firstname-001|lastname-015|house-004| location-008|NULL|\n",
      "| 97|         NULL|lastname-005|house-006|         NULL|  72|\n",
      "| 96|firstname-008|lastname-001|house-020| location-005|  36|\n",
      "| 95|firstname-009|        NULL|house-014| location-004|  31|\n",
      "| 94|firstname-007|lastname-001|house-008| location-005|  65|\n",
      "| 93|firstname-012|lastname-016|house-017| location-020|  52|\n",
      "| 92|firstname-003|lastname-003|     NULL| location-009|  65|\n",
      "+---+-------------+------------+---------+-------------+----+\n",
      "only showing top 10 rows\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# merge with DF syntax, using UNMANAGED!! database (file system folder)\n",
    "print(\"Upserting delta...\")\n",
    "# delta table path\n",
    "deltaTable = DeltaTable.forPath(spark, \"data/delta-table\")\n",
    "deltaTable.toDF().orderBy(\"id\").show(5)\n",
    "\n",
    "# define new data\n",
    "df_data = [(99,\"Gendry\", \"Baratheon\", \"Baratheon\", \"Kings Landing\", 19),\n",
    "        (126,\"Jon\", \"Snow\", \"Stark\", \"Winterfell\", 21),\n",
    "        (127,\"Jamie\", \"Lannister\", \"Lannister\", \"Casterly Rock\", 36)\n",
    "        ]\n",
    "\n",
    "df_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"firstname\", StringType(), True),\n",
    "    StructField(\"lastname\", StringType(), True),\n",
    "    StructField(\"house\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "newData = spark.createDataFrame(data=df_data, schema=df_schema)\n",
    "\n",
    "deltaTable.alias(\"oldData\") \\\n",
    "    .merge(\n",
    "    newData.alias(\"newData\"),\n",
    "    \"oldData.id = newData.id\") \\\n",
    "    .whenMatchedUpdate(\n",
    "    set={\"firstname\": col(\"newData.firstname\"), \"lastname\": col(\"newData.lastname\"), \"house\": col(\"newData.house\"),\n",
    "         \"location\": col(\"newData.location\"), \"age\": col(\"newData.age\")}) \\\n",
    "    .whenNotMatchedInsert(\n",
    "    values={\"id\": col(\"newData.id\"), \"firstname\": col(\"newData.firstname\"), \"lastname\": col(\"newData.lastname\"), \"house\": col(\"newData.house\"),\n",
    "            \"location\": col(\"newData.location\"), \"age\": col(\"newData.age\")}) \\\n",
    "    .execute()\n",
    "\n",
    "deltaTable.toDF().orderBy(desc(\"id\")).show(10)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "78cf380a-67d6-41a0-88b6-84234253ef88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting data...\n",
      "+---+-------------+------------+-------------+------------+----+\n",
      "| id|    firstname|    lastname|        house|    location| age|\n",
      "+---+-------------+------------+-------------+------------+----+\n",
      "|  0|firstname-015|lastname-018|new_house-017|location-002|  22|\n",
      "|  1|firstname-016|lastname-006| newhouse-011|location-001|   2|\n",
      "|  2|firstname-015|lastname-013|    house-010|        NULL|  77|\n",
      "|  3|firstname-001|lastname-003| newhouse-011|location-015|  55|\n",
      "|  4|         NULL|lastname-002|    house-002|location-008|  59|\n",
      "|  5|firstname-001|lastname-012| newhouse-005|location-013|  29|\n",
      "|  6|firstname-002|lastname-003|    house-015|location-002|  59|\n",
      "|  7|firstname-010|lastname-009| newhouse-020|location-006|  21|\n",
      "|  8|firstname-011|lastname-010|    house-008|location-020|  74|\n",
      "|  9|firstname-020|lastname-013| newhouse-011|location-009|  31|\n",
      "| 10|firstname-013|lastname-012|    house-003|location-003|  24|\n",
      "| 11|firstname-001|lastname-010| newhouse-012|location-001|NULL|\n",
      "| 12|firstname-015|lastname-012|    house-017|        NULL|  21|\n",
      "| 13|firstname-013|lastname-007| newhouse-011|        NULL|  11|\n",
      "| 14|firstname-020|lastname-007|    house-001|        NULL|  66|\n",
      "| 15|firstname-019|lastname-008| newhouse-006|location-014|NULL|\n",
      "| 16|firstname-012|lastname-002|    house-015|location-015|  93|\n",
      "| 17|firstname-005|lastname-012| newhouse-002|location-015|   7|\n",
      "| 18|firstname-019|lastname-014|    house-015|location-016|   4|\n",
      "| 19|firstname-004|lastname-010| newhouse-008|location-014|  41|\n",
      "+---+-------------+------------+-------------+------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+-------------+------------+------------+------------+---+\n",
      "| id|    firstname|    lastname|       house|    location|age|\n",
      "+---+-------------+------------+------------+------------+---+\n",
      "|  1|firstname-016|lastname-006|newhouse-011|location-001|  2|\n",
      "|  2|firstname-015|lastname-013|   house-010|        NULL| 77|\n",
      "|  3|firstname-001|lastname-003|newhouse-011|location-015| 55|\n",
      "|  4|         NULL|lastname-002|   house-002|location-008| 59|\n",
      "|  5|firstname-001|lastname-012|newhouse-005|location-013| 29|\n",
      "|  6|firstname-002|lastname-003|   house-015|location-002| 59|\n",
      "|  7|firstname-010|lastname-009|newhouse-020|location-006| 21|\n",
      "|  8|firstname-011|lastname-010|   house-008|location-020| 74|\n",
      "|  9|firstname-020|lastname-013|newhouse-011|location-009| 31|\n",
      "| 10|firstname-013|lastname-012|   house-003|location-003| 24|\n",
      "+---+-------------+------------+------------+------------+---+\n",
      "only showing top 10 rows\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Delete data (single record) using UNMANAGED!! database (file system folder)\n",
    "print(\"Deleting data...\")\n",
    "\n",
    "# delta table path\n",
    "deltaTable = DeltaTable.forPath(spark, \"data/delta-table\")\n",
    "deltaTable.toDF().orderBy(asc(\"id\")).show()\n",
    "\n",
    "deltaTable.delete(condition=expr(\"id in (0)\"))\n",
    "\n",
    "deltaTable.toDF().orderBy(asc(\"id\")).show(10)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe3de293-17bd-4e01-94b1-a35d88e4183d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading delta table info from deltaTable.details()...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "format: delta\n",
      "id: e6434d4e-2951-4560-a29d-c17fb59fc930\n",
      "name: None\n",
      "description: None\n",
      "location: file:/home/jovyan/work/various_tests/spark/delta_tests/data/delta-table\n",
      "createdAt: 2025-05-12 14:11:21.415000\n",
      "lastModified: 2025-05-12 14:21:11.020000\n",
      "partitionColumns: []\n",
      "clusteringColumns: []\n",
      "numFiles: 4\n",
      "sizeInBytes: 9805\n",
      "properties: {}\n",
      "minReaderVersion: 1\n",
      "minWriterVersion: 2\n",
      "tableFeatures: ['appendOnly', 'invariants']\n",
      "\n",
      "Reading delta table history from deltaTable.history()...\n",
      "version: 0\n",
      "timestamp: 2025-05-12 14:11:25.722000\n",
      "userId: None\n",
      "userName: None\n",
      "operation: WRITE\n",
      "operationParameters: {'mode': 'Overwrite', 'partitionBy': '[]'}\n",
      "   mode: Overwrite\n",
      "   partitionBy: []\n",
      "job: None\n",
      "notebook: None\n",
      "clusterId: None\n",
      "readVersion: None\n",
      "isolationLevel: Serializable\n",
      "isBlindAppend: False\n",
      "operationMetrics: {'numOutputRows': '100', 'numOutputBytes': '9730', 'numFiles': '4'}\n",
      "   numOutputRows: 100\n",
      "   numOutputBytes: 9730\n",
      "   numFiles: 4\n",
      "userMetadata: None\n",
      "engineInfo: Apache-Spark/3.5.5 Delta-Lake/3.3.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+---------+------------+---+\n",
      "| id|    firstname|    lastname|    house|    location|age|\n",
      "+---+-------------+------------+---------+------------+---+\n",
      "|  0|firstname-015|lastname-017|house-002|location-012| 84|\n",
      "|  1|firstname-017|        NULL|house-017|location-017| 80|\n",
      "|  2|firstname-009|lastname-004|house-009|location-003| 81|\n",
      "|  3|firstname-002|lastname-015|house-020|        NULL|  8|\n",
      "|  4|firstname-011|lastname-001|house-019|location-002| 20|\n",
      "|  5|firstname-016|lastname-004|house-004|location-007| 91|\n",
      "+---+-------------+------------+---------+------------+---+\n",
      "\n",
      "version: 1\n",
      "timestamp: 2025-05-12 14:17:55.307000\n",
      "userId: None\n",
      "userName: None\n",
      "operation: UPDATE\n",
      "operationParameters: {'predicate': '[\"(id#830 = 0)\"]'}\n",
      "   predicate: [\"(id#830 = 0)\"]\n",
      "job: None\n",
      "notebook: None\n",
      "clusterId: None\n",
      "readVersion: 0\n",
      "isolationLevel: Serializable\n",
      "isBlindAppend: False\n",
      "operationMetrics: {'numDeletionVectorsUpdated': '0', 'numAddedFiles': '1', 'executionTimeMs': '1426', 'numDeletionVectorsRemoved': '0', 'numUpdatedRows': '1', 'numRemovedFiles': '1', 'rewriteTimeMs': '247', 'numRemovedBytes': '2415', 'scanTimeMs': '1178', 'numCopiedRows': '24', 'numDeletionVectorsAdded': '0', 'numAddedChangeFiles': '0', 'numAddedBytes': '2431'}\n",
      "   numDeletionVectorsUpdated: 0\n",
      "   numAddedFiles: 1\n",
      "   executionTimeMs: 1426\n",
      "   numDeletionVectorsRemoved: 0\n",
      "   numUpdatedRows: 1\n",
      "   numRemovedFiles: 1\n",
      "   rewriteTimeMs: 247\n",
      "   numRemovedBytes: 2415\n",
      "   scanTimeMs: 1178\n",
      "   numCopiedRows: 24\n",
      "   numDeletionVectorsAdded: 0\n",
      "   numAddedChangeFiles: 0\n",
      "   numAddedBytes: 2431\n",
      "userMetadata: None\n",
      "engineInfo: Apache-Spark/3.5.5 Delta-Lake/3.3.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+---+\n",
      "| id|    firstname|    lastname|        house|    location|age|\n",
      "+---+-------------+------------+-------------+------------+---+\n",
      "|  0|firstname-015|lastname-017|new_house-002|location-012| 84|\n",
      "|  1|firstname-017|        NULL|    house-017|location-017| 80|\n",
      "|  2|firstname-009|lastname-004|    house-009|location-003| 81|\n",
      "|  3|firstname-002|lastname-015|    house-020|        NULL|  8|\n",
      "|  4|firstname-011|lastname-001|    house-019|location-002| 20|\n",
      "|  5|firstname-016|lastname-004|    house-004|location-007| 91|\n",
      "+---+-------------+------------+-------------+------------+---+\n",
      "\n",
      "version: 2\n",
      "timestamp: 2025-05-12 14:21:11.020000\n",
      "userId: None\n",
      "userName: None\n",
      "operation: UPDATE\n",
      "operationParameters: {'predicate': '[]'}\n",
      "   predicate: []\n",
      "job: None\n",
      "notebook: None\n",
      "clusterId: None\n",
      "readVersion: 1\n",
      "isolationLevel: Serializable\n",
      "isBlindAppend: False\n",
      "operationMetrics: {'numDeletionVectorsUpdated': '0', 'numAddedFiles': '4', 'executionTimeMs': '931', 'numDeletionVectorsRemoved': '0', 'numUpdatedRows': '100', 'numRemovedFiles': '4', 'rewriteTimeMs': '338', 'numRemovedBytes': '9746', 'scanTimeMs': '592', 'numCopiedRows': '0', 'numDeletionVectorsAdded': '0', 'numAddedChangeFiles': '0', 'numAddedBytes': '9805'}\n",
      "   numDeletionVectorsUpdated: 0\n",
      "   numAddedFiles: 4\n",
      "   executionTimeMs: 931\n",
      "   numDeletionVectorsRemoved: 0\n",
      "   numUpdatedRows: 100\n",
      "   numRemovedFiles: 4\n",
      "   rewriteTimeMs: 338\n",
      "   numRemovedBytes: 9746\n",
      "   scanTimeMs: 592\n",
      "   numCopiedRows: 0\n",
      "   numDeletionVectorsAdded: 0\n",
      "   numAddedChangeFiles: 0\n",
      "   numAddedBytes: 9805\n",
      "userMetadata: None\n",
      "engineInfo: Apache-Spark/3.5.5 Delta-Lake/3.3.0\n",
      "\n",
      "+---+-------------+------------+-----------------+------------+---+\n",
      "| id|    firstname|    lastname|            house|    location|age|\n",
      "+---+-------------+------------+-----------------+------------+---+\n",
      "|  0|firstname-015|lastname-017|new_new_house-002|location-012| 84|\n",
      "|  1|firstname-017|        NULL|    new_house-017|location-017| 80|\n",
      "|  2|firstname-009|lastname-004|    new_house-009|location-003| 81|\n",
      "|  3|firstname-002|lastname-015|    new_house-020|        NULL|  8|\n",
      "|  4|firstname-011|lastname-001|    new_house-019|location-002| 20|\n",
      "|  5|firstname-016|lastname-004|    new_house-004|location-007| 91|\n",
      "+---+-------------+------------+-----------------+------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading delta details, history, all versions of delta table using UNMANAGED!! database (file system folder)\n",
    "# deleting a single record creates a new parquet file and \"removes\" the old one, but still keeps the old one for history\n",
    "# i.e. delta format is just a metadata management of the basic parquet format!!!\n",
    "\n",
    "def print_dict(d: dict, level: int = 0) -> None:\n",
    "\n",
    "    sp = \"\".ljust(level * 3)\n",
    "    for k in d.keys():\n",
    "        print(f\"{sp}{k}: {d[k]}\")\n",
    "        \n",
    "        if type(d[k]) is dict:\n",
    "            print_dict(d[k], level + 1)\n",
    "            \n",
    "    if level == 0:\n",
    "        print()\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, \"data/delta-table\")\n",
    "\n",
    "print(\"Reading delta table info from deltaTable.details()...\")\n",
    "df_info = deltaTable.detail().select(\"*\").collect() # returns list of Row\n",
    "print_dict(df_info[0].asDict())\n",
    "\n",
    "print(\"Reading delta table history from deltaTable.history()...\")\n",
    "df_hist = deltaTable.history()\n",
    "for row in df_hist.select(\"*\").orderBy(\"version\", ascending = 1).collect():\n",
    "    print_dict(row.asDict())\n",
    "    \n",
    "    df = spark.read.format(\"delta\").option(\"versionAsOf\", row['version']).load(\"data/delta-table\").filter(\"id <= 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d4c90a2b-0eae-4830-9ffc-0288c1b5f41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# delete all history\n",
    "# first set spark config \"spark.databricks.delta.retentionDurationCheck.enabled\" = false if retentionHours is < 168\n",
    "# vacuum() has a *BUG*, it deletes a file from the oldest version, so ruins the entire table....\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, \"data/delta-table\")\n",
    "#deltaTable.vacuum(15/60)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f38699d7-c361-49d3-a3fa-23001684cc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting for 5 secs...\n",
      "+-------+-----------------------+\n",
      "|version|timestamp              |\n",
      "+-------+-----------------------+\n",
      "|2      |2025-04-01 19:56:02.729|\n",
      "|1      |2025-04-01 19:56:01.509|\n",
      "|0      |2025-04-01 19:56:01.309|\n",
      "+-------+-----------------------+\n",
      "\n",
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  2|hello0|\n",
      "|  3|hello1|\n",
      "+---+------+\n",
      "\n",
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|hello1|\n",
      "|  0|hello0|\n",
      "+---+------+\n",
      "\n",
      "+-------+-----------------------+\n",
      "|version|timestamp              |\n",
      "+-------+-----------------------+\n",
      "|2      |2025-04-01 19:56:02.729|\n",
      "|1      |2025-04-01 19:56:01.509|\n",
      "|0      |2025-04-01 19:56:01.309|\n",
      "+-------+-----------------------+\n",
      "\n",
      "2025-04-01 19:56:01.309000 2025-04-01 19:56:02.729000\n",
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "+---+----+\n",
      "\n",
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  2|hello0|\n",
      "|  3|hello1|\n",
      "+---+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n",
      "|table_size_after_restore|num_of_files_after_restore|num_removed_files|num_restored_files|removed_files_size|restored_files_size|\n",
      "+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n",
      "|                    1402|                         2|                2|                 2|              1402|               1402|\n",
      "+------------------------+--------------------------+-----------------+------------------+------------------+-------------------+\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# \"timetravel\" query as of timestamp / as of version using MANAGED!! database (spark-warehouse)\n",
    "# would go to unmanaged DB if USING csv OPTIONS PATH '...' is used\n",
    "import time\n",
    "\n",
    "# create\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS t2 (\n",
    "  id INT,\n",
    "  name STRING\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# insert\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO  t2 \n",
    "select 0,'hello0' union all \n",
    "select 1,'hello1'\n",
    "\"\"\")\n",
    "\n",
    "# insert OVERWRITE (old parquet overwritten)\n",
    "spark.sql(\"\"\"\n",
    "INSERT OVERWRITE t2 \n",
    "select 2,'hello0' union all \n",
    "select 3,'hello1'\n",
    "\"\"\")\n",
    "\n",
    "# wait for as of timestamp query\n",
    "print(\"waiting for 5 secs...\")\n",
    "time.sleep(5)\n",
    "\n",
    "# update\n",
    "spark.sql(\"\"\"\n",
    "UPDATE t2\n",
    "SET t2.name = concat(t2.name, ' ', 'xxx')\n",
    "WHERE t2.id = 0\n",
    "\"\"\")\n",
    "\n",
    "# get hist\n",
    "spark.sql(\"\"\"\n",
    "DESCRIBE HISTORY t2\n",
    "\"\"\").select(\"version\", \"timestamp\").show(truncate=False)\n",
    "\n",
    "# select\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM t2\n",
    "\"\"\").show()\n",
    "\n",
    "# select from a specific version\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM t2 VERSION AS OF 1\n",
    "\"\"\").show()\n",
    "\n",
    "# select from a specific timestamp (must be between min/max history.  \"describe history\" cannot be queried...)\n",
    "table_name = \"t2\"\n",
    "spark.sql(f\"DESCRIBE HISTORY {table_name}\").select(\"version\", \"timestamp\").show(truncate=False)\n",
    "min_ts = spark.sql(f\"DESCRIBE HISTORY {table_name}\").select(\"timestamp\").orderBy(\"version\", ascending = 1).collect()[0][\"timestamp\"]\n",
    "max_ts = spark.sql(f\"DESCRIBE HISTORY {table_name}\").select(\"timestamp\").orderBy(\"version\", ascending = 0).collect()[0][\"timestamp\"]\n",
    "print(min_ts, max_ts)\n",
    "spark.sql(f\"\"\"SELECT * FROM t2 TIMESTAMP AS OF '{min_ts}'\"\"\").show()\n",
    "spark.sql(f\"\"\"SELECT * FROM t2 TIMESTAMP AS OF '{max_ts}'\"\"\").show()\n",
    "\n",
    "# restore to a previous version\n",
    "spark.sql(\"\"\"\n",
    "RESTORE t2 VERSION AS OF 1\n",
    "\"\"\").show()\n",
    "\n",
    "# drop\n",
    "spark.sql(\"\"\"\n",
    "DROP TABLE t2\n",
    "\"\"\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "957a93f0-d189-4e43-b501-772c7a90ae95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n",
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|           default|\n",
      "+------------------+\n",
      "\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  default|       t1|      false|\n",
      "|         |  newdata|       true|\n",
      "|         |       t1|       true|\n",
      "+---------+---------+-----------+\n",
      "\n",
      "+---------+--------+-----------+\n",
      "|namespace|viewName|isTemporary|\n",
      "+---------+--------+-----------+\n",
      "|         | newdata|       true|\n",
      "|         |      t1|       true|\n",
      "+---------+--------+-----------+\n",
      "\n",
      "+-----------------+\n",
      "|num_affected_rows|\n",
      "+-----------------+\n",
      "|                1|\n",
      "+-----------------+\n",
      "\n",
      "+---+----------+\n",
      "| id|      name|\n",
      "+---+----------+\n",
      "|  1|hello1 xxx|\n",
      "+---+----------+\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a delta table through SQL syntax (USING DELTA)\n",
    "\n",
    "spark.sql(\"Show databases\").show()\n",
    "spark.sql(\"select current_database()\").show()\n",
    "#spark.sql(\"use mytestdb\")\n",
    "spark.sql(\"Show tables\").show()\n",
    "spark.sql(\"Show views\").show()\n",
    "\n",
    "# create\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS my_table (\n",
    "  id INT,\n",
    "  name STRING\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# insert\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO  my_table values (1,'hello1')\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# update\n",
    "spark.sql(\"\"\"\n",
    "UPDATE my_table t1\n",
    "SET t1.name = concat(t1.name, ' ', 'xxx')\n",
    "WHERE t1.id < 100\n",
    "\"\"\").show()\n",
    "\n",
    "# select\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM my_table\n",
    "\"\"\").show()\n",
    "\n",
    "# drop\n",
    "spark.sql(\"\"\"\n",
    "DROP TABLE my_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf33234b-f625-43af-8fda-5b1fe8c35293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+---+-------------+------------+---------+------------+----+\n",
      "|id |firstname    |lastname    |house    |location    |age |\n",
      "+---+-------------+------------+---------+------------+----+\n",
      "|32 |firstname-003|lastname-017|house-018|location-017|3   |\n",
      "|33 |NULL         |lastname-015|house-006|location-003|71  |\n",
      "|34 |firstname-009|lastname-017|house-001|location-016|51  |\n",
      "|35 |firstname-007|lastname-006|house-003|location-018|NULL|\n",
      "|36 |firstname-010|lastname-005|house-006|location-002|44  |\n",
      "+---+-------------+------------+---------+------------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n",
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|           default|\n",
      "+------------------+\n",
      "\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|         |       v1|       true|\n",
      "+---------+---------+-----------+\n",
      "\n",
      "+---------+--------+-----------+\n",
      "|namespace|viewName|isTemporary|\n",
      "+---------+--------+-----------+\n",
      "|         |      v1|       true|\n",
      "+---------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a view on an unmanaged delta file\n",
    "spark.sql(\"\"\"\n",
    "create or replace temporary view v1\n",
    "using delta\n",
    "options(path 'data/delta-table')\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"select * from v1\"\"\").show(5, truncate=False)\n",
    "\n",
    "spark.sql(\"Show databases\").show()\n",
    "spark.sql(\"select current_database()\").show()\n",
    "#spark.sql(\"use mytestdb\")\n",
    "spark.sql(\"Show tables\").show()\n",
    "spark.sql(\"Show views\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
