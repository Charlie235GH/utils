{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0801b0ab-48aa-4ff8-8f18-9bf3d78b1f32",
   "metadata": {},
   "source": [
    "## Shuffling (\"exchange\" in query plan): produce datasets that can be transfered onto separate data nodes, and manipulated as sub-tasks (e.g. sub-joins) to avoid transfering all the data multiple times across joins, etc.\n",
    "## one dataset would have all the ROWS of one or more value of the join key, making sure that join will have full sub-result of that value\n",
    "\n",
    "## used in operations such as groupby(), join(), repartition(), distinct()\n",
    "\n",
    "## smaller datasets (not compared to each other but in spark settings \"spark.sql.autoBroadcastJoinThreshold\") are broadcasted so that shuffling would not happen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2429601d-d051-47c4-88b7-fe0e2bb48b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/11 20:05:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting Spark log level to \"ERROR\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark version: 3.5.5\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         #.master(\"local\")\n",
    "         .appName(\"Shuffle perf test with unordered vs ordered join keys\")\n",
    "         .enableHiveSupport() # enableHiveSupport() needed to make data persistent...\n",
    "         .config(\"spark.log.level\", \"ERROR\")\n",
    "         .config(\"spark.executor.memory\", \"4g\")\n",
    "         .config(\"spark.driver.memory\", \"4g\")\n",
    "         .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) # max. byte size of table to be broadcast. -1 = disable\n",
    "         # enable join reorder (doesn't seem to work...)\n",
    "         .config(\"spark.sql.cbo.enabled\", True) # default is false\n",
    "         .config(\"spark.sql.cbo.joinReorder.enabled\", True) # default is false\n",
    "         .config(\"spark.sql.cbo.joinReorder.dp.threshold\", 20) # max. number of jon reorders. default is 12\n",
    "         .config(\"spark.sql.statistics.histogram.enabled\", True)\n",
    "         .getOrCreate())\n",
    "\n",
    "print('spark version:', spark.version)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a032232-bc8f-457e-9065-6e1e1e82152d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+\n",
      "|origin|origin_display_name|\n",
      "+------+-------------------+\n",
      "|AAAA  |Aaaa Ntldjex       |\n",
      "|BAAA  |Baaa Jqlvv         |\n",
      "|CAAA  |Caaa Izlyxr        |\n",
      "|DAAA  |Daaa Rlvfr         |\n",
      "|EAAA  |Eaaa Aaymq         |\n",
      "+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+------+----+\n",
      "|flight_id|origin|dest|\n",
      "+---------+------+----+\n",
      "|0        |AAAA  |BAAA|\n",
      "|1        |BAAA  |CAAA|\n",
      "|2        |CAAA  |DAAA|\n",
      "|3        |DAAA  |EAAA|\n",
      "|4        |EAAA  |FAAA|\n",
      "+---------+------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+--------------+\n",
      "|flight_id|dep_delay_mins|\n",
      "+---------+--------------+\n",
      "|0        |0             |\n",
      "|1        |0             |\n",
      "|2        |0             |\n",
      "|3        |6             |\n",
      "|4        |23            |\n",
      "+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+-----------+\n",
      "|flight_id|alert      |\n",
      "+---------+-----------+\n",
      "|0        |           |\n",
      "|1        |some alert!|\n",
      "|2        |           |\n",
      "|3        |           |\n",
      "|4        |           |\n",
      "+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# generate some test data\n",
    "from pyspark.sql import functions as F\n",
    "import random\n",
    "import math\n",
    "\n",
    "num_airports =   1000 # max. 26^4\n",
    "num_flights  = 1000000\n",
    "num_delays   = 1000\n",
    "num_alerts   = 1000\n",
    "\n",
    "# origin_airport\n",
    "letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "num_airports = min(num_airports, pow(len(letters), 4))\n",
    "origin_airport_lst = [\n",
    "    ( \n",
    "        \n",
    "        ''.join([letters[c1 // pow(len(letters), c) % len(letters)] for c in range(4)]),                                # airport (ICAO code)\n",
    "        (\n",
    "            ''.join([letters[c1 // pow(len(letters), c) % len(letters)] for c in range(4)]) +                              # display_airport_name\n",
    "            ' ' +\n",
    "            ''.join([letters[random.randint(0, len(letters)-1)] for c in range(random.randint(5, 10))])\n",
    "        ).title(),\n",
    "     )\n",
    "    for c1 in range(num_airports)]\n",
    "\n",
    "origin_airport_df = spark.createDataFrame(origin_airport_lst, ['airport', 'display_airport_name']).select(\n",
    "    F.col('airport').alias('origin'), \n",
    "    F.col('display_airport_name').alias('origin_display_name'))\n",
    "origin_airport_df.show(5, truncate=False)\n",
    "\n",
    "# flights\n",
    "flights_lst = [\n",
    "    (c1,                                                               # flight_id\n",
    "     origin_airport_lst[c1%(len(origin_airport_lst))][0],              # origin\n",
    "     origin_airport_lst[(c1 + 1)%(len(origin_airport_lst))][0]         # dest\n",
    "     )\n",
    "    for c1 in range(num_flights)]\n",
    "flights_df = spark.createDataFrame(flights_lst, ['flight_id', 'origin', 'dest'])\n",
    "flights_df.show(5, truncate=False)\n",
    "\n",
    "def non_negative(n):\n",
    "    return 0 if n < 0 else n\n",
    "\n",
    "# delays\n",
    "delays_lst = [\n",
    "    (c1,                                            # flight_id\n",
    "     non_negative(random.randint(-40, 35)),        # dep_delay_mins\n",
    "     )\n",
    "    for c1 in range(num_delays)]\n",
    "delays_df = spark.createDataFrame(delays_lst, ['flight_id', 'dep_delay_mins'])\n",
    "delays_df.show(5, truncate=False)\n",
    "\n",
    "# flight_alerts\n",
    "flight_alerts_lst = [\n",
    "    (c1,                                                   # flight_id\n",
    "     \"some alert!\" if random.randint(1,10) <= 3 else \"\"    # alert\n",
    "     )\n",
    "    for c1 in range(num_alerts)]\n",
    "flight_alerts_df = spark.createDataFrame(flight_alerts_lst, ['flight_id', 'alert'])\n",
    "flight_alerts_df.show(5, truncate=False)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c33cbda-a10a-48d8-83cd-b48fccb59628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsorted join key done in 0:00:22.142348 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted join key done in 0:00:15.125590 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# dataframe join test\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# join keys VARY in execution order => reshuffling is necessary!\n",
    "ct = datetime.now()\n",
    "result_df = (\n",
    "    flights_df\n",
    "    .select(\"flight_id\", \"origin\", \"dest\")\n",
    "    .join(delays_df, \"flight_id\", \"left\") # flight_id shuffle\n",
    "    .join(origin_airport_df, \"origin\", \"left\") # origin shuffle\n",
    "    .join(flight_alerts_df, \"flight_id\", \"left\") # flight_id re-shuffled\n",
    ")\n",
    "\n",
    "#result_df = result_df.na.fill(value=\"\", subset = ['alert'])\n",
    "result_df.write.mode(\"overwrite\").csv(\"result_df1\") # to make sure all results are used, not only a few that's shown\n",
    "print(f\"Unsorted join key done in {(datetime.now() - ct)} secs.\")\n",
    "\n",
    "# join keys DO NOT VARY in execution order => re-shuffling is NOT necessary!\n",
    "ct = datetime.now()\n",
    "result_df = (\n",
    "    flights_df\n",
    "    .select(\"flight_id\", \"origin\", \"dest\")\n",
    "    .join(origin_airport_df, \"origin\", \"left\") # origin shuffle\n",
    "    .join(delays_df, \"flight_id\", \"left\") # flight_id shuffle\n",
    "    .join(flight_alerts_df, \"flight_id\", \"left\") # flight_id, no re-shuffling needed\n",
    ")\n",
    "\n",
    "#result_df = result_df.na.fill(value=\"\", subset = ['alert'])\n",
    "result_df.write.mode(\"overwrite\").csv(\"result_df2\") # to make sure all results are used, not only a few that's shown\n",
    "print(f\"Sorted join key done in {(datetime.now() - ct)} secs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ba9221e-55ed-4745-b925-51d3d859a5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsorted join key done in 0:00:05.250045 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted join key done in 0:00:02.723144 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# sql join test - join order is kept the same as defined in the query by default and unable to change that...!\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# enable join reordering!\n",
    "spark.conf.set(\"spark.sql.cbo.enabled\", True) # default is false\n",
    "spark.conf.set(\"spark.sql.cbo.joinReorder.enabled\", True) # default is false\n",
    "spark.conf.set(\"spark.sql.cbo.joinReorder.dp.threshold\", 20) # max. number of jon reorders. default is 12\n",
    "\n",
    "\n",
    "spark.sql('USE MYTESTDB')\n",
    "origin_airport_df.createOrReplaceTempView('origin_airport')\n",
    "flights_df.createOrReplaceTempView('flights')\n",
    "delays_df.createOrReplaceTempView('delays')\n",
    "flight_alerts_df.createOrReplaceTempView('flight_alerts')\n",
    "\n",
    "# cache table for ANALYZE\n",
    "spark.catalog.cacheTable(\"origin_airport\")\n",
    "spark.catalog.cacheTable(\"flights\")\n",
    "spark.catalog.cacheTable(\"delays\")\n",
    "spark.catalog.cacheTable(\"flight_alerts\")\n",
    "\n",
    "# ANALYZE for join reordering\n",
    "spark.sql(\"ANALYZE TABLE origin_airport COMPUTE STATISTICS FOR COLUMNS origin\")\n",
    "spark.sql(\"ANALYZE TABLE flights COMPUTE STATISTICS FOR COLUMNS flight_id\")\n",
    "spark.sql(\"ANALYZE TABLE delays COMPUTE STATISTICS FOR COLUMNS flight_id\")\n",
    "spark.sql(\"ANALYZE TABLE flight_alerts COMPUTE STATISTICS FOR COLUMNS flight_id\")\n",
    "\n",
    "# join keys VARY in execution order => reshuffling is necessary!\n",
    "ct = datetime.now()\n",
    "result_df = spark.sql('''\n",
    "SELECT f.flight_id, f.origin, f.dest\n",
    "FROM flights f\n",
    "    left outer join delays d\n",
    "    on (f.flight_id = d.flight_id)\n",
    "    left outer join origin_airport oa\n",
    "    on (f.origin = oa.origin)\n",
    "    left outer join flight_alerts fa\n",
    "    on (f.flight_id = fa.flight_id)\n",
    "''')\n",
    "result_df.write.mode(\"overwrite\").csv(\"result_df1\") # action\n",
    "print(f\"Unsorted join key done in {(datetime.now() - ct)} secs.\")\n",
    "\n",
    "# join keys DO NOT VARY in execution order => re-shuffling is NOT necessary!\n",
    "ct = datetime.now()\n",
    "result_df = spark.sql('''\n",
    "SELECT f.flight_id, f.origin, f.dest\n",
    "FROM flights f\n",
    "    left outer join origin_airport oa\n",
    "    on (f.origin = oa.origin)\n",
    "    left outer join delays d\n",
    "    on (f.flight_id = d.flight_id)\n",
    "    left outer join flight_alerts fa\n",
    "    on (f.flight_id = fa.flight_id)\n",
    "''')\n",
    "result_df.write.mode(\"overwrite\").csv(\"result_df2\") # action\n",
    "print(f\"Sorted join key done in {(datetime.now() - ct)} secs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d49f092b-4e08-41cf-a257-d0b96baef2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
