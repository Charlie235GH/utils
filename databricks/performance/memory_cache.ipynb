{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d7df936-5904-42f6-a150-7d915d4d0cf5",
   "metadata": {},
   "source": [
    "## https://spark.apache.org/docs/3.5.3/sql-performance-tuning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06b57cb0-52e2-4bc5-be8e-b0735e613192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark version: 3.5.5\n",
      "spark.executor.memory:  4g\n",
      "spark.driver.memory:  4g\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# setup spark on linux\n",
    "from pyspark.sql import *\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         #.master(\"local\") # local- no parallelizm at all, local[2] - 2 cores, local[*] - as many cores as local logical cores\n",
    "         .appName(\"SparkSession#1\")\n",
    "         .config(\"spark.log.level\", \"ERROR\")\n",
    "         .enableHiveSupport() # enableHiveSupport() needed to make data persistent... \n",
    "         .config(\"spark.executor.memory\", \"4g\")\n",
    "         .config(\"spark.driver.memory\", \"4g\")\n",
    "         #.config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "         #.config(\"spark.default.parallelism\", \"200\")\n",
    "         .getOrCreate())\n",
    "\n",
    "print('spark version:', spark.version)\n",
    "print('spark.executor.memory: ', spark.sparkContext._conf.get('spark.executor.memory'))\n",
    "print('spark.driver.memory: ', spark.sparkContext._conf.get('spark.driver.memory'))\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe9f7f17-ae8c-4f7e-9b54-ecae131024f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# supporting functions\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, TimestampType\n",
    "\n",
    "def print_dict(d: dict, level: int = 0) -> None:\n",
    "    \"\"\"\n",
    "    print optionally hiearchic dict structure nicely formatted\n",
    "    \"\"\"\n",
    "\n",
    "    sp = \"\".ljust(level * 3)\n",
    "    for k in d.keys():\n",
    "        print(f\"{sp}{k}: {d[k]}\")\n",
    "        \n",
    "        if type(d[k]) is dict:\n",
    "            print_dict(d[k], level + 1)\n",
    "            \n",
    "    if level == 0:\n",
    "        print()\n",
    "\n",
    "def show_plan(plan: DataFrame) -> None:\n",
    "    for row in plan.collect():\n",
    "        print_dict(row.asDict())\n",
    "\n",
    "def generateDF(num_rows: int = 100) -> DataFrame:\n",
    "    \"\"\"\n",
    "    generate a dataframe with random values including nulls, of num_rows records\n",
    "    \"\"\"\n",
    "\n",
    "    df_schema = StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"age\", IntegerType(), True),\n",
    "        StructField(\"from_dt\", DateType(), True),\n",
    "        StructField(\"to_dt\", DateType(), True),\n",
    "    ])\n",
    "    \n",
    "    lst = []\n",
    "    for c in range(num_rows):\n",
    "        from_dt = datetime.now().date() + timedelta(days=random.randint(0, 1000))\n",
    "        to_dt = from_dt + timedelta(days=random.randint(1, 100))\n",
    "    \n",
    "        lst.append([\n",
    "            c,\n",
    "            None if random.random() < 0.1 else 'name-' + str(c).zfill(8),\n",
    "            None if random.random() < 0.1 else random.randint(1, 100),\n",
    "            from_dt,\n",
    "            to_dt\n",
    "             ])\n",
    "    \n",
    "    return spark.createDataFrame(lst, df_schema)\n",
    "\n",
    "# list managed DB objects\n",
    "\n",
    "def show_managed_db(db_name: str = \"default\") -> None:\n",
    "    print(\"Available DBs:\")\n",
    "    spark.sql(\"Show databases\").show(truncate=False)\n",
    "\n",
    "    orig_db_name = spark.sql(\"select current_database()\").collect()[0][0]\n",
    "\n",
    "    print(f\"Getting objects from {db_name}:\")\n",
    "    spark.sql(f\"use {db_name}\")\n",
    "    spark.sql(f\"SHOW TABLES IN {db_name}\").show(truncate=False)\n",
    "    spark.sql(f\"SHOW VIEWS IN {db_name}\").show(truncate=False)\n",
    "\n",
    "print(f\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "854639d6-c3e6-4a6e-9962-fd2abc9d0a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available DBs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 20:09:07 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/04/11 20:09:07 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/04/11 20:09:08 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "25/04/11 20:09:08 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore jovyan@172.17.0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|default  |\n",
      "|mydb     |\n",
      "+---------+\n",
      "\n",
      "Getting objects from mydb:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 20:09:10 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|mydb     |tab_df2  |false      |\n",
      "+---------+---------+-----------+\n",
      "\n",
      "+---------+--------+-----------+\n",
      "|namespace|viewName|isTemporary|\n",
      "+---------+--------+-----------+\n",
      "+---------+--------+-----------+\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#spark.sql(\"CREATE DATABASE mydb\")\n",
    "show_managed_db(\"mydb\")\n",
    "print(f\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb74f43-e422-423d-85c3-8c3ec4711965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create some test data for join...\n",
      "Done in 0:01:35.218784\n",
      "Saving the test DFs as parquet tables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 20:17:20 WARN TaskSetManager: Stage 26 contains a task of very large size (297010 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/11 20:17:32 WARN TaskSetManager: Stage 27 contains a task of very large size (29597 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 27:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# run a join test with / without memory caching\n",
    "\n",
    "print(f\"create some test data for join...\")\n",
    "ct = datetime.now()\n",
    "df1 = generateDF(num_rows = 10000000)\n",
    "df2 = generateDF(num_rows = 1000000)\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "print(f\"Saving the test DFs as parquet tables...\")\n",
    "df1.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"tab_df1\")\n",
    "df2.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"tab_df2\")\n",
    "\n",
    "#df1.write.option(\"maxRecordsPerFile\", 100000).format(\"parquet\").mode(\"overwrite\").saveAsTable(\"tab_df1\")\n",
    "#df2.write.option(\"maxRecordsPerFile\", 100000).format(\"parquet\").mode(\"overwrite\").saveAsTable(\"tab_df2\")\n",
    "print(f\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65b9207e-3d98-419c-b52b-d5ea4d9cefd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running join on test tables...\n",
      "+------+\n",
      "|   cnt|\n",
      "+------+\n",
      "|408556|\n",
      "+------+\n",
      "\n",
      "cnt: 408556\n",
      "\n",
      "Done in 0:00:01.718398\n",
      "Running join on cache (InMemoryRelation)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   cnt|\n",
      "+------+\n",
      "|408556|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 141:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnt: 408556\n",
      "\n",
      "Done in 0:00:30.645252\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# memory cache on single node gets much slower than without caching..???\n",
    "print(f\"Running join on test tables...\")\n",
    "ct = datetime.now()\n",
    "plan = spark.sql(\"\"\"\n",
    "--EXPLAIN\n",
    "SELECT COUNT(1) cnt \n",
    "FROM tab_df1 t1\n",
    "    inner join\n",
    "    tab_df2 t2\n",
    "    on (t1.id = t2.id)\n",
    "WHERE t1.age >= t2.age\n",
    "\"\"\")\n",
    "plan.show()\n",
    "show_plan(plan)\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "## cache the 2 tables\n",
    "spark.catalog.cacheTable(\"tab_df1\")\n",
    "spark.catalog.cacheTable(\"tab_df2\")\n",
    "\n",
    "print(f\"Running join on cache (InMemoryRelation)...\")\n",
    "ct = datetime.now()\n",
    "plan = spark.sql(\"\"\"\n",
    "--EXPLAIN\n",
    "SELECT COUNT(1) cnt \n",
    "FROM tab_df1 t1\n",
    "    inner join\n",
    "    tab_df2 t2\n",
    "    on (t1.id = t2.id)\n",
    "WHERE t1.age >= t2.age\n",
    "\"\"\")\n",
    "plan.show()\n",
    "show_plan(plan)\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "spark.catalog.uncacheTable(\"tab_df1\", )\n",
    "spark.catalog.uncacheTable(\"tab_df2\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "912fd2c0-cbc0-4f2e-9208-81937a5c24df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running join on test tables without partitioning...\n",
      "reading data\n",
      "+---+---+---+\n",
      "| id|age|age|\n",
      "+---+---+---+\n",
      "|  1| 49| 17|\n",
      "|  3| 89|  2|\n",
      "|  4| 98| 15|\n",
      "|  8| 85| 67|\n",
      "| 10| 94| 84|\n",
      "| 11| 80| 19|\n",
      "| 13| 83| 11|\n",
      "| 14| 68| 48|\n",
      "| 15| 81| 75|\n",
      "| 17| 48| 37|\n",
      "| 19| 63| 29|\n",
      "| 21| 68| 61|\n",
      "| 22| 98| 81|\n",
      "| 24| 34|  7|\n",
      "| 26| 77| 31|\n",
      "| 28| 25| 13|\n",
      "| 30| 61|  9|\n",
      "| 31| 99| 59|\n",
      "| 32| 58| 55|\n",
      "| 36| 75| 61|\n",
      "+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "Done in 0:00:00.613147\n",
      "Repartitioning data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 234:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 partition num: 100\n",
      "df2 partition num: 1\n",
      "Running join on test tables with partitioning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+\n",
      "|    id|age|age|\n",
      "+------+---+---+\n",
      "|662186| 80| 78|\n",
      "|583252| 46| 36|\n",
      "|862708| 67| 49|\n",
      "|577002| 73| 59|\n",
      "|491572| 79| 46|\n",
      "|280573| 49| 18|\n",
      "|258952| 39| 27|\n",
      "|257231| 94| 73|\n",
      "|319939| 92| 69|\n",
      "|954189| 46| 16|\n",
      "|619241| 75| 20|\n",
      "|527500| 61| 45|\n",
      "|930785| 19|  6|\n",
      "|900271| 94| 24|\n",
      "|935202| 25|  6|\n",
      "|461122| 83| 38|\n",
      "|636880| 62| 35|\n",
      "|658642|100|  3|\n",
      "|480626| 58| 12|\n",
      "|797179|100| 11|\n",
      "+------+---+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "Done in 0:00:07.981605\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# run a join test with / without partitioning\n",
    "# processing partitioned data on single node is slow...???\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(f\"Running join on test tables without partitioning...\")\n",
    "\n",
    "print(\"reading data\")\n",
    "df1 = spark.read.table(\"tab_df1\")\n",
    "df2 = spark.read.table(\"tab_df2\")\n",
    "\n",
    "ct = datetime.now()\n",
    "df_res = (\n",
    "    df1.alias(\"t1\")\n",
    "    .join(df2.alias(\"t2\"), \n",
    "          (col(\"t1.id\") == col(\"t2.id\")),\n",
    "         \"inner\").filter(df1.age >= df2.age)\n",
    "    .select(col(\"t1.id\"), col(\"t1.age\"), col(\"t2.age\"))\n",
    ")\n",
    "df_res.show()\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "\n",
    "print(f\"Repartitioning data...\")\n",
    "df1 = df1.repartition(100)\n",
    "\n",
    "\n",
    "print(f\"df1 partition num: {df1.rdd.getNumPartitions()}\")\n",
    "print(f\"df2 partition num: {df2.rdd.getNumPartitions()}\")\n",
    "\n",
    "print(f\"Running join on test tables with partitioning...\")\n",
    "ct = datetime.now()\n",
    "df_res = (\n",
    "    df1.alias(\"t1\")\n",
    "    .join(df2.alias(\"t2\"), \n",
    "          (col(\"t1.id\") == col(\"t2.id\")),\n",
    "         \"inner\").filter(df1.age >= df2.age)\n",
    "    .select(col(\"t1.id\"), col(\"t1.age\"), col(\"t2.age\"))\n",
    ")\n",
    "df_res.show()\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22086a6e-4150-410c-9190-8e1b9baabb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hints\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
