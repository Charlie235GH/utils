{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05249cc0-90de-47f5-b8ce-9c3043328716",
   "metadata": {},
   "source": [
    "## spark partitioning: \n",
    "    partitionBy(col) - very much dependent on the selectivity of the values (small vs large files). partition pruning possible!\n",
    "        e.g. by load_id, or by date, but date (year, month is only supported in pyspark 4.0.0)\n",
    "    repartition(partnum, col) - split the records into  <partnum> files using HASH algorythm. parallel processing ok, but no partition pruning\n",
    "    repartitionByRange(partnum, col) - split the records into  <partnum> files using RANGE algorythm, based on data sampling. parallel processing ok, but no partition pruning\n",
    "    write.option(\"maxRecordsPerFile\", recnum) - split the records into recnum per file roughly(!).  parallel processing ok, but no partition pruning\n",
    "    \n",
    "    \n",
    "## partition to_date datetype historical column into year/months by using spark 4.0.0 function?\n",
    "    partitionedBy(pyspark.sql.functions.partitioning.months(col)) - the only reasonable partitioning would be by date (e.g. month or year) which will only be supported in pyspark 4.0.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3754aaf6-2e52-4259-b5d0-d24fad7f881c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active spark session not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/08 13:40:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark version: 3.5.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/08 13:40:28 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/04/08 13:40:28 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/04/08 13:40:30 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "25/04/08 13:40:30 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore jovyan@172.17.0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|default  |\n",
      "|mytestdb |\n",
      "+---------+\n",
      "\n",
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|default           |\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/08 13:40:32 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------+-----------+\n",
      "|namespace|tableName            |isTemporary|\n",
      "+---------+---------------------+-----------+\n",
      "|mytestdb |t_my_date_partitioned|false      |\n",
      "|mytestdb |v_my_date_partitioned|false      |\n",
      "+---------+---------------------+-----------+\n",
      "\n",
      "+---------+---------------------+-----------+\n",
      "|namespace|viewName             |isTemporary|\n",
      "+---------+---------------------+-----------+\n",
      "|mytestdb |v_my_date_partitioned|false      |\n",
      "+---------+---------------------+-----------+\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# setup spark on linux\n",
    "from pyspark.sql import *\n",
    "\n",
    "if SparkSession.getActiveSession() == None:\n",
    "    print(\"Active spark session not found\")\n",
    "else:\n",
    "    print(\"Active spark session found\")\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local\")\n",
    "         .appName(\"SparkSession#1\")\n",
    "         .enableHiveSupport() # enableHiveSupport() needed to make data persistent... \n",
    "         #.config(\"spark.driver.allowMultipleContexts\", \"true\")\n",
    "         #.config(\"spark.sql.cbo.enabled\", \"true\")\n",
    "         #.config(\"spark.sql.cbo.optimizer\", \"true\")\n",
    "         .getOrCreate())\n",
    "\n",
    "print('spark version:', spark.version)\n",
    "\n",
    "spark.sql(\"Show databases\").show(truncate=False)\n",
    "spark.sql(\"select current_database()\").show(truncate=False)\n",
    "spark.sql(\"use mytestdb\")\n",
    "spark.sql(\"Show tables\").show(truncate=False)\n",
    "spark.sql(\"Show views\").show(truncate=False)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5613f2f1-5588-4254-b8d5-fcb1786a678d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating list: \n",
      "Done in 0:00:00.001444\n",
      "creating DF from list: \n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- district: integer (nullable = true)\n",
      " |-- from_date: integer (nullable = true)\n",
      " |-- to_date: integer (nullable = true)\n",
      " |-- to_date_year_month: integer (nullable = true)\n",
      "\n",
      "+---+------+---+--------+---------+--------+------------------+\n",
      "| id|  name|age|district|from_date| to_date|to_date_year_month|\n",
      "+---+------+---+--------+---------+--------+------------------+\n",
      "|  0|name-0| 75|    1001| 20260823|20270604|            202706|\n",
      "|  1|name-1| 77|    1002| 20261005|20271113|            202711|\n",
      "|  2|name-2| 67|    1004| 20261125|20261221|            202612|\n",
      "|  3|name-3| 10|    1002| 20271012|20270619|            202706|\n",
      "|  4|name-4| 81|    1000| 20260701|20270128|            202701|\n",
      "+---+------+---+--------+---------+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Done in 0:00:00.157459\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "| mytestdb|\n",
      "+---------+\n",
      "\n",
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|          mytestdb|\n",
      "+------------------+\n",
      "\n",
      "+---------+---------------------+-----------+\n",
      "|namespace|tableName            |isTemporary|\n",
      "+---------+---------------------+-----------+\n",
      "|mytestdb |t_my_date_partitioned|false      |\n",
      "|mytestdb |v_my_date_partitioned|false      |\n",
      "+---------+---------------------+-----------+\n",
      "\n",
      "Dropping table...\n",
      "Saving table t_my_date_partitioned...\n",
      "creating view v_my_date_partitioned to manage technical column DOES NOT HELP!!!...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# partitioning (hash, range, round-robin) test\n",
    "# CANNOT FIND A WAY to PARTITION a date (to_date) into date ranges!!!\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, TimestampType\n",
    "import shutil\n",
    "\n",
    "# creating large DF:\n",
    "print(\"Creating list: \")\n",
    "ct = datetime.now()\n",
    "l = []\n",
    "rows = 100\n",
    "for c in range(rows):\n",
    "    #from_date = datetime.now() + timedelta(days = random.randint(0, 1000))\n",
    "    d = datetime.now() + timedelta(days = random.randint(0, 1000))\n",
    "    from_date = int((datetime.now() + timedelta(days = random.randint(0, 1000))).strftime(\"%Y%m%d\"))\n",
    "    \n",
    "    #to_date = from_date + timedelta(days = random.randint(1, 100))\n",
    "    to_date = int((d + timedelta(days = random.randint(1, 100))).strftime(\"%Y%m%d\"))\n",
    "    \n",
    "    l.append([c, \n",
    "              ('name-' + str(c)),  \n",
    "              None if random.random() < 0.1 else random.randint(1, 100),  \n",
    "              None if random.random() < 0.1 else random.randint(1000, 1004),\n",
    "              from_date,\n",
    "              to_date,\n",
    "              to_date // 100, #int(to_date.strftime(\"%Y%m\")),\n",
    "             ])\n",
    "\n",
    "df_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"district\", IntegerType(), True),\n",
    "    StructField(\"from_date\", IntegerType(), True), # StructField(\"from_date\", DateType(), True),\n",
    "    StructField(\"to_date\", IntegerType(), True), #StructField(\"to_date\", DateType(), True),\n",
    "    # tech columns\n",
    "    StructField(\"to_date_year_month\", IntegerType(), True),\n",
    "])\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "print(\"creating DF from list: \")\n",
    "ct = datetime.now()\n",
    "df = spark.createDataFrame(l, df_schema)\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "spark.sql(\"create database if not exists mytestdb\")\n",
    "spark.sql(\"Show databases\").show()\n",
    "spark.sql(\"use mytestdb\")\n",
    "spark.sql(\"select current_database()\").show()\n",
    "spark.sql(\"Show tables\").show(truncate=False)\n",
    "\n",
    "print(\"Dropping table...\")\n",
    "spark.sql('drop table if exists t_my_date_partitioned')\n",
    "\n",
    "# save as managed \n",
    "print(\"Saving table t_my_date_partitioned...\")\n",
    "(\n",
    "    df.write\n",
    "    .option(\"header\", True)\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"to_date_year_month\") \n",
    "    .format(\"parquet\") # parquet, csv for testing / readability\n",
    "    .saveAsTable(\"t_my_date_partitioned\")\n",
    ")\n",
    "\n",
    "# creating view to manage technical column\n",
    "print(\"creating view v_my_date_partitioned to manage technical column DOES NOT HELP!!!...\")\n",
    "spark.sql(\"\"\"\n",
    "create or replace view v_my_date_partitioned as\n",
    "SELECT *\n",
    "FROM t_my_date_partitioned t\n",
    "WHERE 1 = 1\n",
    "--AND t.to_date_year_month = to_number(date_format(to_date, 'yyyyMM'),'000000')\n",
    "AND t.to_date_year_month = floor(t.to_date / 100)\n",
    "\"\"\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ee6622b-f9ed-4985-8d3f-891728167461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying the table...\n",
      "+----------+----------+---+\n",
      "|from_date |to_date   |cnt|\n",
      "+----------+----------+---+\n",
      "|2025-04-06|2025-04-22|1  |\n",
      "|2025-04-18|2025-06-21|1  |\n",
      "|2025-05-19|2025-07-01|1  |\n",
      "|2025-05-19|2025-08-15|1  |\n",
      "|2025-06-17|2025-09-08|1  |\n",
      "+----------+----------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------------------+-----------+\n",
      "|namespace|tableName            |isTemporary|\n",
      "+---------+---------------------+-----------+\n",
      "|mytestdb |t_my_date_partitioned|false      |\n",
      "+---------+---------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Querying table using SQL\n",
    "print(\"Querying the table...\")\n",
    "spark.sql(\"\"\"\n",
    "select from_date, to_date, count(1) cnt\n",
    "from t_my_date_partitioned\n",
    "group by from_date, to_date\n",
    "order by from_date, to_date nulls first\n",
    "\"\"\").show(5, truncate=False)\n",
    "\n",
    "# Drop table\n",
    "#print(\"Dropping table...\")\n",
    "#spark.sql('drop table if exists t_my_date_partitioned')\n",
    "\n",
    "spark.sql(\"Show tables\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54faa55f-8382-4fdd-8b22-ae4680ec60f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exec plan for table select with partition filter:\n",
      "plan: == Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet spark_catalog.mytestdb.t_my_date_partitioned[id#361,name#362,age#363,district#364,from_date#365,to_date#366,to_date_year_month#367] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(6 paths)[file:/home/jovyan/work/various_tests/spark/spark-warehouse/mytestdb.db..., PartitionFilters: [isnotnull(to_date_year_month#367), (to_date_year_month#367 >= 202710)], PushedFilters: [], ReadSchema: struct<id:int,name:string,age:int,district:int,from_date:int,to_date:int>\n",
      "\n",
      "\n",
      "\n",
      "exec plan for table select with partition filter and extra filter:\n",
      "plan: == Physical Plan ==\n",
      "org.apache.spark.sql.AnalysisException: [DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve \"(from_date <= to_date(20271015, yyyyMMdd))\" due to data type mismatch: the left and right operands of the binary operator have incompatible types (\"INT\" and \"DATE\").; line 7 pos 4;\n",
      "'Project ['t.id, 't.age]\n",
      "+- 'Filter (((1 = 1) AND (cast(to_date_year_month#367 as decimal(10,0)) >= cast(to_number(202710, 000000) as decimal(10,0)))) AND ((from_date#365 <= to_date(20271015, Some(yyyyMMdd), Some(GMT), false)) AND (to_date#366 > to_date(20271015, Some(yyyyMMdd), Some(GMT), false))))\n",
      "   +- SubqueryAlias t\n",
      "      +- SubqueryAlias spark_catalog.mytestdb.t_my_date_partitioned\n",
      "         +- Relation spark_catalog.mytestdb.t_my_date_partitioned[id#361,name#362,age#363,district#364,from_date#365,to_date#366,to_date_year_month#367] parquet\n",
      "\n",
      "\n",
      "\n",
      "exec plan for VIEW select with partition filter and extra filter using view.  DOES NOT HELP!!!...:\n",
      "plan: == Physical Plan ==\n",
      "*(1) Filter ((((isnotnull(to_date#366) AND isnotnull(from_date#365)) AND (cast(to_date_year_month#367 as bigint) = FLOOR((cast(to_date#366 as double) / 100.0)))) AND (from_date#365 <= 20271015)) AND (to_date#366 > 20271015))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet spark_catalog.mytestdb.t_my_date_partitioned[id#361,name#362,age#363,district#364,from_date#365,to_date#366,to_date_year_month#367] Batched: true, DataFilters: [isnotnull(to_date#366), isnotnull(from_date#365), (from_date#365 <= 20271015), (to_date#366 > 20..., Format: Parquet, Location: InMemoryFileIndex(6 paths)[file:/home/jovyan/work/various_tests/spark/spark-warehouse/mytestdb.db..., PartitionFilters: [isnotnull(to_date_year_month#367), (to_date_year_month#367 >= 202710)], PushedFilters: [IsNotNull(to_date), IsNotNull(from_date), LessThanOrEqual(from_date,20271015), GreaterThan(to_da..., ReadSchema: struct<id:int,name:string,age:int,district:int,from_date:int,to_date:int>\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show SQL explain plan\n",
    "\n",
    "def print_dict(d: dict, level: int = 0) -> None:\n",
    "    \"\"\" print optionally hiearchic dict structure nicely formatted\n",
    "    \"\"\"\n",
    "\n",
    "    sp = \"\".ljust(level * 3)\n",
    "    for k in d.keys():\n",
    "        print(f\"{sp}{k}: {d[k]}\")\n",
    "        \n",
    "        if type(d[k]) is dict:\n",
    "            print_dict(d[k], level + 1)\n",
    "            \n",
    "    if level == 0:\n",
    "        print()\n",
    "\n",
    "print(\"exec plan for table select with partition filter:\")\n",
    "plan = spark.sql(\"\"\"\n",
    "explain\n",
    "select *\n",
    "FROM t_my_date_partitioned t\n",
    "WHERE 1 = 1\n",
    "AND t.to_date_year_month >= to_number('202710','000000')\n",
    "\"\"\")\n",
    "\n",
    "for row in plan.collect():\n",
    "    None\n",
    "    print_dict(row.asDict())\n",
    "\n",
    "#####################################################################\n",
    "print(\"exec plan for table select with partition filter and extra filter:\")\n",
    "plan = spark.sql(\"\"\"\n",
    "explain\n",
    "select t.id, t.age\n",
    "FROM t_my_date_partitioned t\n",
    "WHERE 1 = 1\n",
    "AND to_date_year_month >= to_number('202710','000000')\n",
    "AND t.from_date <= to_date('20271015','yyyyMMdd')\n",
    "AND t.to_date > to_date('20271015','yyyyMMdd')\n",
    "\"\"\")\n",
    "\n",
    "for row in plan.collect():\n",
    "    print_dict(row.asDict())\n",
    "\n",
    "#####################################################################\n",
    "# date type for to_date \n",
    "print(\"exec plan for VIEW select with partition filter and extra filter using view.  DOES NOT HELP!!!...:\")\n",
    "plan = spark.sql(\"\"\"\n",
    "explain\n",
    "select *\n",
    "FROM v_my_date_partitioned t\n",
    "WHERE 1 = 1\n",
    "--AND t.to_date_year_month >= to_number('202710','000000')\n",
    "AND t.from_date <= to_date('20271015','yyyyMMdd')\n",
    "AND t.to_date > to_date('20271015','yyyyMMdd')\n",
    "\"\"\")\n",
    "\n",
    "# int type for to_date\n",
    "plan = spark.sql(\"\"\"\n",
    "explain\n",
    "select *\n",
    "FROM v_my_date_partitioned t\n",
    "WHERE 1 = 1\n",
    "--AND t.to_date_year_month >= 202710\n",
    "AND t.from_date <= 20271015\n",
    "AND t.to_date > 20271015\n",
    "\"\"\")\n",
    "\n",
    "for row in plan.collect():\n",
    "    None\n",
    "    print_dict(row.asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bff5ed03-297a-4a1e-95fa-f784afec309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DF from table...\n",
      "+---+-------+---+--------+----------+----------+------------------+\n",
      "|id |name   |age|district|from_date |to_date   |to_date_year_month|\n",
      "+---+-------+---+--------+----------+----------+------------------+\n",
      "|0  |name-0 |87 |1002    |2025-12-01|2026-02-02|202602            |\n",
      "|9  |name-9 |65 |1003    |2025-12-19|2026-02-08|202602            |\n",
      "|47 |name-47|49 |1003    |2026-01-27|2026-02-04|202602            |\n",
      "|50 |name-50|62 |1002    |2026-01-26|2026-02-25|202602            |\n",
      "+---+-------+---+--------+----------+----------+------------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "plan with partition filter (PartitionFilters[...]):\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [from_date#513 ASC NULLS FIRST, to_date#514 ASC NULLS FIRST, id#509 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(from_date#513 ASC NULLS FIRST, to_date#514 ASC NULLS FIRST, id#509 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=2040]\n",
      "      +- FileScan parquet spark_catalog.mytestdb.t_my_date_partitioned[id#509,age#511,from_date#513,to_date#514,to_date_year_month#515] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(5 paths)[file:/home/jovyan/work/various_tests/spark/spark-warehouse/mytestdb.db..., PartitionFilters: [isnotnull(to_date_year_month#515), (to_date_year_month#515 >= 202710)], PushedFilters: [], ReadSchema: struct<id:int,age:int,from_date:date,to_date:date>\n",
      "\n",
      "\n",
      "+---+----+----------+----------+------------------+\n",
      "|id |age |from_date |to_date   |to_date_year_month|\n",
      "+---+----+----------+----------+------------------+\n",
      "|25 |15  |2027-06-30|2027-10-01|202710            |\n",
      "|31 |80  |2027-07-23|2027-10-08|202710            |\n",
      "|62 |31  |2027-08-08|2027-10-01|202710            |\n",
      "|45 |23  |2027-08-15|2027-11-20|202711            |\n",
      "|17 |NULL|2027-08-28|2027-10-05|202710            |\n",
      "|91 |NULL|2027-09-01|2027-11-12|202711            |\n",
      "|40 |NULL|2027-09-14|2027-11-11|202711            |\n",
      "|15 |73  |2027-10-09|2027-12-12|202712            |\n",
      "|69 |85  |2027-10-30|2028-02-05|202802            |\n",
      "|1  |46  |2027-11-16|2027-11-29|202711            |\n",
      "|49 |39  |2027-11-20|2027-11-26|202711            |\n",
      "|66 |27  |2027-12-05|2028-02-25|202802            |\n",
      "|46 |45  |2027-12-19|2027-12-23|202712            |\n",
      "|82 |NULL|2027-12-23|2028-03-09|202803            |\n",
      "|72 |1   |2027-12-31|2028-03-13|202803            |\n",
      "+---+----+----------+----------+------------------+\n",
      "\n",
      "plan with partition filter (PartitionFilters[...]) and extra filter:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [from_date#513 ASC NULLS FIRST, to_date#514 ASC NULLS FIRST, id#509 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(from_date#513 ASC NULLS FIRST, to_date#514 ASC NULLS FIRST, id#509 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=2095]\n",
      "      +- Filter (((isnotnull(from_date#513) AND isnotnull(to_date#514)) AND (from_date#513 <= 2027-10-15)) AND (to_date#514 > 2027-10-15))\n",
      "         +- FileScan parquet spark_catalog.mytestdb.t_my_date_partitioned[id#509,age#511,from_date#513,to_date#514,to_date_year_month#515] Batched: true, DataFilters: [isnotnull(from_date#513), isnotnull(to_date#514), (from_date#513 <= 2027-10-15), (to_date#514 > ..., Format: Parquet, Location: InMemoryFileIndex(5 paths)[file:/home/jovyan/work/various_tests/spark/spark-warehouse/mytestdb.db..., PartitionFilters: [isnotnull(to_date_year_month#515), (to_date_year_month#515 >= 202710)], PushedFilters: [IsNotNull(from_date), IsNotNull(to_date), LessThanOrEqual(from_date,2027-10-15), GreaterThan(to_..., ReadSchema: struct<id:int,age:int,from_date:date,to_date:date>\n",
      "\n",
      "\n",
      "+---+----+----------+----------+------------------+\n",
      "|id |age |from_date |to_date   |to_date_year_month|\n",
      "+---+----+----------+----------+------------------+\n",
      "|45 |23  |2027-08-15|2027-11-20|202711            |\n",
      "|91 |NULL|2027-09-01|2027-11-12|202711            |\n",
      "|40 |NULL|2027-09-14|2027-11-11|202711            |\n",
      "|15 |73  |2027-10-09|2027-12-12|202712            |\n",
      "+---+----+----------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show python DF explain plan\n",
    "\n",
    "def print_dict(d: dict, level: int = 0) -> None:\n",
    "    \"\"\" print optionally hiearchic dict structure nicely formatted\n",
    "    \"\"\"\n",
    "\n",
    "    sp = \"\".ljust(level * 3)\n",
    "    for k in d.keys():\n",
    "        print(f\"{sp}{k}: {d[k]}\")\n",
    "        \n",
    "        if type(d[k]) is dict:\n",
    "            print_dict(d[k], level + 1)\n",
    "            \n",
    "    if level == 0:\n",
    "        print()\n",
    "\n",
    "print(\"Reading DF from table...\")\n",
    "spark.sql(\"use mytestdb\")\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .table(\"t_my_date_partitioned\")\n",
    ")\n",
    "df.show(4, truncate=False)\n",
    "\n",
    "print(\"plan with partition filter (PartitionFilters[...]):\")\n",
    "df1 = (\n",
    "    df\n",
    "    .filter(\"to_date_year_month >= 202710\")\n",
    "    .select(df.id, df.age, df.from_date, df.to_date, df.to_date_year_month)\n",
    "    .orderBy(df.from_date, df.to_date, df.id)\n",
    "    )\n",
    "\n",
    "df1.explain()\n",
    "\n",
    "df1.show(100, truncate=False)\n",
    "\n",
    "for row in df1.collect():\n",
    "    None\n",
    "    #print_dict(row.asDict())\n",
    "\n",
    "###################################################################\n",
    "print(\"plan with partition filter (PartitionFilters[...]) and extra filter:\")\n",
    "df1 = (\n",
    "    df\n",
    "    .filter(\"to_date_year_month >= 202710\")\n",
    "    .filter(\"from_date <= to_date('20271015','yyyyMMdd') AND to_date > to_date('20271015','yyyyMMdd')\")\n",
    "    .select(df.id, df.age, df.from_date, df.to_date, df.to_date_year_month)\n",
    "    .orderBy(df.from_date, df.to_date, df.id)\n",
    "    )\n",
    "\n",
    "df1.explain()\n",
    "\n",
    "df1.show(100, truncate=False)\n",
    "\n",
    "for row in df1.collect():\n",
    "    None\n",
    "    #print_dict(row.asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "abd03cc2-ecbe-4e6e-8cd3-594eb6bc61b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan with partition filter (PartitionFilters[...]):\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [from_date#513 ASC NULLS FIRST, to_date#514 ASC NULLS FIRST, id#509 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(from_date#513 ASC NULLS FIRST, to_date#514 ASC NULLS FIRST, id#509 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=2175]\n",
      "      +- Filter (((isnotnull(from_date#513) AND isnotnull(to_date#514)) AND (from_date#513 <= 2027-10-15)) AND (to_date#514 > 2027-10-15))\n",
      "         +- FileScan parquet spark_catalog.mytestdb.t_my_date_partitioned[id#509,age#511,from_date#513,to_date#514,to_date_year_month#515] Batched: true, DataFilters: [isnotnull(from_date#513), isnotnull(to_date#514), (from_date#513 <= 2027-10-15), (to_date#514 > ..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/work/various_tests/spark/spark-warehouse/mytestdb.db..., PartitionFilters: [isnotnull(to_date_year_month#515), (to_date_year_month#515 = 202710)], PushedFilters: [IsNotNull(from_date), IsNotNull(to_date), LessThanOrEqual(from_date,2027-10-15), GreaterThan(to_..., ReadSchema: struct<id:int,age:int,from_date:date,to_date:date>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"plan with partition filter (PartitionFilters[...]):\")\n",
    "df1 = (\n",
    "    df\n",
    "    .filter(\"to_date_year_month >= 202710\")\n",
    "    .filter(\"from_date <= to_date('20271015','yyyyMMdd') AND to_date > to_date('20271015','yyyyMMdd')\")\n",
    "    .select(df.id, df.age, df.from_date, df.to_date, df.to_date_year_month)\n",
    "    .orderBy(df.from_date, df.to_date, df.id)\n",
    "    )\n",
    "\n",
    "df1.explain()\n",
    "\n",
    "#df1.show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "552ca245-add6-40e9-957b-7788aae16442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lst size: 100\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "| mytestdb|\n",
      "|   testdb|\n",
      "+---------+\n",
      "\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n",
      "+---+-------------+----+----------+----------+\n",
      "|id |name         |age |from_dt   |to_dt     |\n",
      "+---+-------------+----+----------+----------+\n",
      "|0  |name-00000000|72  |2026-06-22|2026-07-20|\n",
      "|1  |name-00000001|19  |2026-06-27|2026-08-19|\n",
      "|2  |name-00000002|98  |2026-11-16|2026-12-11|\n",
      "|3  |name-00000003|52  |2025-09-12|2025-09-13|\n",
      "|4  |name-00000004|65  |2027-11-30|2028-02-09|\n",
      "|5  |name-00000005|18  |2027-02-20|2027-03-01|\n",
      "|6  |name-00000006|59  |2025-11-16|2025-12-15|\n",
      "|7  |name-00000007|67  |2025-05-05|2025-08-13|\n",
      "|8  |name-00000008|72  |2025-04-10|2025-04-11|\n",
      "|9  |name-00000009|29  |2025-04-23|2025-07-26|\n",
      "|10 |name-00000010|72  |2026-05-31|2026-07-30|\n",
      "|11 |name-00000011|52  |2026-01-03|2026-02-22|\n",
      "|12 |name-00000012|46  |2027-12-13|2027-12-17|\n",
      "|13 |name-00000013|NULL|2026-05-23|2026-06-24|\n",
      "|14 |name-00000014|NULL|2027-11-30|2028-01-13|\n",
      "|15 |name-00000015|NULL|2027-12-12|2028-02-26|\n",
      "|16 |name-00000016|34  |2026-08-19|2026-11-06|\n",
      "|17 |name-00000017|21  |2027-03-30|2027-06-27|\n",
      "|18 |name-00000018|7   |2025-06-25|2025-09-10|\n",
      "|19 |name-00000019|99  |2026-05-10|2026-05-20|\n",
      "+---+-------------+----+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20250408 15:00:11 : Done.\n"
     ]
    }
   ],
   "source": [
    "# partition by various methods\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, TimestampType\n",
    "\n",
    "df_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"from_dt\", DateType(), True),\n",
    "    StructField(\"to_dt\", DateType(), True),\n",
    "])\n",
    "\n",
    "rows = 100\n",
    "lst = []\n",
    "for c in range(rows):\n",
    "    from_dt = datetime.now().date() + timedelta(days=random.randint(0, 1000))\n",
    "    to_dt = from_dt + timedelta(days=random.randint(1, 100))\n",
    "\n",
    "    lst.append([\n",
    "        c,\n",
    "        'name-' + str(c).zfill(8),\n",
    "        None if random.random() < 0.1 else random.randint(1, 100),\n",
    "        from_dt,\n",
    "        to_dt\n",
    "         ])\n",
    "\n",
    "print(f\"lst size: {len(lst)}\")\n",
    "\n",
    "df = spark.createDataFrame(lst, df_schema)\n",
    "\n",
    "    \n",
    "spark.sql(\"create database if not exists testdb\")\n",
    "spark.sql(\"Show databases\").show()\n",
    "spark.sql(\"use testdb\")\n",
    "spark.sql(\"show tables\").show()\n",
    "\n",
    "df.show(truncate = False)\n",
    "\n",
    "#df.groupBy(\"to_dt\").agg(\"*\", \"count\").orderBy(\"to_dt\")\n",
    "\n",
    "# partition by all occuring to_dt values\n",
    "#df.writeTo(\"my_partitioned_table\").partitionedBy(partitioning.months(\"to_dt\")).createOrReplace() # spark 4.0.0 (buggy)\n",
    "df.write.partitionBy(\"to_dt\").mode(\"overwrite\").parquet('mydf_1.parquet')  # spark 3.5\n",
    "\n",
    "# partition by a given number of partitions using HASH\n",
    "# processing the data must be done as one, only splitting the file to chunks is achieved\n",
    "df1 = df.repartition(4, \"to_dt\")\n",
    "df1.write.mode(\"overwrite\").parquet('mydf_2.parquet')  # spark 3.5\n",
    "\n",
    "# partition by a given number of partitions using RANGE by sampling, to automatically estimate the ranges\n",
    "# processing the data must be done as one, only splitting the file to chunks is achieved\n",
    "df1 = df.repartitionByRange(4, \"to_dt\")\n",
    "df1.write.mode(\"overwrite\").parquet('mydf_3.parquet')  # spark 3.5\n",
    "\n",
    "# limit the records, thus partitioning data in file roughly(!) equally.  full data processing is necessary\n",
    "df.write.mode(\"overwrite\").option(\"maxRecordsPerFile\", 20).csv(\"mydf_4.csv\")\n",
    "\n",
    "print(datetime.now().strftime(\"%Y%m%d %H:%M:%S\"), \": Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
