{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05249cc0-90de-47f5-b8ce-9c3043328716",
   "metadata": {},
   "source": [
    "## spark partitioning: \n",
    "    partitionBy(col) - very much dependent on the selectivity of the values (small vs large files). partition pruning possible!\n",
    "        e.g. by load_id, or by date, but date (year, month is only supported in pyspark 4.0.0)\n",
    "    repartition(partnum, col) - split the records into  <partnum> files using HASH algorithm. parallel processing ok, but no partition pruning\n",
    "    repartitionByRange(partnum, col) - split the records into  <partnum> files using RANGE algorythm, based on data sampling. parallel processing ok, but no partition pruning\n",
    "    write.option(\"maxRecordsPerFile\", recnum) - split the records into recnum per file roughly(!).  parallel processing ok, but no partition pruning\n",
    "    reduceByKey() - aggregate inside a partition only\n",
    "    \n",
    "    \n",
    "## partition to_date datetype historical column into year/months by using spark 4.0.0 function?\n",
    "    partitionedBy(pyspark.sql.functions.partitioning.months(col)) - the only reasonable partitioning would be by date (e.g. month or year) which will only be supported in pyspark 4.0.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3754aaf6-2e52-4259-b5d0-d24fad7f881c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active spark session not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/13 15:29:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting Spark log level to \"ERROR\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark version: 3.5.5\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# setup spark on linux\n",
    "from pyspark.sql import *\n",
    "\n",
    "if SparkSession.getActiveSession() == None:\n",
    "    print(\"Active spark session not found\")\n",
    "else:\n",
    "    print(\"Active spark session found\")\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         #.master(\"local\") # local- no parallelizm at all, local[2] - 2 cores, local[*] - as many cores as local logical cores\n",
    "         .appName(\"SparkSession#1\")\n",
    "         .enableHiveSupport() # enableHiveSupport() needed to make data persistent... \n",
    "         .config(\"spark.log.level\", \"ERROR\")\n",
    "         #.config(\"spark.driver.allowMultipleContexts\", True)\n",
    "         #.config(\"spark.sql.cbo.enabled\", True)\n",
    "         #.config(\"spark.sql.cbo.optimizer\", True)\n",
    "         #.config(\"spark.executor.memory\", \"4g\")\n",
    "         #.config(\"spark.driver.memory\", \"4g\")\n",
    "         .getOrCreate())\n",
    "\n",
    "print('spark version:', spark.version)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5e7f452-ade6-4f7a-b4de-2c71eeaadfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.app.startTime', '1747143032136'), ('spark.executor.memory', '4g'), ('spark.app.submitTime', '1747142959124'), ('spark.driver.allowMultipleContexts', 'true'), ('spark.executor.id', 'driver'), ('spark.app.name', 'SparkSession#1'), ('spark.executor.cores', '2'), ('spark.cores.max', '4'), ('spark.driver.host', 'jupyternotebook'), ('spark.log.level', 'ERROR'), ('spark.sql.catalogImplementation', 'hive'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.driver.memory', '2g'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true'), ('spark.driver.port', '40957'), ('spark.app.id', 'local-1747143032208')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting Spark log level to \"ERROR\".\n"
     ]
    }
   ],
   "source": [
    "# change some configs\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    None\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "        #.master(\"local\") # local- no parallelizm at all, local[2] - 2 cores, local[*] - as many cores as local logical cores\n",
    "        .appName(\"SparkSession#1\")\n",
    "        .enableHiveSupport() # enableHiveSupport() needed to make data persistent... \n",
    "        .config(\"spark.log.level\", \"ERROR\")\n",
    "        .config(\"spark.driver.allowMultipleContexts\", True)\n",
    "        .config('spark.cores.max', '4')\n",
    "        .config('spark.executor.cores', '2')\n",
    "        .config('spark.driver.memory','2g')\n",
    "        .config('spark.executor.memory', '4g')\n",
    "        .getOrCreate())\n",
    "\n",
    "# check config\n",
    "sc = spark.sparkContext\n",
    "config = sc.getConf().getAll()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3717564-f3e2-42f3-87d5-2e40bd4a6f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# stop spark session (localhost:4040 will be reset, unaccessible until a new session starts)\n",
    "spark.stop()\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6772fa54-f937-41f3-b685-39a05d0e502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 20\n",
      "0, Store_75, Product_422, 23\n",
      "1, Store_38, Product_71, 71\n",
      "2, Store_41, Product_123, 10\n",
      "3, Store_69, Product_411, 61\n",
      "4, Store_15, Product_881, 71\n",
      "5, Store_24, Product_621, 23\n",
      "6, Store_53, Product_416, 55\n",
      "7, Store_15, Product_269, 34\n",
      "8, Store_70, Product_497, 91\n",
      "9, Store_82, Product_11, 97\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# test without any narrow (operates within a partition, no shuffling) or wide (operates on multiple partitions, shuffling is required) function\n",
    "\n",
    "import random\n",
    "\n",
    "lst = [\n",
    "    (\n",
    "        c, \n",
    "        f'Store_{random.randint(1, 100)}',\n",
    "        f'Product_{random.randint(1, 1000)}',  \n",
    "        random.randint(5, 100)\n",
    "     )\n",
    "    for c in range(100000)]\n",
    "rdd = spark.sparkContext.parallelize(lst, 20)\n",
    "\n",
    "print(f\"Number of partitions: {rdd.getNumPartitions()}\")\n",
    "\n",
    "for row in rdd.take(10):\n",
    "    print(f\"{row[0]}, {row[1]}, {row[2]}, {row[3]}\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e548f1a-da15-4ed2-99c3-f695f7483b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions by default: 20\n",
      "0, Store_75, Product_422, 20.7\n",
      "1, Store_38, Product_71, 63.9\n",
      "2, Store_41, Product_123, 9.0\n",
      "3, Store_69, Product_411, 54.9\n",
      "4, Store_15, Product_881, 63.9\n",
      "5, Store_24, Product_621, 20.7\n",
      "6, Store_53, Product_416, 49.5\n",
      "7, Store_15, Product_269, 30.6\n",
      "8, Store_70, Product_497, 81.9\n",
      "9, Store_82, Product_11, 87.3\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# apply narrow transformation (e.g.: map)\n",
    "discounted_rdd = rdd.map(lambda x: (x[0], x[1], x[2], 0 if x[3] is None else round(x[3]*0.9, 2)))\n",
    "print(f\"Number of partitions by default: {discounted_rdd.getNumPartitions()}\")\n",
    "\n",
    "for row in discounted_rdd.take(10): # take() makes the rdd an iterable list\n",
    "    print(f\"{row[0]}, {row[1]}, {row[2]}, {row[3]}\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8190116a-2ea4-4d7a-837a-b156ab0d6abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store_sales_rdd:\n",
      "Store_79, 66.6\n",
      "Store_50, 49.5\n",
      "Store_70, 52.2\n",
      "Store_3, 48.6\n",
      "Store_77, 89.1\n",
      "Store_61, 58.5\n",
      "Store_10, 14.4\n",
      "Store_71, 5.4\n",
      "Store_85, 51.3\n",
      "Store_21, 9.0\n",
      "total_sales_rdd (sorted - all):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:====================================>                   (13 + 4) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store_1, 47258.1\n",
      "Store_10, 47618.1\n",
      "Store_100, 44203.5\n",
      "Store_11, 46812.6\n",
      "Store_12, 47889.9\n",
      "Store_13, 46645.2\n",
      "Store_14, 48726.9\n",
      "Store_15, 47983.50000000001\n",
      "Store_16, 46007.999999999985\n",
      "Store_17, 49195.799999999996\n",
      "Store_18, 49141.799999999996\n",
      "Store_19, 45961.2\n",
      "Store_2, 48431.70000000001\n",
      "Store_20, 49540.5\n",
      "Store_21, 44891.99999999999\n",
      "Store_22, 45115.20000000001\n",
      "Store_23, 49547.7\n",
      "Store_24, 48068.10000000001\n",
      "Store_25, 49866.29999999999\n",
      "Store_26, 44555.399999999994\n",
      "Store_27, 50857.200000000004\n",
      "Store_28, 45452.700000000004\n",
      "Store_29, 47119.5\n",
      "Store_3, 45395.100000000006\n",
      "Store_30, 46899.9\n",
      "Store_31, 46566.899999999994\n",
      "Store_32, 49765.5\n",
      "Store_33, 47819.7\n",
      "Store_34, 46121.399999999994\n",
      "Store_35, 46304.100000000006\n",
      "Store_36, 48351.59999999999\n",
      "Store_37, 47301.299999999996\n",
      "Store_38, 48435.29999999999\n",
      "Store_39, 48004.19999999999\n",
      "Store_4, 44557.2\n",
      "Store_40, 48171.60000000001\n",
      "Store_41, 48733.200000000004\n",
      "Store_42, 44543.700000000004\n",
      "Store_43, 47921.4\n",
      "Store_44, 47403.00000000001\n",
      "Store_45, 47803.5\n",
      "Store_46, 46477.8\n",
      "Store_47, 49269.6\n",
      "Store_48, 44699.399999999994\n",
      "Store_49, 47001.6\n",
      "Store_5, 46674.9\n",
      "Store_50, 45770.399999999994\n",
      "Store_51, 45285.3\n",
      "Store_52, 45734.40000000001\n",
      "Store_53, 48124.8\n",
      "Store_54, 47993.4\n",
      "Store_55, 48740.40000000001\n",
      "Store_56, 49410.9\n",
      "Store_57, 43327.79999999999\n",
      "Store_58, 46639.8\n",
      "Store_59, 46913.4\n",
      "Store_6, 46785.600000000006\n",
      "Store_60, 46855.80000000001\n",
      "Store_61, 46918.8\n",
      "Store_62, 46898.1\n",
      "Store_63, 48461.40000000001\n",
      "Store_64, 47978.1\n",
      "Store_65, 46005.30000000001\n",
      "Store_66, 49043.7\n",
      "Store_67, 50541.299999999996\n",
      "Store_68, 44768.700000000004\n",
      "Store_69, 47758.50000000001\n",
      "Store_7, 47249.999999999985\n",
      "Store_70, 47805.30000000001\n",
      "Store_71, 47751.3\n",
      "Store_72, 46360.8\n",
      "Store_73, 48249.00000000001\n",
      "Store_74, 47635.200000000004\n",
      "Store_75, 46860.3\n",
      "Store_76, 45225.00000000001\n",
      "Store_77, 48024.89999999999\n",
      "Store_78, 49126.5\n",
      "Store_79, 48729.600000000006\n",
      "Store_8, 44458.200000000004\n",
      "Store_80, 45701.10000000001\n",
      "Store_81, 47203.19999999999\n",
      "Store_82, 49289.40000000001\n",
      "Store_83, 48330.90000000001\n",
      "Store_84, 48789.90000000001\n",
      "Store_85, 49571.1\n",
      "Store_86, 46552.5\n",
      "Store_87, 50658.299999999996\n",
      "Store_88, 47236.49999999999\n",
      "Store_89, 45374.40000000001\n",
      "Store_9, 44613.0\n",
      "Store_90, 47113.19999999998\n",
      "Store_91, 48395.70000000001\n",
      "Store_92, 45947.7\n",
      "Store_93, 49730.4\n",
      "Store_94, 48683.7\n",
      "Store_95, 47195.1\n",
      "Store_96, 45873.00000000001\n",
      "Store_97, 47808.899999999994\n",
      "Store_98, 43676.99999999999\n",
      "Store_99, 47575.799999999996\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# apply wide transformation (e.g.: reducebyKey, because the same store could be in different partitions)\n",
    "store_sales_rdd = discounted_rdd.map(lambda x: (x[1], x[3]))  # key value pairs for store / sold-price\n",
    "\n",
    "print(\"store_sales_rdd:\")\n",
    "for row in store_sales_rdd.take(10):\n",
    "    print(f\"{row[0]}, {row[1]}\")\n",
    "    \n",
    "total_sales_rdd = store_sales_rdd.reduceByKey(lambda x, y: x + y) # merge the values(x,y) per key according to lambda function (addition)\n",
    "\n",
    "print(\"total_sales_rdd (sorted - all):\")\n",
    "for row in total_sales_rdd.sortBy(lambda x: x[0]).collect(): # sort by x[0]\n",
    "    print(f\"{row[0]}, {row[1]}\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "537b9c09-05c1-4b9e-b6ce-58f814ec6ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 1, count: 100000\n",
      "Store_75, 47582.09999999999\n",
      "Store_38, 46957.499999999985\n",
      "Done in 0:00:00.648159\n",
      "Number of partitions: 2, count: 100000\n",
      "Store_38, 46957.50000000001\n",
      "Store_41, 46283.40000000004\n",
      "Done in 0:00:00.554230\n",
      "Number of partitions: 4, count: 100000\n",
      "Store_38, 46957.5\n",
      "Store_53, 46169.09999999999\n",
      "Done in 0:00:00.574467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 20, count: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store_91, 46575.0\n",
      "Store_5, 46870.200000000004\n",
      "Done in 0:00:02.060508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 50, count: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:====================================================>   (47 + 3) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store_5, 46870.19999999999\n",
      "Store_43, 49484.69999999998\n",
      "Done in 0:00:05.178291\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# performance of count() with different parititioning numbers\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# partitions: 1\n",
    "rdd = spark.sparkContext.parallelize(lst, 1)\n",
    "ct = datetime.now()\n",
    "print(f\"Number of partitions: {rdd.getNumPartitions()}, count: {rdd.count()}\")\n",
    "discounted_rdd = rdd.map(lambda x: (x[0], x[1], x[2], 0 if x[3] is None else round(x[3]*0.9, 2)))\n",
    "store_sales_rdd = discounted_rdd.map(lambda x: (x[1], x[3]))  # key value pairs for store / sold-price\n",
    "total_sales_rdd = store_sales_rdd.reduceByKey(lambda x, y: x + y)\n",
    "for row in total_sales_rdd.take(2):\n",
    "    print(f\"{row[0]}, {row[1]}\")\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "# partitions: 2\n",
    "rdd = spark.sparkContext.parallelize(lst, 2)\n",
    "ct = datetime.now()\n",
    "print(f\"Number of partitions: {rdd.getNumPartitions()}, count: {rdd.count()}\")\n",
    "discounted_rdd = rdd.map(lambda x: (x[0], x[1], x[2], 0 if x[3] is None else round(x[3]*0.9, 2)))\n",
    "store_sales_rdd = discounted_rdd.map(lambda x: (x[1], x[3]))  # key value pairs for store / sold-price\n",
    "total_sales_rdd = store_sales_rdd.reduceByKey(lambda x, y: x + y)\n",
    "for row in total_sales_rdd.take(2):\n",
    "    print(f\"{row[0]}, {row[1]}\")\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "# partitions: 4\n",
    "rdd = spark.sparkContext.parallelize(lst, 4)\n",
    "ct = datetime.now()\n",
    "print(f\"Number of partitions: {rdd.getNumPartitions()}, count: {rdd.count()}\")\n",
    "discounted_rdd = rdd.map(lambda x: (x[0], x[1], x[2], 0 if x[3] is None else round(x[3]*0.9, 2)))\n",
    "store_sales_rdd = discounted_rdd.map(lambda x: (x[1], x[3]))  # key value pairs for store / sold-price\n",
    "total_sales_rdd = store_sales_rdd.reduceByKey(lambda x, y: x + y)\n",
    "for row in total_sales_rdd.take(2):\n",
    "    print(f\"{row[0]}, {row[1]}\")\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "# partitions: 20\n",
    "rdd = spark.sparkContext.parallelize(lst, 20)\n",
    "ct = datetime.now()\n",
    "print(f\"Number of partitions: {rdd.getNumPartitions()}, count: {rdd.count()}\")\n",
    "discounted_rdd = rdd.map(lambda x: (x[0], x[1], x[2], 0 if x[3] is None else round(x[3]*0.9, 2)))\n",
    "store_sales_rdd = discounted_rdd.map(lambda x: (x[1], x[3]))  # key value pairs for store / sold-price\n",
    "total_sales_rdd = store_sales_rdd.reduceByKey(lambda x, y: x + y)\n",
    "for row in total_sales_rdd.take(2):\n",
    "    print(f\"{row[0]}, {row[1]}\")\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "# partitions: 50\n",
    "rdd = spark.sparkContext.parallelize(lst, 50)\n",
    "ct = datetime.now()\n",
    "print(f\"Number of partitions: {rdd.getNumPartitions()}, count: {rdd.count()}\")\n",
    "discounted_rdd = rdd.map(lambda x: (x[0], x[1], x[2], 0 if x[3] is None else round(x[3]*0.9, 2)))\n",
    "store_sales_rdd = discounted_rdd.map(lambda x: (x[1], x[3]))  # key value pairs for store / sold-price\n",
    "total_sales_rdd = store_sales_rdd.reduceByKey(lambda x, y: x + y)\n",
    "for row in total_sales_rdd.take(2):\n",
    "    print(f\"{row[0]}, {row[1]}\")\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d35b9542-a0f5-4fd7-9b16-a013bf549013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating without cache (1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Store_5', 46870.19999999999), ('Store_43', 49484.69999999998), ('Store_45', 45533.7), ('Store_92', 48852.00000000001), ('Store_35', 43554.6), ('Store_32', 47685.6), ('Store_83', 48815.10000000001), ('Store_86', 48418.200000000004), ('Store_99', 49761.00000000001), ('Store_44', 46052.1), ('Store_42', 54375.299999999996), ('Store_26', 46850.4), ('Store_6', 47938.5), ('Store_78', 48230.1), ('Store_53', 46169.1), ('Store_60', 48076.200000000004), ('Store_11', 48035.70000000002), ('Store_76', 50866.20000000001), ('Store_91', 46575.0), ('Store_77', 46448.09999999999), ('Store_2', 48123.0), ('Store_97', 44341.200000000004), ('Store_34', 43551.89999999999), ('Store_82', 46419.3), ('Store_90', 47033.10000000002), ('Store_13', 49746.600000000006), ('Store_72', 48070.799999999996), ('Store_79', 47737.80000000001), ('Store_71', 46295.09999999999), ('Store_14', 44536.49999999999), ('Store_57', 44807.4), ('Store_63', 46746.90000000001), ('Store_41', 46283.4), ('Store_47', 46030.50000000001), ('Store_84', 45423.00000000001), ('Store_95', 48535.2), ('Store_36', 48034.80000000001), ('Store_40', 45870.299999999996), ('Store_50', 49016.69999999999), ('Store_49', 49614.30000000001), ('Store_81', 45366.299999999996), ('Store_10', 50695.200000000004), ('Store_51', 48934.79999999998), ('Store_55', 48852.9), ('Store_59', 48716.100000000006), ('Store_94', 47904.3), ('Store_88', 45821.69999999998), ('Store_67', 47731.5), ('Store_8', 45753.299999999996), ('Store_25', 47561.4), ('Store_33', 47780.99999999999), ('Store_68', 48503.69999999998), ('Store_27', 46578.600000000006), ('Store_18', 47193.29999999999), ('Store_52', 49655.69999999998), ('Store_38', 46957.49999999999), ('Store_29', 46940.4), ('Store_12', 45073.8), ('Store_3', 48351.6), ('Store_46', 44900.100000000006), ('Store_16', 45898.19999999999), ('Store_21', 48510.0), ('Store_9', 51319.80000000001), ('Store_98', 49287.600000000006), ('Store_1', 45810.89999999999), ('Store_73', 44778.6), ('Store_96', 48907.79999999998), ('Store_23', 46700.100000000006), ('Store_75', 47582.100000000006), ('Store_15', 52423.20000000001), ('Store_19', 47031.3), ('Store_62', 44961.3), ('Store_93', 48809.7), ('Store_58', 47890.79999999999), ('Store_37', 48234.600000000006), ('Store_22', 47962.799999999996), ('Store_89', 44500.5), ('Store_61', 47812.500000000015), ('Store_87', 43372.80000000001), ('Store_24', 45510.299999999996), ('Store_31', 47892.6), ('Store_54', 46744.2), ('Store_70', 46935.9), ('Store_20', 47242.8), ('Store_39', 50463.899999999994), ('Store_80', 45804.60000000001), ('Store_30', 45563.39999999999), ('Store_74', 45777.600000000006), ('Store_28', 48878.09999999999), ('Store_69', 46978.200000000004), ('Store_56', 49460.39999999999), ('Store_100', 46266.300000000025), ('Store_65', 48387.6), ('Store_48', 50833.79999999999), ('Store_7', 48561.3), ('Store_4', 45411.299999999996), ('Store_66', 45571.50000000001), ('Store_85', 48628.79999999999), ('Store_17', 44178.299999999996), ('Store_64', 48771.900000000016)]\n",
      "Done in 0:00:04.889445\n",
      "Calculating without cache (2)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Store_5', 46870.19999999999), ('Store_43', 49484.69999999998), ('Store_45', 45533.7), ('Store_92', 48852.00000000001), ('Store_35', 43554.6), ('Store_32', 47685.6), ('Store_83', 48815.10000000001), ('Store_86', 48418.200000000004), ('Store_99', 49761.00000000001), ('Store_44', 46052.1), ('Store_42', 54375.299999999996), ('Store_26', 46850.4), ('Store_6', 47938.5), ('Store_78', 48230.1), ('Store_53', 46169.1), ('Store_60', 48076.200000000004), ('Store_11', 48035.70000000002), ('Store_76', 50866.20000000001), ('Store_91', 46575.0), ('Store_77', 46448.09999999999), ('Store_2', 48123.0), ('Store_97', 44341.200000000004), ('Store_34', 43551.89999999999), ('Store_82', 46419.3), ('Store_90', 47033.10000000002), ('Store_13', 49746.600000000006), ('Store_72', 48070.799999999996), ('Store_79', 47737.80000000001), ('Store_71', 46295.09999999999), ('Store_14', 44536.49999999999), ('Store_57', 44807.4), ('Store_63', 46746.90000000001), ('Store_41', 46283.4), ('Store_47', 46030.50000000001), ('Store_84', 45423.00000000001), ('Store_95', 48535.2), ('Store_36', 48034.80000000001), ('Store_40', 45870.299999999996), ('Store_50', 49016.69999999999), ('Store_49', 49614.30000000001), ('Store_81', 45366.299999999996), ('Store_10', 50695.200000000004), ('Store_51', 48934.79999999998), ('Store_55', 48852.9), ('Store_59', 48716.100000000006), ('Store_94', 47904.3), ('Store_88', 45821.69999999998), ('Store_67', 47731.5), ('Store_8', 45753.299999999996), ('Store_25', 47561.4), ('Store_33', 47780.99999999999), ('Store_68', 48503.69999999998), ('Store_27', 46578.600000000006), ('Store_18', 47193.29999999999), ('Store_52', 49655.69999999998), ('Store_38', 46957.49999999999), ('Store_29', 46940.4), ('Store_12', 45073.8), ('Store_3', 48351.6), ('Store_46', 44900.100000000006), ('Store_16', 45898.19999999999), ('Store_21', 48510.0), ('Store_9', 51319.80000000001), ('Store_98', 49287.600000000006), ('Store_1', 45810.89999999999), ('Store_73', 44778.6), ('Store_96', 48907.79999999998), ('Store_23', 46700.100000000006), ('Store_75', 47582.100000000006), ('Store_15', 52423.20000000001), ('Store_19', 47031.3), ('Store_62', 44961.3), ('Store_93', 48809.7), ('Store_58', 47890.79999999999), ('Store_37', 48234.600000000006), ('Store_22', 47962.799999999996), ('Store_89', 44500.5), ('Store_61', 47812.500000000015), ('Store_87', 43372.80000000001), ('Store_24', 45510.299999999996), ('Store_31', 47892.6), ('Store_54', 46744.2), ('Store_70', 46935.9), ('Store_20', 47242.8), ('Store_39', 50463.899999999994), ('Store_80', 45804.60000000001), ('Store_30', 45563.39999999999), ('Store_74', 45777.600000000006), ('Store_28', 48878.09999999999), ('Store_69', 46978.200000000004), ('Store_56', 49460.39999999999), ('Store_100', 46266.300000000025), ('Store_65', 48387.6), ('Store_48', 50833.79999999999), ('Store_7', 48561.3), ('Store_4', 45411.299999999996), ('Store_66', 45571.50000000001), ('Store_85', 48628.79999999999), ('Store_17', 44178.299999999996), ('Store_64', 48771.900000000016)]\n",
      "Done in 0:00:01.978048\n",
      "Calculating with cache (1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Store_5', 46870.19999999999), ('Store_43', 49484.69999999998), ('Store_45', 45533.7), ('Store_92', 48852.00000000001), ('Store_35', 43554.6), ('Store_32', 47685.6), ('Store_83', 48815.10000000001), ('Store_86', 48418.200000000004), ('Store_99', 49761.00000000001), ('Store_44', 46052.1), ('Store_42', 54375.299999999996), ('Store_26', 46850.4), ('Store_6', 47938.5), ('Store_78', 48230.1), ('Store_53', 46169.1), ('Store_60', 48076.200000000004), ('Store_11', 48035.70000000002), ('Store_76', 50866.20000000001), ('Store_91', 46575.0), ('Store_77', 46448.09999999999), ('Store_2', 48123.0), ('Store_97', 44341.200000000004), ('Store_34', 43551.89999999999), ('Store_82', 46419.3), ('Store_90', 47033.10000000002), ('Store_13', 49746.600000000006), ('Store_72', 48070.799999999996), ('Store_79', 47737.80000000001), ('Store_71', 46295.09999999999), ('Store_14', 44536.49999999999), ('Store_57', 44807.4), ('Store_63', 46746.90000000001), ('Store_41', 46283.4), ('Store_47', 46030.50000000001), ('Store_84', 45423.00000000001), ('Store_95', 48535.2), ('Store_36', 48034.80000000001), ('Store_40', 45870.299999999996), ('Store_50', 49016.69999999999), ('Store_49', 49614.30000000001), ('Store_81', 45366.299999999996), ('Store_10', 50695.200000000004), ('Store_51', 48934.79999999998), ('Store_55', 48852.9), ('Store_59', 48716.100000000006), ('Store_94', 47904.3), ('Store_88', 45821.69999999998), ('Store_67', 47731.5), ('Store_8', 45753.299999999996), ('Store_25', 47561.4), ('Store_33', 47780.99999999999), ('Store_68', 48503.69999999998), ('Store_27', 46578.600000000006), ('Store_18', 47193.29999999999), ('Store_52', 49655.69999999998), ('Store_38', 46957.49999999999), ('Store_29', 46940.4), ('Store_12', 45073.8), ('Store_3', 48351.6), ('Store_46', 44900.100000000006), ('Store_16', 45898.19999999999), ('Store_21', 48510.0), ('Store_9', 51319.80000000001), ('Store_98', 49287.600000000006), ('Store_1', 45810.89999999999), ('Store_73', 44778.6), ('Store_96', 48907.79999999998), ('Store_23', 46700.100000000006), ('Store_75', 47582.100000000006), ('Store_15', 52423.20000000001), ('Store_19', 47031.3), ('Store_62', 44961.3), ('Store_93', 48809.7), ('Store_58', 47890.79999999999), ('Store_37', 48234.600000000006), ('Store_22', 47962.799999999996), ('Store_89', 44500.5), ('Store_61', 47812.500000000015), ('Store_87', 43372.80000000001), ('Store_24', 45510.299999999996), ('Store_31', 47892.6), ('Store_54', 46744.2), ('Store_70', 46935.9), ('Store_20', 47242.8), ('Store_39', 50463.899999999994), ('Store_80', 45804.60000000001), ('Store_30', 45563.39999999999), ('Store_74', 45777.600000000006), ('Store_28', 48878.09999999999), ('Store_69', 46978.200000000004), ('Store_56', 49460.39999999999), ('Store_100', 46266.300000000025), ('Store_65', 48387.6), ('Store_48', 50833.79999999999), ('Store_7', 48561.3), ('Store_4', 45411.299999999996), ('Store_66', 45571.50000000001), ('Store_85', 48628.79999999999), ('Store_17', 44178.299999999996), ('Store_64', 48771.900000000016)]\n",
      "Done in 0:00:04.615173\n",
      "Calculating with cache (2)...\n",
      "[('Store_5', 46870.19999999999), ('Store_43', 49484.69999999998), ('Store_45', 45533.7), ('Store_92', 48852.00000000001), ('Store_35', 43554.6), ('Store_32', 47685.6), ('Store_83', 48815.10000000001), ('Store_86', 48418.200000000004), ('Store_99', 49761.00000000001), ('Store_44', 46052.1), ('Store_42', 54375.299999999996), ('Store_26', 46850.4), ('Store_6', 47938.5), ('Store_78', 48230.1), ('Store_53', 46169.1), ('Store_60', 48076.200000000004), ('Store_11', 48035.70000000002), ('Store_76', 50866.20000000001), ('Store_91', 46575.0), ('Store_77', 46448.09999999999), ('Store_2', 48123.0), ('Store_97', 44341.200000000004), ('Store_34', 43551.89999999999), ('Store_82', 46419.3), ('Store_90', 47033.10000000002), ('Store_13', 49746.600000000006), ('Store_72', 48070.799999999996), ('Store_79', 47737.80000000001), ('Store_71', 46295.09999999999), ('Store_14', 44536.49999999999), ('Store_57', 44807.4), ('Store_63', 46746.90000000001), ('Store_41', 46283.4), ('Store_47', 46030.50000000001), ('Store_84', 45423.00000000001), ('Store_95', 48535.2), ('Store_36', 48034.80000000001), ('Store_40', 45870.299999999996), ('Store_50', 49016.69999999999), ('Store_49', 49614.30000000001), ('Store_81', 45366.299999999996), ('Store_10', 50695.200000000004), ('Store_51', 48934.79999999998), ('Store_55', 48852.9), ('Store_59', 48716.100000000006), ('Store_94', 47904.3), ('Store_88', 45821.69999999998), ('Store_67', 47731.5), ('Store_8', 45753.299999999996), ('Store_25', 47561.4), ('Store_33', 47780.99999999999), ('Store_68', 48503.69999999998), ('Store_27', 46578.600000000006), ('Store_18', 47193.29999999999), ('Store_52', 49655.69999999998), ('Store_38', 46957.49999999999), ('Store_29', 46940.4), ('Store_12', 45073.8), ('Store_3', 48351.6), ('Store_46', 44900.100000000006), ('Store_16', 45898.19999999999), ('Store_21', 48510.0), ('Store_9', 51319.80000000001), ('Store_98', 49287.600000000006), ('Store_1', 45810.89999999999), ('Store_73', 44778.6), ('Store_96', 48907.79999999998), ('Store_23', 46700.100000000006), ('Store_75', 47582.100000000006), ('Store_15', 52423.20000000001), ('Store_19', 47031.3), ('Store_62', 44961.3), ('Store_93', 48809.7), ('Store_58', 47890.79999999999), ('Store_37', 48234.600000000006), ('Store_22', 47962.799999999996), ('Store_89', 44500.5), ('Store_61', 47812.500000000015), ('Store_87', 43372.80000000001), ('Store_24', 45510.299999999996), ('Store_31', 47892.6), ('Store_54', 46744.2), ('Store_70', 46935.9), ('Store_20', 47242.8), ('Store_39', 50463.899999999994), ('Store_80', 45804.60000000001), ('Store_30', 45563.39999999999), ('Store_74', 45777.600000000006), ('Store_28', 48878.09999999999), ('Store_69', 46978.200000000004), ('Store_56', 49460.39999999999), ('Store_100', 46266.300000000025), ('Store_65', 48387.6), ('Store_48', 50833.79999999999), ('Store_7', 48561.3), ('Store_4', 45411.299999999996), ('Store_66', 45571.50000000001), ('Store_85', 48628.79999999999), ('Store_17', 44178.299999999996), ('Store_64', 48771.900000000016)]\n",
      "Done in 0:00:00.075914\n",
      "Calculating with persist (1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 89:==================================================>     (45 + 4) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Store_5', 46870.19999999999), ('Store_43', 49484.69999999998), ('Store_45', 45533.7), ('Store_92', 48852.00000000001), ('Store_35', 43554.6), ('Store_32', 47685.6), ('Store_83', 48815.10000000001), ('Store_86', 48418.200000000004), ('Store_99', 49761.00000000001), ('Store_44', 46052.1), ('Store_42', 54375.299999999996), ('Store_26', 46850.4), ('Store_6', 47938.5), ('Store_78', 48230.1), ('Store_53', 46169.1), ('Store_60', 48076.200000000004), ('Store_11', 48035.70000000002), ('Store_76', 50866.20000000001), ('Store_91', 46575.0), ('Store_77', 46448.09999999999), ('Store_2', 48123.0), ('Store_97', 44341.200000000004), ('Store_34', 43551.89999999999), ('Store_82', 46419.3), ('Store_90', 47033.10000000002), ('Store_13', 49746.600000000006), ('Store_72', 48070.799999999996), ('Store_79', 47737.80000000001), ('Store_71', 46295.09999999999), ('Store_14', 44536.49999999999), ('Store_57', 44807.4), ('Store_63', 46746.90000000001), ('Store_41', 46283.4), ('Store_47', 46030.50000000001), ('Store_84', 45423.00000000001), ('Store_95', 48535.2), ('Store_36', 48034.80000000001), ('Store_40', 45870.299999999996), ('Store_50', 49016.69999999999), ('Store_49', 49614.30000000001), ('Store_81', 45366.299999999996), ('Store_10', 50695.200000000004), ('Store_51', 48934.79999999998), ('Store_55', 48852.9), ('Store_59', 48716.100000000006), ('Store_94', 47904.3), ('Store_88', 45821.69999999998), ('Store_67', 47731.5), ('Store_8', 45753.299999999996), ('Store_25', 47561.4), ('Store_33', 47780.99999999999), ('Store_68', 48503.69999999998), ('Store_27', 46578.600000000006), ('Store_18', 47193.29999999999), ('Store_52', 49655.69999999998), ('Store_38', 46957.49999999999), ('Store_29', 46940.4), ('Store_12', 45073.8), ('Store_3', 48351.6), ('Store_46', 44900.100000000006), ('Store_16', 45898.19999999999), ('Store_21', 48510.0), ('Store_9', 51319.80000000001), ('Store_98', 49287.600000000006), ('Store_1', 45810.89999999999), ('Store_73', 44778.6), ('Store_96', 48907.79999999998), ('Store_23', 46700.100000000006), ('Store_75', 47582.100000000006), ('Store_15', 52423.20000000001), ('Store_19', 47031.3), ('Store_62', 44961.3), ('Store_93', 48809.7), ('Store_58', 47890.79999999999), ('Store_37', 48234.600000000006), ('Store_22', 47962.799999999996), ('Store_89', 44500.5), ('Store_61', 47812.500000000015), ('Store_87', 43372.80000000001), ('Store_24', 45510.299999999996), ('Store_31', 47892.6), ('Store_54', 46744.2), ('Store_70', 46935.9), ('Store_20', 47242.8), ('Store_39', 50463.899999999994), ('Store_80', 45804.60000000001), ('Store_30', 45563.39999999999), ('Store_74', 45777.600000000006), ('Store_28', 48878.09999999999), ('Store_69', 46978.200000000004), ('Store_56', 49460.39999999999), ('Store_100', 46266.300000000025), ('Store_65', 48387.6), ('Store_48', 50833.79999999999), ('Store_7', 48561.3), ('Store_4', 45411.299999999996), ('Store_66', 45571.50000000001), ('Store_85', 48628.79999999999), ('Store_17', 44178.299999999996), ('Store_64', 48771.900000000016)]\n",
      "Done in 0:00:04.032516\n",
      "Calculating with persist (2)...\n",
      "[('Store_5', 46870.19999999999), ('Store_43', 49484.69999999998), ('Store_45', 45533.7), ('Store_92', 48852.00000000001), ('Store_35', 43554.6), ('Store_32', 47685.6), ('Store_83', 48815.10000000001), ('Store_86', 48418.200000000004), ('Store_99', 49761.00000000001), ('Store_44', 46052.1), ('Store_42', 54375.299999999996), ('Store_26', 46850.4), ('Store_6', 47938.5), ('Store_78', 48230.1), ('Store_53', 46169.1), ('Store_60', 48076.200000000004), ('Store_11', 48035.70000000002), ('Store_76', 50866.20000000001), ('Store_91', 46575.0), ('Store_77', 46448.09999999999), ('Store_2', 48123.0), ('Store_97', 44341.200000000004), ('Store_34', 43551.89999999999), ('Store_82', 46419.3), ('Store_90', 47033.10000000002), ('Store_13', 49746.600000000006), ('Store_72', 48070.799999999996), ('Store_79', 47737.80000000001), ('Store_71', 46295.09999999999), ('Store_14', 44536.49999999999), ('Store_57', 44807.4), ('Store_63', 46746.90000000001), ('Store_41', 46283.4), ('Store_47', 46030.50000000001), ('Store_84', 45423.00000000001), ('Store_95', 48535.2), ('Store_36', 48034.80000000001), ('Store_40', 45870.299999999996), ('Store_50', 49016.69999999999), ('Store_49', 49614.30000000001), ('Store_81', 45366.299999999996), ('Store_10', 50695.200000000004), ('Store_51', 48934.79999999998), ('Store_55', 48852.9), ('Store_59', 48716.100000000006), ('Store_94', 47904.3), ('Store_88', 45821.69999999998), ('Store_67', 47731.5), ('Store_8', 45753.299999999996), ('Store_25', 47561.4), ('Store_33', 47780.99999999999), ('Store_68', 48503.69999999998), ('Store_27', 46578.600000000006), ('Store_18', 47193.29999999999), ('Store_52', 49655.69999999998), ('Store_38', 46957.49999999999), ('Store_29', 46940.4), ('Store_12', 45073.8), ('Store_3', 48351.6), ('Store_46', 44900.100000000006), ('Store_16', 45898.19999999999), ('Store_21', 48510.0), ('Store_9', 51319.80000000001), ('Store_98', 49287.600000000006), ('Store_1', 45810.89999999999), ('Store_73', 44778.6), ('Store_96', 48907.79999999998), ('Store_23', 46700.100000000006), ('Store_75', 47582.100000000006), ('Store_15', 52423.20000000001), ('Store_19', 47031.3), ('Store_62', 44961.3), ('Store_93', 48809.7), ('Store_58', 47890.79999999999), ('Store_37', 48234.600000000006), ('Store_22', 47962.799999999996), ('Store_89', 44500.5), ('Store_61', 47812.500000000015), ('Store_87', 43372.80000000001), ('Store_24', 45510.299999999996), ('Store_31', 47892.6), ('Store_54', 46744.2), ('Store_70', 46935.9), ('Store_20', 47242.8), ('Store_39', 50463.899999999994), ('Store_80', 45804.60000000001), ('Store_30', 45563.39999999999), ('Store_74', 45777.600000000006), ('Store_28', 48878.09999999999), ('Store_69', 46978.200000000004), ('Store_56', 49460.39999999999), ('Store_100', 46266.300000000025), ('Store_65', 48387.6), ('Store_48', 50833.79999999999), ('Store_7', 48561.3), ('Store_4', 45411.299999999996), ('Store_66', 45571.50000000001), ('Store_85', 48628.79999999999), ('Store_17', 44178.299999999996), ('Store_64', 48771.900000000016)]\n",
      "Done in 0:00:00.063574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[155] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache and persistence\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "discounted_rdd = rdd.map(lambda x: (x[0], x[1], x[2], 0 if x[3] is None else round(x[3]*0.9, 2)))\n",
    "store_sales_rdd = discounted_rdd.map(lambda x: (x[1], x[3]))  # key value pairs for store / sold-price\n",
    "print(f\"Calculating without cache (1)...\")\n",
    "\n",
    "ct = datetime.now()\n",
    "total_sales_rdd = store_sales_rdd.reduceByKey(lambda x, y: x + y)\n",
    "print(f\"{total_sales_rdd.collect()}\")\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "print(f\"Calculating without cache (2)...\")\n",
    "ct = datetime.now()\n",
    "print(f\"{total_sales_rdd.collect()}\")\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "print(f\"Calculating with cache (1)...\")\n",
    "ct = datetime.now()\n",
    "total_sales_rdd_cached = store_sales_rdd.reduceByKey(lambda x, y: x + y).cache()\n",
    "print(f\"{total_sales_rdd_cached.collect()}\")\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "print(f\"Calculating with cache (2)...\")\n",
    "ct = datetime.now()\n",
    "print(f\"{total_sales_rdd_cached.collect()}\")\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "print(f\"Calculating with persist (1)...\")\n",
    "ct = datetime.now()\n",
    "total_sales_rdd_peristed = store_sales_rdd.reduceByKey(lambda x, y: x + y).persist(StorageLevel.MEMORY_AND_DISK) # store in memory, spill to disk if needed\n",
    "print(f\"{total_sales_rdd_peristed.collect()}\")\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "print(f\"Calculating with persist (2)...\")\n",
    "ct = datetime.now()\n",
    "print(f\"{total_sales_rdd_peristed.collect()}\")\n",
    "print(f\"Done in {datetime.now() - ct}\")\n",
    "\n",
    "# remove from cache & storage\n",
    "total_sales_rdd_cached.unpersist()\n",
    "total_sales_rdd_peristed.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9fa8c2f8-be0b-4a00-b724-ffc0a46df966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# define data generation utility\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, TimestampType\n",
    "import shutil\n",
    "\n",
    "def generateDF(rows: int)->DataFrame:\n",
    "\n",
    "    # defining DF structure\n",
    "    df_schema = StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"age\", IntegerType(), True),\n",
    "        StructField(\"district\", IntegerType(), True),\n",
    "        StructField(\"from_date\", DateType(), True), # StructField(\"from_date\", DateType(), True),\n",
    "        StructField(\"to_date\", DateType(), True), #StructField(\"to_date\", DateType(), True),\n",
    "        # tech columns\n",
    "        StructField(\"to_date_year_month\", IntegerType(), True),\n",
    "    ])\n",
    "    \n",
    "    l = []\n",
    "    ct = datetime.now()\n",
    "    print(f\"Creating a dataset ({rows} rows)...\")\n",
    "    for c in range(rows):\n",
    "        from_date = datetime.now() + timedelta(days = random.randint(0, 1000))\n",
    "        #d = datetime.now() + timedelta(days = random.randint(0, 1000))\n",
    "        #from_date = int((datetime.now() + timedelta(days = random.randint(0, 1000))).strftime(\"%Y%m%d\"))\n",
    "        \n",
    "        to_date = from_date + timedelta(days = random.randint(1, 100))\n",
    "        #to_date = int((d + timedelta(days = random.randint(1, 100))).strftime(\"%Y%m%d\"))\n",
    "        \n",
    "        l.append([c, \n",
    "                  ('name-' + str(c)),  \n",
    "                  None if random.random() < 0.1 else random.randint(1, 100),  \n",
    "                  None if random.random() < 0.1 else random.randint(1000, 1004),\n",
    "                  from_date,\n",
    "                  to_date,\n",
    "                  #to_date // 100,\n",
    "                  int(to_date.strftime(\"%Y%m\")),\n",
    "                 ])\n",
    "    \n",
    "    df = spark.createDataFrame(l, df_schema)\n",
    "    print(f\"Done in {datetime.now() - ct}\")\n",
    "    return df\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5613f2f1-5588-4254-b8d5-fcb1786a678d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a dataset (100 rows)...\n",
      "Done in 0:00:00.028466\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- district: integer (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      " |-- to_date_year_month: integer (nullable = true)\n",
      "\n",
      "+---+------+---+--------+----------+----------+------------------+\n",
      "| id|  name|age|district| from_date|   to_date|to_date_year_month|\n",
      "+---+------+---+--------+----------+----------+------------------+\n",
      "|  0|name-0| 72|    1004|2025-06-04|2025-09-03|            202509|\n",
      "|  1|name-1| 39|    NULL|2025-10-28|2026-01-14|            202601|\n",
      "|  2|name-2|  7|    1003|2027-01-11|2027-03-14|            202703|\n",
      "|  3|name-3| 93|    1002|2027-05-22|2027-06-09|            202706|\n",
      "|  4|name-4| 44|    1003|2027-12-19|2028-03-23|            202803|\n",
      "+---+------+---+--------+----------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|     mydb|\n",
      "| mytestdb|\n",
      "+---------+\n",
      "\n",
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|          mytestdb|\n",
      "+------------------+\n",
      "\n",
      "+---------+---------------------+-----------+\n",
      "|namespace|tableName            |isTemporary|\n",
      "+---------+---------------------+-----------+\n",
      "|mytestdb |t_my_date_partitioned|false      |\n",
      "|mytestdb |v_my_date_partitioned|false      |\n",
      "+---------+---------------------+-----------+\n",
      "\n",
      "Dropping table...\n",
      "Saving table t_my_date_partitioned...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating view v_my_date_partitioned to manage technical column DOES NOT HELP!!!...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# partitioning (hash, range, round-robin) test\n",
    "# CANNOT FIND A WAY to PARTITION a date (to_date) into date ranges!!!\n",
    "\n",
    "df = generateDF(100)\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n",
    "spark.sql(\"create database if not exists mytestdb\")\n",
    "spark.sql(\"Show databases\").show()\n",
    "spark.sql(\"use mytestdb\")\n",
    "spark.sql(\"select current_database()\").show()\n",
    "spark.sql(\"Show tables\").show(truncate=False)\n",
    "\n",
    "print(\"Dropping table...\")\n",
    "spark.sql('drop table if exists t_my_date_partitioned')\n",
    "\n",
    "# save as managed \n",
    "print(\"Saving table t_my_date_partitioned...\")\n",
    "(\n",
    "    df.write\n",
    "    #.option(\"header\", True)\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"to_date_year_month\") \n",
    "    .format(\"parquet\") # parquet, csv for testing / readability\n",
    "    .saveAsTable(\"t_my_date_partitioned\")\n",
    ")\n",
    "\n",
    "# creating view to manage technical column\n",
    "print(\"creating view v_my_date_partitioned to manage technical column DOES NOT HELP!!!...\")\n",
    "spark.sql(\"\"\"\n",
    "create or replace view v_my_date_partitioned as\n",
    "SELECT *\n",
    "FROM t_my_date_partitioned t\n",
    "WHERE 1 = 1\n",
    "AND t.to_date_year_month = to_number(date_format(to_date, 'yyyyMM'),'000000')\n",
    "--AND t.to_date_year_month = floor(t.to_date / 100)\n",
    "\"\"\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ee6622b-f9ed-4985-8d3f-891728167461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying the table...\n",
      "+----------+----------+---+\n",
      "|from_date |to_date   |cnt|\n",
      "+----------+----------+---+\n",
      "|2025-06-04|2025-09-03|1  |\n",
      "|2025-06-07|2025-06-26|1  |\n",
      "|2025-06-15|2025-09-04|1  |\n",
      "|2025-08-16|2025-11-05|1  |\n",
      "|2025-08-24|2025-11-11|1  |\n",
      "+----------+----------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------------------+-----------+\n",
      "|namespace|tableName            |isTemporary|\n",
      "+---------+---------------------+-----------+\n",
      "|mytestdb |t_my_date_partitioned|false      |\n",
      "|mytestdb |v_my_date_partitioned|false      |\n",
      "+---------+---------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Querying table using SQL\n",
    "print(\"Querying the table...\")\n",
    "spark.sql(\"\"\"\n",
    "select from_date, to_date, count(1) cnt\n",
    "from t_my_date_partitioned\n",
    "group by from_date, to_date\n",
    "order by from_date, to_date nulls first\n",
    "\"\"\").show(5, truncate=False)\n",
    "\n",
    "# Drop table\n",
    "#print(\"Dropping table...\")\n",
    "#spark.sql('drop table if exists t_my_date_partitioned')\n",
    "\n",
    "spark.sql(\"Show tables\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "753f8415-9b68-417d-a0d9-81c8f8b11c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "| 86| 90|\n",
      "| 97| 29|\n",
      "| 27| 23|\n",
      "| 42| 20|\n",
      "| 72| 11|\n",
      "| 31| 43|\n",
      "| 96| 83|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "plan = spark.sql(\"\"\"\n",
    "select t.id, t.age\n",
    "FROM t_my_date_partitioned t\n",
    "WHERE 1 = 1\n",
    "AND to_date_year_month >= to_number('202710','000000')\n",
    "AND t.from_date <= to_date('20271015','yyyyMMdd')\n",
    "AND t.to_date > to_date('20271015','yyyyMMdd')\n",
    "\"\"\")\n",
    "plan.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54faa55f-8382-4fdd-8b22-ae4680ec60f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exec plan for table select with partition filter:\n",
      "plan: == Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet spark_catalog.mytestdb.t_my_date_partitioned[id#577,name#578,age#579,district#580,from_date#581,to_date#582,to_date_year_month#583] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(7 paths)[file:/home/jovyan/work/various_tests/spark/Performance_tuning/spark-wa..., PartitionFilters: [isnotnull(to_date_year_month#583), (to_date_year_month#583 >= 202710)], PushedFilters: [], ReadSchema: struct<id:int,name:string,age:int,district:int,from_date:date,to_date:date>\n",
      "\n",
      "\n",
      "\n",
      "exec plan for table select with partition filter and extra filter:\n",
      "plan: == Physical Plan ==\n",
      "*(1) Project [id#577, age#579]\n",
      "+- *(1) Filter (((isnotnull(from_date#581) AND isnotnull(to_date#582)) AND (from_date#581 <= 2027-10-15)) AND (to_date#582 > 2027-10-15))\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet spark_catalog.mytestdb.t_my_date_partitioned[id#577,age#579,from_date#581,to_date#582,to_date_year_month#583] Batched: true, DataFilters: [isnotnull(from_date#581), isnotnull(to_date#582), (from_date#581 <= 2027-10-15), (to_date#582 > ..., Format: Parquet, Location: InMemoryFileIndex(7 paths)[file:/home/jovyan/work/various_tests/spark/Performance_tuning/spark-wa..., PartitionFilters: [isnotnull(to_date_year_month#583), (to_date_year_month#583 >= 202710)], PushedFilters: [IsNotNull(from_date), IsNotNull(to_date), LessThanOrEqual(from_date,2027-10-15), GreaterThan(to_..., ReadSchema: struct<id:int,age:int,from_date:date,to_date:date>\n",
      "\n",
      "\n",
      "\n",
      "exec plan for VIEW select with partition filter and extra filter using view.  DOES NOT HELP!!!...:\n",
      "plan: == Physical Plan ==\n",
      "org.apache.spark.sql.AnalysisException: [DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve \"(from_date <= 20271015)\" due to data type mismatch: the left and right operands of the binary operator have incompatible types (\"DATE\" and \"INT\").; line 7 pos 4;\n",
      "'Project [*]\n",
      "+- 'Filter (((1 = 1) AND (from_date#726 <= 20271015)) AND (to_date#727 > 20271015))\n",
      "   +- SubqueryAlias t\n",
      "      +- SubqueryAlias spark_catalog.mytestdb.v_my_date_partitioned\n",
      "         +- View (`spark_catalog`.`mytestdb`.`v_my_date_partitioned`, [id#722,name#723,age#724,district#725,from_date#726,to_date#727,to_date_year_month#728])\n",
      "            +- Project [cast(id#577 as int) AS id#722, cast(name#578 as string) AS name#723, cast(age#579 as int) AS age#724, cast(district#580 as int) AS district#725, cast(from_date#581 as date) AS from_date#726, cast(to_date#582 as date) AS to_date#727, cast(to_date_year_month#583 as int) AS to_date_year_month#728]\n",
      "               +- Project [id#577, name#578, age#579, district#580, from_date#581, to_date#582, to_date_year_month#583]\n",
      "                  +- Filter ((1 = 1) AND (cast(to_date_year_month#583 as decimal(10,0)) = cast(to_number(date_format(cast(to_date#582 as timestamp), yyyyMM, Some(Europe/Budapest)), 000000) as decimal(10,0))))\n",
      "                     +- SubqueryAlias t\n",
      "                        +- SubqueryAlias spark_catalog.mytestdb.t_my_date_partitioned\n",
      "                           +- Relation spark_catalog.mytestdb.t_my_date_partitioned[id#577,name#578,age#579,district#580,from_date#581,to_date#582,to_date_year_month#583] parquet\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show SQL explain plan\n",
    "\n",
    "def print_dict(d: dict, level: int = 0) -> None:\n",
    "    \"\"\" print optionally hiearchic dict structure nicely formatted\n",
    "    \"\"\"\n",
    "\n",
    "    sp = \"\".ljust(level * 3)\n",
    "    for k in d.keys():\n",
    "        print(f\"{sp}{k}: {d[k]}\")\n",
    "        \n",
    "        if type(d[k]) is dict:\n",
    "            print_dict(d[k], level + 1)\n",
    "            \n",
    "    if level == 0:\n",
    "        print()\n",
    "\n",
    "print(\"exec plan for table select with partition filter:\")\n",
    "plan = spark.sql(\"\"\"\n",
    "explain\n",
    "select *\n",
    "FROM t_my_date_partitioned t\n",
    "WHERE 1 = 1\n",
    "AND t.to_date_year_month >= to_number('202710','000000')\n",
    "\"\"\")\n",
    "\n",
    "for row in plan.collect():\n",
    "    None\n",
    "    print_dict(row.asDict())\n",
    "\n",
    "#####################################################################\n",
    "print(\"exec plan for table select with partition filter and extra filter:\")\n",
    "plan = spark.sql(\"\"\"\n",
    "explain\n",
    "select t.id, t.age\n",
    "FROM t_my_date_partitioned t\n",
    "WHERE 1 = 1\n",
    "AND to_date_year_month >= to_number('202710','000000')\n",
    "AND t.from_date <= to_date('20271015','yyyyMMdd')\n",
    "AND t.to_date > to_date('20271015','yyyyMMdd')\n",
    "\"\"\")\n",
    "\n",
    "for row in plan.collect():\n",
    "    print_dict(row.asDict())\n",
    "\n",
    "#####################################################################\n",
    "# date type for to_date \n",
    "print(\"exec plan for VIEW select with partition filter and extra filter using view.  DOES NOT HELP!!!...:\")\n",
    "plan = spark.sql(\"\"\"\n",
    "explain\n",
    "select *\n",
    "FROM v_my_date_partitioned t\n",
    "WHERE 1 = 1\n",
    "--AND t.to_date_year_month >= to_number('202710','000000')\n",
    "AND t.from_date <= to_date('20271015','yyyyMMdd')\n",
    "AND t.to_date > to_date('20271015','yyyyMMdd')\n",
    "\"\"\")\n",
    "\n",
    "# int type for to_date\n",
    "plan = spark.sql(\"\"\"\n",
    "explain\n",
    "select *\n",
    "FROM v_my_date_partitioned t\n",
    "WHERE 1 = 1\n",
    "--AND t.to_date_year_month >= 202710\n",
    "AND t.from_date <= 20271015\n",
    "AND t.to_date > 20271015\n",
    "\"\"\")\n",
    "\n",
    "for row in plan.collect():\n",
    "    None\n",
    "    print_dict(row.asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bff5ed03-297a-4a1e-95fa-f784afec309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DF from table...\n",
      "+---+-------+---+--------+----------+----------+------------------+\n",
      "|id |name   |age|district|from_date |to_date   |to_date_year_month|\n",
      "+---+-------+---+--------+----------+----------+------------------+\n",
      "|52 |name-52|3  |1003    |2026-01-10|2026-03-14|202603            |\n",
      "|63 |name-63|82 |1000    |2025-12-04|2026-03-14|202603            |\n",
      "|70 |name-70|92 |1003    |2026-02-01|2026-03-15|202603            |\n",
      "|26 |name-26|25 |1000    |2026-02-14|2026-03-02|202603            |\n",
      "+---+-------+---+--------+----------+----------+------------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "plan with partition filter (PartitionFilters[...]):\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [from_date#581 ASC NULLS FIRST, to_date#582 ASC NULLS FIRST, id#577 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(from_date#581 ASC NULLS FIRST, to_date#582 ASC NULLS FIRST, id#577 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=550]\n",
      "      +- FileScan parquet spark_catalog.mytestdb.t_my_date_partitioned[id#577,age#579,from_date#581,to_date#582,to_date_year_month#583] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(7 paths)[file:/home/jovyan/work/various_tests/spark/Performance_tuning/spark-wa..., PartitionFilters: [isnotnull(to_date_year_month#583), (to_date_year_month#583 >= 202710)], PushedFilters: [], ReadSchema: struct<id:int,age:int,from_date:date,to_date:date>\n",
      "\n",
      "\n",
      "+---+----+----------+----------+------------------+\n",
      "|id |age |from_date |to_date   |to_date_year_month|\n",
      "+---+----+----------+----------+------------------+\n",
      "|96 |83  |2027-08-16|2027-11-23|202711            |\n",
      "|28 |NULL|2027-08-26|2027-10-03|202710            |\n",
      "|72 |11  |2027-08-26|2027-11-23|202711            |\n",
      "|27 |23  |2027-08-29|2027-10-22|202710            |\n",
      "|97 |29  |2027-09-15|2027-12-22|202712            |\n",
      "|86 |90  |2027-09-17|2027-12-21|202712            |\n",
      "|31 |43  |2027-09-23|2027-12-14|202712            |\n",
      "|42 |20  |2027-09-28|2027-11-29|202711            |\n",
      "|6  |22  |2027-11-14|2027-11-30|202711            |\n",
      "|4  |44  |2027-12-19|2028-03-23|202803            |\n",
      "|18 |NULL|2027-12-22|2028-02-19|202802            |\n",
      "|60 |94  |2027-12-25|2028-01-15|202801            |\n",
      "|15 |34  |2027-12-30|2028-01-17|202801            |\n",
      "|46 |53  |2028-01-13|2028-04-21|202804            |\n",
      "+---+----+----------+----------+------------------+\n",
      "\n",
      "plan with partition filter (PartitionFilters[...]) and extra filter:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [from_date#581 ASC NULLS FIRST, to_date#582 ASC NULLS FIRST, id#577 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(from_date#581 ASC NULLS FIRST, to_date#582 ASC NULLS FIRST, id#577 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=605]\n",
      "      +- Filter (((isnotnull(from_date#581) AND isnotnull(to_date#582)) AND (from_date#581 <= 2027-10-15)) AND (to_date#582 > 2027-10-15))\n",
      "         +- FileScan parquet spark_catalog.mytestdb.t_my_date_partitioned[id#577,age#579,from_date#581,to_date#582,to_date_year_month#583] Batched: true, DataFilters: [isnotnull(from_date#581), isnotnull(to_date#582), (from_date#581 <= 2027-10-15), (to_date#582 > ..., Format: Parquet, Location: InMemoryFileIndex(7 paths)[file:/home/jovyan/work/various_tests/spark/Performance_tuning/spark-wa..., PartitionFilters: [isnotnull(to_date_year_month#583), (to_date_year_month#583 >= 202710)], PushedFilters: [IsNotNull(from_date), IsNotNull(to_date), LessThanOrEqual(from_date,2027-10-15), GreaterThan(to_..., ReadSchema: struct<id:int,age:int,from_date:date,to_date:date>\n",
      "\n",
      "\n",
      "+---+---+----------+----------+------------------+\n",
      "|id |age|from_date |to_date   |to_date_year_month|\n",
      "+---+---+----------+----------+------------------+\n",
      "|96 |83 |2027-08-16|2027-11-23|202711            |\n",
      "|72 |11 |2027-08-26|2027-11-23|202711            |\n",
      "|27 |23 |2027-08-29|2027-10-22|202710            |\n",
      "|97 |29 |2027-09-15|2027-12-22|202712            |\n",
      "|86 |90 |2027-09-17|2027-12-21|202712            |\n",
      "|31 |43 |2027-09-23|2027-12-14|202712            |\n",
      "|42 |20 |2027-09-28|2027-11-29|202711            |\n",
      "+---+---+----------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show python DF explain plan\n",
    "\n",
    "def print_dict(d: dict, level: int = 0) -> None:\n",
    "    \"\"\" print optionally hiearchic dict structure nicely formatted\n",
    "    \"\"\"\n",
    "\n",
    "    sp = \"\".ljust(level * 3)\n",
    "    for k in d.keys():\n",
    "        print(f\"{sp}{k}: {d[k]}\")\n",
    "        \n",
    "        if type(d[k]) is dict:\n",
    "            print_dict(d[k], level + 1)\n",
    "            \n",
    "    if level == 0:\n",
    "        print()\n",
    "\n",
    "print(\"Reading DF from table...\")\n",
    "spark.sql(\"use mytestdb\")\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .table(\"t_my_date_partitioned\")\n",
    ")\n",
    "df.show(4, truncate=False)\n",
    "\n",
    "print(\"plan with partition filter (PartitionFilters[...]):\")\n",
    "df1 = (\n",
    "    df\n",
    "    .filter(\"to_date_year_month >= 202710\")\n",
    "    .select(df.id, df.age, df.from_date, df.to_date, df.to_date_year_month)\n",
    "    .orderBy(df.from_date, df.to_date, df.id)\n",
    "    )\n",
    "\n",
    "df1.explain()\n",
    "\n",
    "df1.show(100, truncate=False)\n",
    "\n",
    "for row in df1.collect():\n",
    "    None\n",
    "    #print_dict(row.asDict())\n",
    "\n",
    "###################################################################\n",
    "print(\"plan with partition filter (PartitionFilters[...]) and extra filter:\")\n",
    "df1 = (\n",
    "    df\n",
    "    .filter(\"to_date_year_month >= 202710\")\n",
    "    .filter(\"from_date <= to_date('20271015','yyyyMMdd') AND to_date > to_date('20271015','yyyyMMdd')\")\n",
    "    .select(df.id, df.age, df.from_date, df.to_date, df.to_date_year_month)\n",
    "    .orderBy(df.from_date, df.to_date, df.id)\n",
    "    )\n",
    "\n",
    "df1.explain()\n",
    "\n",
    "df1.show(100, truncate=False)\n",
    "\n",
    "for row in df1.collect():\n",
    "    None\n",
    "    #print_dict(row.asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "abd03cc2-ecbe-4e6e-8cd3-594eb6bc61b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan with partition filter (PartitionFilters[...]):\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [from_date#513 ASC NULLS FIRST, to_date#514 ASC NULLS FIRST, id#509 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(from_date#513 ASC NULLS FIRST, to_date#514 ASC NULLS FIRST, id#509 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=2175]\n",
      "      +- Filter (((isnotnull(from_date#513) AND isnotnull(to_date#514)) AND (from_date#513 <= 2027-10-15)) AND (to_date#514 > 2027-10-15))\n",
      "         +- FileScan parquet spark_catalog.mytestdb.t_my_date_partitioned[id#509,age#511,from_date#513,to_date#514,to_date_year_month#515] Batched: true, DataFilters: [isnotnull(from_date#513), isnotnull(to_date#514), (from_date#513 <= 2027-10-15), (to_date#514 > ..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/work/various_tests/spark/spark-warehouse/mytestdb.db..., PartitionFilters: [isnotnull(to_date_year_month#515), (to_date_year_month#515 = 202710)], PushedFilters: [IsNotNull(from_date), IsNotNull(to_date), LessThanOrEqual(from_date,2027-10-15), GreaterThan(to_..., ReadSchema: struct<id:int,age:int,from_date:date,to_date:date>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"plan with partition filter (PartitionFilters[...]):\")\n",
    "df1 = (\n",
    "    df\n",
    "    .filter(\"to_date_year_month >= 202710\")\n",
    "    .filter(\"from_date <= to_date('20271015','yyyyMMdd') AND to_date > to_date('20271015','yyyyMMdd')\")\n",
    "    .select(df.id, df.age, df.from_date, df.to_date, df.to_date_year_month)\n",
    "    .orderBy(df.from_date, df.to_date, df.id)\n",
    "    )\n",
    "\n",
    "df1.explain()\n",
    "\n",
    "#df1.show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "552ca245-add6-40e9-957b-7788aae16442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lst size: 100\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "| mytestdb|\n",
      "|   testdb|\n",
      "+---------+\n",
      "\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n",
      "+---+-------------+----+----------+----------+\n",
      "|id |name         |age |from_dt   |to_dt     |\n",
      "+---+-------------+----+----------+----------+\n",
      "|0  |name-00000000|72  |2026-06-22|2026-07-20|\n",
      "|1  |name-00000001|19  |2026-06-27|2026-08-19|\n",
      "|2  |name-00000002|98  |2026-11-16|2026-12-11|\n",
      "|3  |name-00000003|52  |2025-09-12|2025-09-13|\n",
      "|4  |name-00000004|65  |2027-11-30|2028-02-09|\n",
      "|5  |name-00000005|18  |2027-02-20|2027-03-01|\n",
      "|6  |name-00000006|59  |2025-11-16|2025-12-15|\n",
      "|7  |name-00000007|67  |2025-05-05|2025-08-13|\n",
      "|8  |name-00000008|72  |2025-04-10|2025-04-11|\n",
      "|9  |name-00000009|29  |2025-04-23|2025-07-26|\n",
      "|10 |name-00000010|72  |2026-05-31|2026-07-30|\n",
      "|11 |name-00000011|52  |2026-01-03|2026-02-22|\n",
      "|12 |name-00000012|46  |2027-12-13|2027-12-17|\n",
      "|13 |name-00000013|NULL|2026-05-23|2026-06-24|\n",
      "|14 |name-00000014|NULL|2027-11-30|2028-01-13|\n",
      "|15 |name-00000015|NULL|2027-12-12|2028-02-26|\n",
      "|16 |name-00000016|34  |2026-08-19|2026-11-06|\n",
      "|17 |name-00000017|21  |2027-03-30|2027-06-27|\n",
      "|18 |name-00000018|7   |2025-06-25|2025-09-10|\n",
      "|19 |name-00000019|99  |2026-05-10|2026-05-20|\n",
      "+---+-------------+----+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20250408 15:00:11 : Done.\n"
     ]
    }
   ],
   "source": [
    "# partition by various methods\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, TimestampType\n",
    "\n",
    "df_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"from_dt\", DateType(), True),\n",
    "    StructField(\"to_dt\", DateType(), True),\n",
    "])\n",
    "\n",
    "rows = 100\n",
    "lst = []\n",
    "for c in range(rows):\n",
    "    from_dt = datetime.now().date() + timedelta(days=random.randint(0, 1000))\n",
    "    to_dt = from_dt + timedelta(days=random.randint(1, 100))\n",
    "\n",
    "    lst.append([\n",
    "        c,\n",
    "        'name-' + str(c).zfill(8),\n",
    "        None if random.random() < 0.1 else random.randint(1, 100),\n",
    "        from_dt,\n",
    "        to_dt\n",
    "         ])\n",
    "\n",
    "print(f\"lst size: {len(lst)}\")\n",
    "\n",
    "df = spark.createDataFrame(lst, df_schema)\n",
    "\n",
    "    \n",
    "spark.sql(\"create database if not exists testdb\")\n",
    "spark.sql(\"Show databases\").show()\n",
    "spark.sql(\"use testdb\")\n",
    "spark.sql(\"show tables\").show()\n",
    "\n",
    "df.show(truncate = False)\n",
    "\n",
    "#df.groupBy(\"to_dt\").agg(\"*\", \"count\").orderBy(\"to_dt\")\n",
    "\n",
    "# partition by all occuring to_dt values\n",
    "#df.writeTo(\"my_partitioned_table\").partitionedBy(partitioning.months(\"to_dt\")).createOrReplace() # spark 4.0.0 (buggy)\n",
    "df.write.partitionBy(\"to_dt\").mode(\"overwrite\").parquet('mydf_1.parquet')  # spark 3.5\n",
    "\n",
    "# partition by a given number of partitions using HASH\n",
    "# processing the data must be done as one (no pruning), only splitting the file to chunks is achieved\n",
    "df1 = df.repartition(4, \"to_dt\")\n",
    "df1.write.mode(\"overwrite\").parquet('mydf_2.parquet')  # spark 3.5\n",
    "\n",
    "# partition by a given number of partitions using RANGE by sampling, to automatically estimate the ranges\n",
    "# processing the data must be done as one, only splitting the file to chunks is achieved\n",
    "df1 = df.repartitionByRange(4, \"to_dt\")\n",
    "df1.write.mode(\"overwrite\").parquet('mydf_3.parquet')  # spark 3.5\n",
    "\n",
    "# limit the records, thus partitioning data in file roughly(!) equally.  full data processing is necessary\n",
    "df.write.mode(\"overwrite\").option(\"maxRecordsPerFile\", 20).csv(\"mydf_4.csv\")\n",
    "\n",
    "print(datetime.now().strftime(\"%Y%m%d %H:%M:%S\"), \": Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
