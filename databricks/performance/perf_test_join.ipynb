{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c532ce-fc55-42af-b86b-ec64b2c60147",
   "metadata": {},
   "source": [
    "## 1. Small-large datasets: Broadcast joins, to send small ref data to all workers, to avoid many data shuffling (repartitioning both tables the same way) in joins.\n",
    "## 2. Skewed (uneven divided) data: Bucketing, to group data based on hash function of join/groupby key, to balance worker worflows executions, reduce shuffling during joins, avoid data skews.  Used for high cardinality (highly unique, HIGH selectivity, hence HASH function must be applied to map unique values to a few \"buckets\"). Partitioning is the opposite: used for low cardinality (non-unique, LOW selectivity), partition key is directly the column values, that are only a few!\n",
    "## 3. too small or too large partitions: Repartitioning (splitting large partitions) or coalescing (merging small partitions), to redistribute data evenly (same computational sizes), so to avoid data skewing (small and large partitions, where large partition computations will delay the overall result). Ideally 1 partition should be of about 128MB\n",
    "## 4. AQE (Adaptive Query Execution): Spark optimizer based on runtime statistics (broadcast, repartition, coalesce, dynamic join stragegy selection)\n",
    "### Skewed data handling to ensure balanced workflows (dynamically adjust partition sizes: repartition() -  split large partitions / coalesce() - merge small partitions)\n",
    "### Dynamic join strategy selection\n",
    "### Dynamic partition coalescing (merging of small partitions to reduce join shuffling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6370c3ef-b0bf-4789-bcdb-14e24feec70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/13 18:12:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/13 18:12:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark version: 3.5.5\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# setup spark on linux\n",
    "from pyspark.sql import *\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         #.master(\"local\") # local- no parallelizm at all, local[2] - 2 cores, local[*] - as many cores as local logical cores\n",
    "         .appName(\"Broadcast joins - controlled\")\n",
    "         .enableHiveSupport() # enableHiveSupport() needed to make data persistent... \n",
    "         .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) # max. byte size of table to be broadcast. -1 = disable\n",
    "         .config(\"spark.sql.adaptive.enabled\", False) # disable AQE\n",
    "         #.config(\"spark.log.level\", \"ERROR\")\n",
    "         .getOrCreate())\n",
    "\n",
    "print('spark version:', spark.version)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c11922ba-c9b0-4d7c-83d0-1f4699d919cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 18:13:12 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/05/13 18:13:12 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/05/13 18:13:14 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "25/05/13 18:13:14 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore jovyan@172.17.0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|     mydb|\n",
      "| mytestdb|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0350ee1f-64b7-43c6-8e88-af12be9d2391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 18:17:37 WARN TaskSetManager: Stage 2 contains a task of very large size (4325 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|product_id|sales_amount|\n",
      "+----------+------------+\n",
      "|Product_70|574         |\n",
      "|Product_99|683         |\n",
      "|Product_32|304         |\n",
      "|Product_27|498         |\n",
      "|Product_59|989         |\n",
      "+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-----------+\n",
      "|product_id|category_id|\n",
      "+----------+-----------+\n",
      "|Product_1 |Category_9 |\n",
      "|Product_2 |Category_6 |\n",
      "|Product_3 |Category_4 |\n",
      "|Product_4 |Category_9 |\n",
      "|Product_5 |Category_9 |\n",
      "+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# generate test data\n",
    "import random\n",
    "\n",
    "transaction_list = [\n",
    "    (f'Product_{random.randint(1, 100)}',  # product_id\n",
    "     random.randint(1, 1000)               # sales_amount\n",
    "     )\n",
    "    for c in range(1000000)]\n",
    "transaction_df = spark.createDataFrame(transaction_list, ['product_id', 'sales_amount'])\n",
    "transaction_df.show(5, truncate = False)\n",
    "\n",
    "ref_list = [\n",
    "    (f'Product_{c}',                     # product_id\n",
    "     f'Category_{random.randint(1, 10)}' # category_id\n",
    "     )\n",
    "    for c in range(1, 101)]\n",
    "ref_df = spark.createDataFrame(ref_list, ['product_id', 'category_id'])\n",
    "ref_df.show(5, truncate = False)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e768f9b-04c9-4ca0-9441-d2e057100289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 18:20:16 WARN TaskSetManager: Stage 5 contains a task of very large size (4325 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard_join_df done in 0:00:05.296089 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 18:20:21 WARN TaskSetManager: Stage 9 contains a task of very large size (4325 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broadcast_join_df done in 0:00:01.263778 secs.\n",
      "+----------+------------+----------+-----------+\n",
      "|product_id|sales_amount|product_id|category_id|\n",
      "+----------+------------+----------+-----------+\n",
      "|Product_70|574         |Product_70|Category_8 |\n",
      "|Product_99|683         |Product_99|Category_5 |\n",
      "|Product_32|304         |Product_32|Category_3 |\n",
      "|Product_27|498         |Product_27|Category_10|\n",
      "|Product_59|989         |Product_59|Category_3 |\n",
      "+----------+------------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 18:20:22 WARN TaskSetManager: Stage 12 contains a task of very large size (4325 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "# 1. standard vs broadcast join (for fact + ref data)\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "if 'standard_join_df' in globals():\n",
    "    del standard_join_df\n",
    "\n",
    "ct = datetime.now()\n",
    "standard_join_df = transaction_df.alias('tr').join(ref_df.alias('ref'), col('tr.product_id') == col('ref.product_id'), 'inner')\n",
    "standard_join_df.count()  # action\n",
    "print(f\"standard_join_df done in {(datetime.now() - ct)} secs.\")\n",
    "\n",
    "ct = datetime.now()\n",
    "broadcast_join_df = transaction_df.alias('tr').join(broadcast(ref_df).alias('ref'), col('tr.product_id') == col('ref.product_id'), 'inner')\n",
    "broadcast_join_df.count()  # action\n",
    "print(f\"broadcast_join_df done in {(datetime.now() - ct)} secs.\")\n",
    "\n",
    "broadcast_join_df.show(5, truncate= False)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "701ed84c-c2ab-44f3-b887-3c6bbadb5ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 18:22:20 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "25/05/13 18:22:20 WARN FileUtils: File file:/home/jovyan/work/various_tests/spark/Performance_tuning/spark-warehouse/bucketed_transaction does not exist; Force to delete it.\n",
      "25/05/13 18:22:20 ERROR FileUtils: Failed to delete file:/home/jovyan/work/various_tests/spark/Performance_tuning/spark-warehouse/bucketed_transaction\n",
      "25/05/13 18:22:20 WARN FileUtils: File file:/home/jovyan/work/various_tests/spark/Performance_tuning/spark-warehouse/bucketed_ref does not exist; Force to delete it.\n",
      "25/05/13 18:22:20 ERROR FileUtils: Failed to delete file:/home/jovyan/work/various_tests/spark/Performance_tuning/spark-warehouse/bucketed_ref\n",
      "25/05/13 18:22:21 WARN TaskSetManager: Stage 13 contains a task of very large size (4325 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/13 18:22:24 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/05/13 18:22:24 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "25/05/13 18:22:24 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/05/13 18:22:24 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketed_join done in 0:00:01.627052 secs.\n",
      "+----------+------------+----------+-----------+\n",
      "|product_id|sales_amount|product_id|category_id|\n",
      "+----------+------------+----------+-----------+\n",
      "|Product_10|851         |Product_10|Category_3 |\n",
      "|Product_10|699         |Product_10|Category_3 |\n",
      "|Product_10|404         |Product_10|Category_3 |\n",
      "|Product_10|236         |Product_10|Category_3 |\n",
      "|Product_10|759         |Product_10|Category_3 |\n",
      "+----------+------------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# 2. bucketed join (needs to save the data after bucketing.  good for multiple reads. only saveAsTable is supported for now!...)\n",
    "\n",
    "# 0. delete the tables if exist\n",
    "spark.sql('DROP TABLE IF EXISTS bucketed_transaction')\n",
    "spark.sql('DROP TABLE IF EXISTS bucketed_ref')\n",
    "\n",
    "# 1. save the data in buckets\n",
    "buckets = 10\n",
    "transaction_df.write.format('parquet').bucketBy(buckets, 'product_id').saveAsTable('bucketed_transaction')\n",
    "ref_df.write.format('parquet').bucketBy(buckets, 'product_id').saveAsTable('bucketed_ref')\n",
    "\n",
    "# 2. read back the data\n",
    "bucketed_transaction_df = spark.table('bucketed_transaction')\n",
    "bucketed_ref_df = spark.table('bucketed_ref')\n",
    "\n",
    "# 3. execute bucketed join\n",
    "ct = datetime.now()\n",
    "bucketed_join_df = bucketed_transaction_df.alias('tr').join(bucketed_ref_df.alias('ref'), col('tr.product_id') == col('ref.product_id'), 'inner')\n",
    "bucketed_join_df.count()  # action\n",
    "print(f\"bucketed_join done in {(datetime.now() - ct)} secs.\")\n",
    "bucketed_join_df.show(5, truncate= False)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b91ac64f-bf4e-47d6-8dd7-3661ecf6761f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark version: 3.5.5\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# 3. AQE with new config setup (AQE, skewJoins, coalescePartitions enabled, broadcast disabled)\n",
    "# setup spark on linux\n",
    "from pyspark.sql import *\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    None\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         #.master(\"local\") # local- no parallelizm at all, local[2] - 2 cores, local[*] - as many cores as local logical cores\n",
    "         .appName(\"AQE\")\n",
    "         .enableHiveSupport() # enableHiveSupport() needed to make data persistent... \n",
    "         .config(\"spark.sql.adaptive.enabled\", True) # enable AQE\n",
    "         .config(\"spark.sql.adaptive.skewJoin.enabled\", True) # enable skewJoin\n",
    "         .config(\"spark.sql.adaptive.coalescePartitions.enabled\", True) # enable coalescePartitions\n",
    "         .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) # max. byte size of table to be broadcast. -1 = disable\n",
    "         .getOrCreate())\n",
    "\n",
    "print('spark version:', spark.version)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3edafc81-7654-4078-b65f-35219efbb15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 18:25:18 WARN TaskSetManager: Stage 0 contains a task of very large size (4102 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 0:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|product_id|count |\n",
      "+----------+------+\n",
      "|Product_1 |999003|\n",
      "|Product_50|18    |\n",
      "|Product_92|17    |\n",
      "|Product_10|17    |\n",
      "|Product_14|16    |\n",
      "+----------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-----------+\n",
      "|product_id|category_id|\n",
      "+----------+-----------+\n",
      "|Product_1 |Category_2 |\n",
      "|Product_2 |Category_5 |\n",
      "|Product_3 |Category_2 |\n",
      "|Product_4 |Category_8 |\n",
      "|Product_5 |Category_4 |\n",
      "+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# change the transaction test data to be skewed (Product_1 with much higher occurence)\n",
    "import random\n",
    "\n",
    "transaction_list = [\n",
    "    (f'Product_{random.randint(1, 100)  if c < 1000 else 1}',  # product_id\n",
    "     random.randint(1, 1000)                                   # sales_amount\n",
    "     )\n",
    "    for c in range(1000000)]\n",
    "transaction_df = spark.createDataFrame(transaction_list, ['product_id', 'sales_amount'])\n",
    "transaction_df.groupBy('product_id').count().orderBy('count', ascending = False).show(5, truncate = False)\n",
    "\n",
    "ref_list = [\n",
    "    (f'Product_{c}',                     # product_id\n",
    "     f'Category_{random.randint(1, 10)}' # category_id\n",
    "     )\n",
    "    for c in range(1, 101)]\n",
    "ref_df = spark.createDataFrame(ref_list, ['product_id', 'category_id'])\n",
    "ref_df.show(5, truncate = False)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d549b6a-cfa6-46be-8e23-6700e23b6ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 18:25:25 WARN TaskSetManager: Stage 5 contains a task of very large size (4102 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_AQE_join done in 0:00:02.921626 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 18:25:28 WARN TaskSetManager: Stage 8 contains a task of very large size (4102 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 9:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AQE_join done in 0:00:01.322358 secs.\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# join with / without AQE\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# change config, disable AQE\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)  # disable AQE\n",
    "ct = datetime.now()\n",
    "non_aqe_join_df = transaction_df.alias('tr').join(ref_df.alias('ref'), col('tr.product_id') == col('ref.product_id'), 'inner')\n",
    "non_aqe_join_df.count()  # action\n",
    "print(f\"non_AQE_join done in {(datetime.now() - ct)} secs.\")\n",
    "\n",
    "# change config, enable AQE\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", True)  # enable AQE\n",
    "ct = datetime.now()\n",
    "aqe_join_df = transaction_df.alias('tr').join(ref_df.alias('ref'), col('tr.product_id') == col('ref.product_id'), 'inner')\n",
    "aqe_join_df.count()  # action\n",
    "print(f\"AQE_join done in {(datetime.now() - ct)} secs.\")\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8783d1b-0930-4dcb-9e6a-4b806c09296f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark version: 3.5.5\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# 3. AQE with new config setup (AQE disabled - sort merge join, enabled - broadcasting)\n",
    "# setup spark on linux\n",
    "from pyspark.sql import *\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    None\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         #.master(\"local\") # local- no parallelizm at all, local[2] - 2 cores, local[*] - as many cores as local logical cores\n",
    "         .appName(\"AQE\")\n",
    "         .enableHiveSupport() # enableHiveSupport() needed to make data persistent... \n",
    "         .config(\"spark.sql.adaptive.enabled\", True) # enable AQE\n",
    "         .getOrCreate())\n",
    "\n",
    "print('spark version:', spark.version)# change the transaction test data to be skewed (Product_1 with much higher occurence)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a5de623-7fad-4bc9-9cc8-db4fd440f27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 18:28:19 WARN TaskSetManager: Stage 0 contains a task of very large size (4325 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 0:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|product_id|count|\n",
      "+----------+-----+\n",
      "|Product_78|10282|\n",
      "|Product_48|10254|\n",
      "|Product_3 |10201|\n",
      "|Product_43|10171|\n",
      "|Product_79|10161|\n",
      "+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-----------+\n",
      "|product_id|category_id|\n",
      "+----------+-----------+\n",
      "|Product_1 |Category_7 |\n",
      "|Product_2 |Category_10|\n",
      "|Product_3 |Category_1 |\n",
      "|Product_4 |Category_6 |\n",
      "|Product_5 |Category_10|\n",
      "+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# generate test data\n",
    "import random\n",
    "\n",
    "transaction_list = [\n",
    "    (f'Product_{random.randint(1, 100)}',  # product_id\n",
    "     random.randint(1, 1000)               # sales_amount\n",
    "     )\n",
    "    for c in range(1000000)]\n",
    "transaction_df = spark.createDataFrame(transaction_list, ['product_id', 'sales_amount'])\n",
    "transaction_df.groupBy('product_id').count().orderBy('count', ascending = False).show(5, truncate = False)\n",
    "\n",
    "ref_list = [\n",
    "    (f'Product_{c}',                     # product_id\n",
    "     f'Category_{random.randint(1, 10)}' # category_id\n",
    "     )\n",
    "    for c in range(1, 101)]\n",
    "ref_df = spark.createDataFrame(ref_list, ['product_id', 'category_id'])\n",
    "ref_df.show(5, truncate = False)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "146ccc73-2941-4f1a-8c73-2d7a1fa9ca63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 18:28:23 WARN TaskSetManager: Stage 4 contains a task of very large size (4325 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 6:===============================================>       (171 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_AQE_join done in 0:00:02.634615 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 18:28:25 WARN TaskSetManager: Stage 8 contains a task of very large size (4325 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 9:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AQE_join done in 0:00:01.508587 secs.\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# join with / without AQE\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# change config, disable AQE (not using broadcast, but sort-merge join strategy)\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)  # disable AQE\n",
    "ct = datetime.now()\n",
    "non_aqe_join_df = transaction_df.alias('tr').join(ref_df.alias('ref'), col('tr.product_id') == col('ref.product_id'), 'inner')\n",
    "non_aqe_join_df.count()  # action\n",
    "print(f\"non_AQE_join done in {(datetime.now() - ct)} secs.\")\n",
    "\n",
    "# change config, enable AQE (using broadcast)\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", True)  # enable AQE\n",
    "ct = datetime.now()\n",
    "aqe_join_df = transaction_df.alias('tr').join(ref_df.alias('ref'), col('tr.product_id') == col('ref.product_id'), 'inner')\n",
    "aqe_join_df.count()  # action\n",
    "print(f\"AQE_join done in {(datetime.now() - ct)} secs.\")\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92454d68-eff5-4a2f-bcab-6f579eac6f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/25 00:26:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark version: 3.5.5\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# 4. Dynamic partition coalescing\n",
    "# 3. AQE with new config setup (AQE disabled - sort merge join, enabled - broadcasting)\n",
    "# setup spark on linux\n",
    "from pyspark.sql import *\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    None\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         #.master(\"local\") # local- no parallelizm at all, local[2] - 2 cores, local[*] - as many cores as local logical cores\n",
    "         .appName(\"Dynamic partition coalescing\")\n",
    "         .enableHiveSupport() # enableHiveSupport() needed to make data persistent... \n",
    "         .config(\"spark.sql.adaptive.enabled\", True) # enable AQE\n",
    "         .config(\"spark.sql.adaptive.coalescePartitions.enabled\", True) # will coalesce partitions to be of size advisoryPartitionSizeInBytes\n",
    "         .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"64MB\") # size of the partitions to shuffle\n",
    "         .config(\"spark.sql.shuffle.Partitions\", \"20\") # number of partitions to use when shuffling (default is 200)\n",
    "         .getOrCreate())\n",
    "\n",
    "print('spark version:', spark.version)# change the transaction test data to be skewed (Product_1 with much higher occurence)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6844fa4b-4583-46fd-b669-439e48207c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# generate test data\n",
    "import random\n",
    "\n",
    "transaction_list = [\n",
    "    (f'Product_{random.randint(1, 100)}',  # product_id\n",
    "     random.randint(1, 1000)               # sales_amount\n",
    "     )\n",
    "    for c in range(1000000)]\n",
    "transaction_df = spark.createDataFrame(transaction_list, ['product_id', 'sales_amount'])\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a45d1c1-abf8-4371-934e-f5c84b7164bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 18:35:29 WARN TaskSetManager: Stage 17 contains a task of very large size (4325 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/13 18:35:31 WARN TaskSetManager: Stage 23 contains a task of very large size (4325 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_coalesced_aggregate done in 0:00:01.833081 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coalesced_aggregate done in 0:00:00.837081 secs.\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# aggregate\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# change config, disable coalescePartitions\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)  # disable coalescePartitions\n",
    "ct = datetime.now()\n",
    "none_coalesced_result_df = transaction_df.groupBy('product_id').sum('sales_amount')\n",
    "none_coalesced_result_df.count() # action\n",
    "print(f\"non_coalesced_aggregate done in {(datetime.now() - ct)} secs.\")\n",
    "\n",
    "# change config, enable AQE (using broadcast)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)  # enable coalescePartitions, spark will dynamically merge (coalesce small partitions)\n",
    "ct = datetime.now()\n",
    "coalesced_result_df = transaction_df.groupBy('product_id').sum('sales_amount')\n",
    "coalesced_result_df.count() # action\n",
    "print(f\"coalesced_aggregate done in {(datetime.now() - ct)} secs.\")\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e83b9b27-2122-4fa5-a09f-3b3446fd7d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# stop spark session (localhost:4040 will be reset, unaccessible until a new session starts)\n",
    "spark.stop()\n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
